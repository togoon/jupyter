
'''

pip install jupyter
http://pypi.douban.com/simple/
pip install  -i https://pypi.tuna.tsinghua.edu.cn/simple  jupyter
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas
python  -m pip install --upgrade pip
python  -m pip install  -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip
pip install jupyter_contrib_nbextensions
pip install   -i https://pypi.tuna.tsinghua.edu.cn/simple  jupyter_contrib_nbextensions
工作目录
jupyter notebook --generate-config
Writing default config to: C:\Users\Administrator\.jupyter\jupyter_notebook_conf
ig.py
c.NotebookApp.notebook_dir
D:\\Doc\\Test\\ipynb
jupyter notebook

Nbextensions标签
pip uninstall jupyter_contrib_nbextensions
pip uninstall jupyter_nbextensions_configurator

pip install jupyter_contrib_nbextensions
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple jupyter_contrib_nbextensions
jupyter contrib nbextension install --user
pip install jupyter_nbextensions_configurator
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple  jupyter_nbextensions_configurator

pip install -i https://pypi.tuna.tsinghua.edu.cn/simple lxml
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tushare --upgrade
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple numpy
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple matplotlib
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple plotly
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple sklearn  scikit-learn
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple scipy
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple seaborn
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple StatsModels
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple keras
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple TensorFlow
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas_datareader

pip install baostock -i https://pypi.tuna.tsinghua.edu.cn/simple/ --trusted-host pypi.tuna.tsinghua.edu.cn
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple akshare


http://127.0.0.1:8888/?token=a5188b94f0e62a8e340196dcb6580406868ae55abeffa60d

"D:\Cache\jSoft\Python3.8\python.exe" -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple arrow

'''

new_label = {'SecurityID':'代码', 'Symbol':'名称', 'Increase':'涨幅%', 'New':'现价', 'Chg':'涨跌', 'BuyPx':'买价', 'SellPx':'卖价', 'Vol':'总量', 'NowVol':'现量', 'Accer':'涨速%', 'Tor':'换手%', 'Open':'今开', 'High':'最高', 'Low':'最低', 'Pre':'昨收', 'PE':'市盈(动)', 'Amt':'总金额', 'VR':'量比', 'Industry':'细分行业', 'Area':'地区', 'Amp':'振幅%', 'AvgPx':'均价', 'InVol':'内盘', 'OutVol':'外盘', 'CR':'内外比', 'BuyVol':'买量', 'SellVol':'卖量', 'UnknownVol':'未匹配量', 'TradableShare':'流通股(亿)', 'Cmv':'流通市值', 'TmvAB':'AB股总市值', 'DOP':'强弱度%', 'Activation':'活跃度', 'VolPO':'笔均量', 'TorPO':'笔换手', 'UpDays':'连涨天', 'Increase3':'3日涨幅%', 'PEttm':'市盈(TTM)', 'PEstatic':'市盈(静)', 'OpenR':'开盘%', 'High':'最高%', 'LowR':'最低%', 'IncreaseAvg':'均涨幅%', 'IncreaseEntity':'实体涨幅%', 'BackWave':'回头波%', 'AttackWave':'攻击波%', 'FinancialDate':'财务更新', 'IpoDate':'上市日期', 'TotalStockIssue':'总股本(亿)', 'ShareAB':'B/A股(亿)', 'ShareH':'H股(亿)', 'FundAsset':'总资产(亿)', 'NetAssets':'净资产(亿)', 'MinorityInterest':'少数股权(亿)', 'ROA':'资产负债率%', 'LiquefiableAssets ':'流动资产(亿)', 'FixedAssets':'固定资产(亿)', 'IntangibleAssets)':'无形资产(亿)', 'CurrentLiability ':'流动负债(亿)', 'CapitalReserve':'资本公积金(亿)', 'Inventory':'存货(亿)', 'AccountsReceivable':'应收帐款(亿)', 'OperatingIncome -':'营业收入(亿)', 'OperatingCost':'营业成本(亿)', 'OperatingProfit':'营业利润(亿)', 'InvestmentIncome':'投资收益(亿)', 'TNP':'利润总额(亿)', 'NPAT':'税后利润(亿)', 'NetProfit ':'净利润(亿)', 'UDP':'未分利润(亿)', 'OCF':'经营现金流(亿)', 'TCF':'总现金流(亿)', 'ShareHolders':'股东人数', 'SharePC':'人均持股', 'MVPC':'人均市值', 'ProfitR':'利润同比%', 'RevenueR':'收入同比%', 'PB':'市净率', 'PCF':'市现率', 'PSR ':'市销率', 'EPS':'每股收益', 'BVPS':'每股净资', 'ANAV':'调整后净资', 'AFPS':'每股公积', 'UDPPS':'每股未分配', 'ER':'权益比%', 'NPR':'净益率%', 'GM':'毛利率%', 'OROA':'营业利润率%', 'CROE':'净利润率%', 'Code':'交易代码', 'ChoiceDate':'自选日', 'ChoicePx':'自选价', 'ChoiceYield':'自选收益%', 'AmtOpen':'开盘金额', 'SealR':'封成比', 'AmtSeal':'封单额', 'Increase20':'20日涨幅%', 'IncreaseYear':'年初至今%', 'Increase60':'60日涨幅%', 'OpneTUNZ':'开盘换手Z', 'TUNZ':'换手Z', 'CmvZ':'流通市值Z', 'LiquidRZ':'流通比例Z', 'NewDAvg':'现均差%', 'MidForm':'中期形态', 'ShortForm':'短期形态', 'LongForm':'长期形态', 'Beta':'贝塔系数', 'IndicatorTips':'近日指标提示', 'NonNetProfit':'扣非净利润(亿)', 'DividendYield ':'股息率%', 'CashFlowPS':'每股现金流', 'RDcost':'研发费用(亿)', 'Employee':'员工人数'}




import numpy as np
import pandas as pd
df = pd.read_pickle('AStock20200911')

df = pd.read_pickle('AStock20200911A')

df.head(2)

df.columns.values.tolist()

pd.set_option('Display.max_rows',None)  # 展示全部行
pd.set_option('Display.max_columns',None)   # 展示全部列

# df.rename(columns=new_col, inplace = True)

for i, v in df.dtypes.items():
if v == "object":
print(i,v)


# False :  '',(),[],{},None,set(), 0
# print ('True' if True else 'False')

#df 遍历 效率
# DF['eee'] = DF['aaa'].values * DF['bbb'].values  #131us
# DF['eee'] = DF['aaa'] * DF['bbb']  #263us
# DF['eee'] = [ a*b for a,b in zip(DF['aaa'],DF['bbb']) ] #1.1ms
# DF['eee'] = DF[['aaa','bbb']].apply(lambda x: x.aaa * x.bbb, axis=1) #90ms   frame=data.apply(lambda x:x*2)
# DF['eee'] = DF.apply(lambda x: x.aaa * x.bbb, axis=1) #89ms

# kdj_position=df['K']>df['D']
# df.loc[kdj_position[(kdj_position == True) & (kdj_position.shift() == False)].index, 'KDJ_金叉死叉'] = '金叉'

# for index, row in df.iteritems():
# print(index) # 输出列名
# for row in df.iteritems():
# print(row[0], row[1], row[2]) # 输出各列

# for row in df.itertuples(index=True, name='Pandas'):
# print getattr(row, "c1"), getattr(row, "c2")

# for index, rows in DF.iterrows(): #342ms
# rows['eee'] = rows['aaa'] * rows['bbb']

# for i in range(len(DF)): #58ms
# DF.iat[i,4] = DF.iat[i,0] * DF.iat[i,1]

# for i in range(len(DF)): #2s
# DF.iloc[i,4] = DF.iloc[i,0] * DF.iloc[i,1]

# 数据整体描述
# df.shape # 行数 列数 返回表示DataFrame的维度的元组.
# df.team.dtype # 某个字段team的类型
# df.dtypes # 列数据类型
# df.dtypes.value_counts() # 各类型有多少个字段
# df.columns.values.tolist() # 列字段表
# df.ndim  # 轴/数组维度大小
# df.index # 行索引
# df.columns # 列索引
# df.values # 对象值，二维ndarray数组 NDFrame的Numpy表示
# df.size # 元素数
# df.empty # 如果NDFrame完全为空[无项目]，则返回为True; 如果任何轴的长度为0; 注意有空值不认为是空
# df.axes # 获取行及列索引 返回一个列，行轴标签和列轴标签作为唯一的成员
# df.T  # index 与 columns 对调

# df.head(10) # 显示前10行，默认是5行
# df.tail() # 显示末尾几行，默认是5
# df.info() # 相关系数，如行数，列数，列索引、列非空值个数，列类型，内存占用
# df.describe() # 列计算汇总快速统计结果，计数、均值、标准差、最大值、四分数、最小值
# df.sample(3) # 查看 n 个样本，随机
# dfs.keys()  # Series 的索引, DataFrame 的列名

# s.name # 'Q1'
# s.array # 值组成的数组 <PandasArray>
# s.dtype # 类型，dtype('int64')
# s.hasnans # False 是否有空


# 统计函数 简单分析
# df.mean() # 返回所有列的均值，若加上参数
# df.mean(1)则对每一行求平均值
# df.corr() # 返回列与列之间的相关系数
# df.count() # 返回每一列中的非空值的个数 非NA值的数量
# df.max() # 返回每一列的最大值
# df.min() # 返回每一列的最小值
# df.median() # 返回每一列的中位数
# df.std() # 返回每一列的标准差
# df.var() # 样本值的方差
# df.mode() # 众数
# df.sum() # 值的总和
# df.sum(0, skipna=False) # 否要排除缺失数据
# df.sum(level='blooded') # 索引级别
# df.sum(level=0)
# df.sum(min_count=1) # 如果有空值总共算几
# df.media() # 值的算术中位数（50%分位数)
# df.quantile() # 计算样本的分位数（0到 1）   (不同 % 的值)
# df.argmin() # 计算能够获取到最小值的索引位置（整数)
# df.argmax() # 计算能够获取到最大值的索引位置（整数)
# df.idxmin() # 计算能够获取到最小值的索引值
# df.idxmax() # 计算能够获取到最大值的索引值
# df.mad() # 根据平均值计算平均绝对离差
# df.skew() # 样本值的偏度（三阶矩）
# df.kurt() # 样本值的峰度（四阶矩）
# df.cumsum() # 样本值的累计和 df.cumsum(axis=0) # 累积连加,累加
# df.cummin() # 根样本值的累计最小
# df.cummax() # 样本值的累计最大值
# df.cumprod() # 样本值的累计积,累乘
# df.diff() # 计算一阶差分（对时间序列很有用)
# df.pct_change() # 计算百分数变化
# df.abs() # 绝对值
# df.sem() # 平均值的标准误差
# df.prod() # 连乘
# df.mad() # 平均绝对偏差
# df.nunique() # 去重数量，不同值的量

# 位置差值
# df.diff() 本行与前一行的差值（即当前值比上一行增加了多少）
# df.diff(axis=1) # 向右一列  无前一行的本行值为 NaN
# df.diff(2)
# df.diff(-1) # 新的本行为本行减去后一行
# df.shift() # 整体下移一行，最顶的一行为 NaN，不做任何计算，移动后目标位置的类型无法接收收的为 NaN.
# df.shift(3) # 移三行
# df.Q1.head().shift(-1) # 整体上移一行，最底的一行为 NaN
# df.shift(axis=1) # 向右移动一位
# df.shift(3, axis=1) # 移三位
# df.shift(-1, axis=1) # 向左移动一位
# df.Q1 - df.Q1.shift() # 实现了 df.Q1.diff()


# df=df.reset_index(drop=True)
# df.reindex(index=range(df.shape[0]))
# df["index"] = range(len(df))
# df = df.set_index(["index"])

# point_x = [A_x, B_x, C_x, D_x]
# point_y = [A_y, B_y, C_y, D_y]
# points_tulpe = list(zip(point_x, point_y))
# print(points_tulpe)

# df1 = df.values.tolist()


# 生成序号 df.rank()
# df.rank() 可以生成数据的排序值，典例的例子如学生的成绩表，给出排名.

# 排名, 将值变了序号数
# df.rank()
# df.rank(axis=1) # 横向排名
# 相同值的排名处理：
# method='average' 并列第1 计算(1+2)/2=都是1.5，下个是3
# method='max':并列第1，显示2，下个 3
# method='min':并列第1，显示1，下个3
# method='dense':并列第1，显示1，下个 2
# method='first':按索引顺序看谁在索引前
# df.Q1.rank(method='max')
# df.rank(na_option='bottom') # 把空值放在最后
# df.rank(pct=True) # 以百分比形式返回

# 常用函数
# df.all() # 返回所有列all()值的Series
# df.any()

# 用表达式计算生成列.仅支持列，不是太安全
# df.eval('Q2Q3 = Q2 + Q3')
# df.eval('Q2Q3 = Q2 + Q3', inplace=True) # 替换生效

# 四舍五入
# df.round(2) # 指定字段指定保留小数位，如有
# df.round({'Q1': 2, 'Q2': 0})
# df.round(-1) # 保留10位

# 每个列的去重值的数量
# df.nunique()
# s.nunique() # 本列的去重值

# 真假检测
# df.isna() # 值的真假值替换
# df.notna() # 与上相反

# 对 df 整体所有元素做加减乘除等计算：
# df + 1 # 等运算
# df.add() # 加
# df.sub() # 减
# df.mul() # 乘
# df.div() # 除
# df.truediv() # Divide DataFrames (float division).
# df.floordiv() # Divide DataFrames (integer division).
# df.mod() # 模，除后的余数
# df.pow() # 指数幂
# df.dot(df2) # 矩阵运算

# Series 专门函数

# 不重复的值及数量
# s.value_counts()
# s.value_counts(normalize=True) # 重复值的频率
# s.value_counts(sort=False) # 不按频率排序
# s.unique() # 去重的值 array
# s.is_unique # 是否有重复

# 最大最小值
# s.nlargest() # 最大的前5个
# s.nlargest(15) # 最大的前15个
# s.nsmallest() # 最小的前5个
# s.nsmallest(15) # 最小的前15个
# s.pct_change() # 计算与前一行的变化百分比
# s.pct_change(periods=2) # 前两行
# s1.cov(s2) # 两个序列的协方差


# 函数应用
# df.apply(np.cumsum) # 累加

# 导入
# df =pd.read_csv("Counts.csv", header=0)
# pd.read_csv('foo.csv')  # 从 csv 文件读取数据

# 保存到 csv 文件
# df.to_csv('foo.csv')

# 读取 excel 文件
# pd.read_excel('foo.xlsx','Sheet1', index_col=None, na_values=['NA'])
# df.to_excel('foo.xlsx', sheet_name='Sheet1')


# 选择行

# 可以通过将行标签传递给loc函数或者ix函数来选择行
# df.loc['a']
# df.loc[:,'two']
# df.ix['a']

# 按整数位置选择
# 可以通过将整数位置传递给iloc函数来选择行.参考以下示例代码 -
# df.iloc[2]

# 行切片
# 可以使用:运算符选择多行.参考以下示例代码 -
# df[2:4]

# 用人工索引选取
# df[df.index == '000001'] # 指定索引

# 用自然索引选择，类似列表的切片
# df[0:3] # 取前三行,
# df[0:10:2] # 前10个，每两个取一个
# df.iloc[:10,:] # 前10个

# 指定行列
# df.loc['Ben', 'Q1':'Q4'] # 只看 Ben 的四个季度成绩
# df.loc['Eorge':'Alexander', 'team':'Q4'] # 指定行区间

# 行添加
# df.ix['e'] = [22,33,444]
# df.loc['e'] = [22,33,444]
# df = df.append(data2)

# 行删除
# df06 = df06.drop('e')

# 保留表头 清空 DataFrame 空表
# df=df.drop(index=df.index)
# df.drop(df.index, inplace=True)

# 列选择
# df['one']

# 查看指定列
# df['Chg']
# df.Chg # 同上，如果列名符合 python 变量名要求，可使用
# df.set_index('SecurityID', inplace=True) # 建立索引并生效

# 选择多列
# df[['Increase', 'Chg']] # 只看这两列，注意括号
# df.loc[:, ['Increase', 'Chg']] # 和上边效果一样

# 增加列
# df['one'] = 1 # 增加一个固定值的列
# df['total'] = df.Q1 + df.Q2 + df.Q3 + df.Q4 # 增加总成绩列
# # 指定一些列相加增加一个新列
# df['total'] = df.loc[:,'Q1':'Q4'].apply(lambda x:sum(x), axis=1)
# df['total'] = df.sum(axis=1) # 可以把所有为数字的列相加
# df['avg'] = df.total/4 # 增加平均成绩列
# df['three'] = [7,8,9,10] # 直接通过列名进行修改

# 列删除
# del(df['three']) # 使用del删除列
# df.pop('two') # 使用pop删除

# 方法链创建新列
# 方法链（method chains）是一种预计算，并没有改变原变量的数据，同时使用起来非常方便，不需要频繁给变量赋值，后期的数据分析极力推荐这种思路，后期会专门讲解.用 括号 () 是为了方便在多条语句的情况下方便换行对齐.
# 定义一个名为 rate 的新列，并给定计算公式
# (df.assign(rate=df['one']/df['two']).head())
# 可以用 lambda 进行计算，变量 x 是指整个 df
# df.assign(rate=lambda x:x['one']/x['two']).head()
# 可指定多个
# df.assign(rate=lambda x:x['one']/x['two'],
#           rate2=lambda x:x['one']+x['two']).head()


# 选择数据 常用操作
# Operation 	   Syntax 	       Result       e.g.
# 选择列 	        df[col] 	   Series        # df['one']
# 按索引选择行 	    df.loc[label] 	Series       # df.loc['a']
# 按数字索引选择行 	df.iloc[loc] 	Series       # df.iloc[3]
# 使用切片选择行 	df[5:10] 	    DataFrame    # df[1:3]
# 用表达式筛选行 	df[bool_vec] 	DataFrame    # df[df.one > 1]

# 按标签 .loc
# df.loc() 的格式为 df.loc[<索引表达式>, <列表达式>]，表达式支持以下形式：

# 单个标签:
# df.loc[0] # 选择索引为 0 的行
# df.loc[8] # 代表索引，如果是字符需要加引号

# 单个列表标签：
# df.loc[[0,5,10]] # 指定索引 0，5，10 的行
# df.loc[['Eli', 'Ben']] # 如果索引是 name
# df.loc[[False, True]*50] # 为真的列显示，隔一个显示一个 # 真假选择，长度要和索引一样

# 带标签的切片（包括起始和停止）：
# df.loc[0:5] # 索引切片, 代表0-5行，包括5
# df.loc['2010':'2014'] # 如果索引是时间可以用字符查询
# df.loc[:] # 所有 # 本方法支持 Series

# 列筛选，必须有行筛选：
# dft.loc[:, ['Q1', 'Q2']] # 所有行，Q1 和 Q2两列
# dft.loc[:, ['Q1', 'Q2']] # 所有行，Q1 和 Q2两列
# dft.loc[:10, 'Q1':] # 0-10 行，Q1后边的所有列

# 按位置 .iloc
# df.iloc 与 df.loc 相似，但只能用自然索引（行和列的 0 - n 索引），不能用标签.
# df.iloc[:3]
# df.iloc[:]
# df.iloc[2:20:3]
# s.iloc[:3]

# 取具体值 .at
# 类似于 loc, 但仅取一个具体的值，结构为 at[<索引>,<列名>]：
# df.at[4, 'Q1'] # 65 # 注：索引是字符需要加引号
# df.at['lily', 'Q1'] # 65 假定索引是 name
# df.at[0, 'name'] # 'Liver'
# df.loc[0].at['name'] # 'Liver'
# df.set_index('name').at['Eorge', 'team'] # 'C' # 指定列的值对应其他列的值
# df.set_index('name').team.at['Eorge'] # 'C'
# df.team.at[3] # 'C' # 指定列的对应索引的值

# 同样 iat 和 iloc 一样，仅支持数字索引：
# df.iat[4, 2] # 65
# df.loc[0].iat[1] # 'E'

# .get 可以做类似字典的操作，如果无值给返回默认值（例中是0）：
# df.get('name', 0) # 是 name 列
# df.get('nameXXX', 0) # 0, 返回默认值
# s.get(3, 0) # 93, Series 传索引返回具体值
# df.name.get(99, 0) # 'Ben'

# df1 = pd.DataFrame([['Snow','M',22],['Tyrion','M',32],['Sansa','F',18],['Arya','F',14]], columns=['name','gender','age'])

# print("--------更换单个值----------")
# loc和iloc 可以更换单行、单列、多行、多列的值
# df1.loc[0,'age']=25      # 思路：先用loc找到要更改的值，再用赋值（=）的方法实现更换值
# df1.iloc[0,2]=25         # iloc：用索引位置来查找

# at 、iat只能更换单个值
# df1.at[0,'age']=25      # iat 用来取某个单值,参数只能用数字索引
# df1.iat[0,2]=25         # at 用来取某个单值,参数只能用index和columns索引名称
# print(df1)


# 条件选择 表达式筛选

# 单一条件 [] 切片里可以使用表达式进行筛选
# df[df.Q1 > 90] # Q1 列大于90的
# df[df.team == 'C'] # team 列为 'C' 的
# df[df.index == 'Oscar'] # 指定索引即原数据中的 name
# df[df['Q1'] == 8] # Q1 等于8
# df[~(df['Q1'] == 8)] # 不等于8
# df[df.name == 'Ben'] # 姓名为Ben
# df.loc[df['Q1'] > 90, 'Q1':]  # Q1 大于90，只显示 Q1
# df.loc[(df.Q1 > 80) & (df.Q2 < 15)] # and 关系
# df.loc[(df.Q1 > 90) | (df.Q2 < 90)] # or 关系
# df[df.Q1 > df.Q2]

# 组合条件
# df[(df['Q1'] > 90) & (df['team'] == 'C')] # and 关系
# df[df['team'] == 'C'].loc[df.Q1>90] # 多重筛选

# 逻辑判断和函数：
# df.eq() # 等于相等 ==
# df.ne() # 不等于 !=
# df.le() # 小于等于 >=
# df.lt() # 小于 <
# df.ge() # 大于等于 >=
# df.gt() # 大于 >
# 都支持  axis{0 or 'index', 1 or 'columns'}, default 'columns'
# df[df.Q1.ne(89)] # Q1 不等于8
# df.loc[df.Q1.gt(90) & df.Q2.lt(90)] # and 关系 Q1>90 Q2<90

# 其他函数：
# isin
# df[df.team.isin(['A','B'])] # 包含 AB 两组的
# df[df.isin({'team': ['C', 'D'], 'Q1':[36,93]})] # 复杂查询，其他值为 NaN

# 函数筛选
# df[lambda df: df['Q1'] == 8] # Q1为8的
# df.loc[lambda df: df.Q1 == 8, 'Q1':'Q2'] # Q1为8的, 显示 Q1 Q2

# where 和 mask
# s.where(s > 90) # 不符合条件的为 NaN
# s.where(s > 90, 0) # 不符合条件的为 0
# np.where(s>80, True, False) # np.where, 大于80是真否则是假
# np.where(df.num>=60, '合格', '不合格')
# s.mask(s > 90) # 符合条件的为 NaN
# s.mask(s > 90, 0) # 符合条件的为 0

# 例：能被整除的显示，不能的显示相反数
# m = df.loc[:,'Q1':'Q4'] % 3 == 0
# df.loc[:,'Q1':'Q4'].where(m, -df.loc[:,'Q1':'Q4'])

# 行列相同数量，返回一个 array
# df.lookup([1,3,4], ['Q1','Q2','Q3']) # array([36, 96, 61])
# df.lookup([1], ['Q1']) # array([36])

# query
# df.query('Q1 > Q2 > 90') # 直接写类型 sql where 语句
# df.query('Q1 + Q2 > 180')
# df.query('Q1 == Q2')
# df.query('(Q1<50) & (Q2>40) and (Q3>90)')
# df.query('Q1 > Q2 > Q3 > Q4')
# df.query('team != "C"')
# df.query('team not in ("E","A","B")')
# df.query('B == `team name`') # 对于名称中带有空格的列，可以使用反引号引起来

# 支持传入变量，如：大于平均分40分的
# a = df.Q1.mean()
# df.query('Q1 > @a+40')
# df.query('Q1 > `Q2`+@a')

# df.eval() 用法与 df.query 类似
# df[df.eval("Q1 > 90 > Q3 > 10")]
# df[df.eval("Q1 > `Q2`+@a")]

# filter
# 使用 filter 可以对行名和列名进行筛选.
# df.filter(items=['Q1', 'Q2']) # 选择两列
# df.filter(regex='Q', axis=1) # 列名包含Q的
# df.filter(regex='e$', axis=1) # 以 e 结尾的
# df.filter(regex='1$', axis=0) # 正则, 索引名包含1的
# df.filter(like='2', axis=0) # 索引中有2的
# df.filter(regex='^2', axis=0).filter(like='Q', axis=1) # 索引中2开头列名有Q的

# 索引选择器 pd.IndexSlice
# pd.IndexSlice 的使用方法类似于df.loc[] 切片中的方法，常用在多层索引中，以及需要指定应用范围（subset 参数）的函数中，特别是在链式方法中.
# df.loc[pd.IndexSlice[:, ['Q1', 'Q2']]]
# idx = pd.IndexSlice # 变量化使用
# df.loc[idx[:, ['Q1', 'Q2']]]
# df.loc[idx[:, 'Q1':'Q4'], :] # 多索引

# 按数据类型
# 可以只选择或者排除指定类型数据：
# df.select_dtypes(include=['float64']) # 选择 float64 型数据
# df.select_dtypes(include='bool')
# df.select_dtypes(include=['number']) # 只取数字型
# df.select_dtypes(exclude=['int']) # 排除 int 类型
# df.select_dtypes(exclude=['datetime64'])

# 理解筛选原理
# df[<表达式>] 里边的表达式如果单独拿出来，可以看到：
# df.Q1.gt(90)
# out:
# 0     False
# 1     False
# 2     False
# 3      True
# 4     False
#       ...
# Name: Q1, Length: 100, dtype: bool
# 会有一个由真假值组成的数据，筛选后的结果就是为 True 的内容.


# 排序
# df.sort_values(by='Q1') # 按 Q1 列数据升序排列
# df.sort_values(by='Q1', ascending=False) # 降序

# df.sort_values(['team', 'Q1'], ascending=[True, False]) # team 升，Q1 降序

# 分组聚合
# 我们可以实现类似 SQL groupby 那样的数据透视功能：

# df.groupby('team').sum() # 按团队分组对应列相加
# df.groupby('team').mean() # 按团队分组对应列求平均
# 不同列不同的计算方法
# df.groupby('team').agg({'Q1': sum,  # 总和
#                         'Q2': 'count', # 总数
#                         'Q3':'mean', # 平均
#                         'Q4': max}) # 最大值

# 数据转换
# df.groupby('team').sum().T
# df.groupby('team').sum().stack()
# df.groupby('team').sum().unstack()

# 字符处理
# s =pd.Series(['A','B','C','Aaba','Baca', np.nan,'CABA','dog','cat'])
# s.str.lower()

# 文本处理
# 操作方法 .str.
# 方法属性 s.str.lower() etc.
# 切分替换 .str.split('_').str.get(1) .str.replace('^.a|dog', 'XX-XX ', case=False)
# 连接 .str.cat(sep=',')
# 索引 s.str[0]
# 提取子串 .str.extract("(?P[a-zA-Z])")


# 合并
# 使用 concat() 连接 pandas 对象:=
# df =pd.DataFrame(np.random.randn(10,4))
# pieces =[df[:3], df[3:7], df[7:]]
# pd.concat(pieces)

# join 合并：=
# left =pd.DataFrame({'key': ['foo','foo'],'lval': [1,2]})
# right =pd.DataFrame({'key': ['foo','foo'],'rval': [4,5]})
# pd.merge(left, right, on='key')

# 追加
# 在 dataframe 数据后追加行
# df =pd.DataFrame(np.random.randn(8,4), columns=['A','B','C','D'])
# s =df.iloc[3]
# df.append(s, ignore_index=True)
# maTac= maTac.append(maTac2)

# 分组
# 分组常常意味着可能包含以下的几种的操作中一个或多个# 依据一些标准分离数据 对组单独地应用函数 将结果合并到一个数据结构中
# df =pd.DataFrame({'A': ['foo','bar','foo','bar','foo','bar','foo','foo'],
#                   'B': ['one','one','two','three','two','two','one','three'],
#                   'C': np.random.randn(8),
#                   'D': np.random.randn(8)})
# df1 = df.groupby('A').sum()
# df2 = df.groupby(['A','B']).sum()

# 分组时，组内运算
# 代表运算的字符串包括'sum'、'mean'、'min'、'max'、'count'
# pd3 = pd3.groupby('a').agg('sum').reset_index()

# 或者自定义函数
# 或自定义函数不需要参数，则x是serise，如果x有自定参数，则x为DataFrame
# def funname(x,name):
#     print(name)
#     print(type(x),'\n',x)
#     return 2

# pd3 = pd3.groupby('a').agg(funname,'aaa').reset_index()

# 数据透视表
# df =pd.DataFrame({'A': ['one','one','two','three']*3,
#    'B': ['A','B','C']*4,
#    'C': ['foo','foo','foo','bar','bar','bar']*2,
#    'D': np.random.randn(12),
#    'E': np.random.randn(12)})

# 生成数据透视表
# pd.pivot_table(df, values='D', index=['A','B'], columns=['C'])

# 时间序列
# pandas 拥有既简单又强大的频率变换重新采样功能，下面的例子从 1次/秒 转换到了 1次/5分钟：
# rng =pd.date_range('1/1/2012', periods=100, freq='S')
# ts =pd.Series(np.random.randint(0,500,len(rng)), index=rng)
# ts.resample('5Min', how='sum')

# 本地化时区表示
# rng =pd.date_range('3/6/2012 00:00', periods=5, freq='D')
# ts =pd.Series(np.random.randn(len(rng)), rng)
# ts_utc =ts.tz_localize('UTC')

# 转换为周期
# ps =ts.to_period()

# 转换为时间戳
# ps.to_timestamp()

# 分类
# df =pd.DataFrame({"id":[1,2,3,4,5,6],"raw_grade":['a','b','b','a','a','e']})

# 将 raw_grades 转换成 Categoricals 类型
# df["grade"]=df["raw_grade"].astype("category")


# 重命名分类
# df["grade"]=df["grade"].cat.set_categories(["very bad","bad","medium","good","very good"])

# 根据分类的顺序对数据进行排序
# df.sort("grade")


# 数据类型转换

# 数据初始化时指定
# df = pd.DataFrame(data, dtype='float32') # 对所的字段指定类型
# 每个字段分别指定
# df = pd.read_excel(data, dtype={'team': 'string', 'Q1': 'int32'})

# 自动推定类型
# Pandas 可以用以下方法智能地推定各列的数据类型，以下方法不妨一试：

# 自动转换合适的数据类型
# df.convert_dtypes() # 推荐！新的方法，支持 string 类型
# df.infer_objects()

# 按大体类型推定
# m = ['1', 2, 3]
# s = pd.to_numeric(s) # 转成数字
# pd.to_datetime(m) # 转成时间
# pd.to_timedelta(m) # 转成时差
# pd.to_datetime(m, errors='coerce') # 错误处理
# pd.to_numeric(m, errors='ignore')
# pd.to_numeric(m errors='coerce').fillna(0) # 兜底填充
# pd.to_datetime(df[['year', 'month', 'day']]) # 组合成日期

# 最低期望
# pd.to_numeric(m, downcast='integer') # smallest signed int dtype
# array([1, 2, 3], dtype=int8)
# pd.to_numeric(m, downcast='signed') # same as 'integer'
# array([1, 2, 3], dtype=int8)
# pd.to_numeric(m, downcast='unsigned') # smallest unsigned int dtype
# array([1, 2, 3], dtype=uint8)
# pd.to_numeric(m, downcast='float') # smallest float dtype
# array([1., 2., 3.], dtype=float32)

# 应用函数
# df.apply(pd.to_timedelta)

# 类型转换 astype()
# 这也是最常见的数据类型转换方式，各数据类型的介绍可参阅：

# df.dtypes # 查看数据类型
# df.index.astype('int64') # 索引类型转换
# df.astype('int32') # 所有数据转换为 int32
# df.astype({'col1': 'int32'}) # 指定字段转指定类型
# s.astype('int64')
# s.astype('int64', copy=False) # 不与原数据关联
# s.astype(np.uint8)
# df['name'].astype('object')
# data['Q4'].astype('float')
# s.astype('datetime64[ns]')
# data['状态'].astype('bool')

# 转换为时间
# pd.to_datetime() 和 s.astype('datetime64[ns]') 是最简单的时间转换方法.

# 将 89.3% 这样的文本转为浮点数字
# data.rate.apply(lambda x: x.replace('%', '')).astype('float') / 100


# 数据排序

# 索引排序
# sort_index() 可将索引重新排序，意味着每行数据的位置跟着索引而变化.

# s.sort_index() # 升序排列
# df.sort_index() # df 也是按索引进行排序
# df.team.sort_index()
# s.sort_index(ascending=False) # 降序排列
# s.sort_index(inplace=True) # 排序后生效，改变原数据
# s.sort_index(ignore_index=True) # 索引重新0-(n-1) 排, 很有用，可以得到它的排序号
# s.sort_index(na_position='first') # 空值在前，另 'last'
# s.sort_index(level=1) # 如果多层，排一级
# s.sort_index(level=1, sort_remaining=False) # 这层不排

# 行索引排序，表头排序
# df.sort_index(axis=1) # 会把列按列名顺序排列

# 数据值排序
# 数据值的排序主要使用 sort_values()，数值按大小顺序，字符按字母顺序.
# s.sort_values() # 升序
# s.sort_values(ascending=False) # 降序
# s.sort_values(inplace=True) # 修改生效
# s.sort_values(na_position='first') # 空值在前

# df.sort_values(by=['team']) # df 按指定字段顺序
# df.sort_values('Q1')
# df.sort_values(by=['team', 'Q1']) # 按多个字段，先排 team, 在同 team 内再看 Q1
# df.sort_values(by=['team', 'Q1'], ascending=False) # 全降序
# df.sort_values(by=['team', 'Q1'], ascending=[True, False]) # 对应指定team升Q1降
# df.sort_values('team', ignore_index=True) # 索引重新0-(n-1) 排

# 索引和值同时排序
# 有些时间就需要索引和值混合排序，比如先按名字排序同序的再按团队排：
# df.set_index('name', inplace=True)
# df.index.names = ['s_name']
# df.sort_values(by=['s_name', 'team'])  #也适用于多层索引.
# df.set_index('name').sort_values('team').sort_index() # 以下方法也可以实现上述需求，不过要注意顺序

# 以下多层索引示例，a 为一级 a1 为 a 下边的二级索引
# df1.sort_values(by=('a', 'a1'))

# 其他
# 也可以用 nsmallest() 和 nlargest() 来实现排序（只支持数字）：
# s.nsmallest(3) # 最小的三个
# s.nlargest(3) # 最大的三个

# 指定列
# df.nlargest(3, 'Q1')
# df.nlargest(5, ['Q1', 'Q2'])
# df.nsmallest(5, ['Q1', 'Q2'])



# 画图
# df['Q1'].plot() # Q1 成绩的折线分布
# df.loc['Ben','Q1':'Q4'].plot() # ben 四个季度的成绩变化
# df.loc[ 'Ben','Q1':'Q4'].plot.bar() # 柱状图
# df.loc[ 'Ben','Q1':'Q4'].plot.barh() # 横向柱状图
# df.groupby('team').sum().T.plot() # 各 Team
# df.groupby('team').count().Q1.plot.pie() # 各组人数对比

# 直方图
# s = pd.Series(np.random.randint(0,7, size=10))
# s.value_counts()
# ts =pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))
# ts =ts.cumsum()
# ts.plot()

# DataFrame

# df = pd.DataFrame(data=None, index = None, columns = None)
# data: 具体数据，结构化或者同构的 ndarray、可迭代对象、字典或者 DataFrame
# index: 索引，类似数组的对象，支持解包，如果没有指定会自动生成 RangeIndex (0, 1, 2, …, n)
# columns: 列索引，表头，如果没有指定会自动生成 RangeIndex (0, 1, 2, …, n)
# 此外还可以 dtype 指定数据类型，如果未指定，系统会自动推断.

# 创建DateFrame
# df = pd.DataFrame({'国家': ['中国', '美国', '日本'],
#                    '地区': ['亚洲', '北美', '亚洲'],
#                    '人口': [14.33, 3.29, 1.26],
#                    'GDP': [14.22, 21.34, 5.18],
#                   })

# dates =pd.date_range('20130101', periods=6)  # 创建日期索引序
# df =pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))  # 创建Dataframe，其中 index 决定索引序列，columns 决定列名

# 字典创建 DataFrame  Series  一维 ndarray、字典、列表
# df =pd.DataFrame({'A' : 1.,
#    'B': pd.Timestamp('20130102'),
#    'C': pd.Series(1,index=list(range(4)),dtype='float32'),
#    'D': np.array([3]*4,dtype='int32'),
#    'E': pd.Categorical(["test","train","test","train"]),
#    'F':'foo' })

# 从列表创建DataFrame
# data = [1,2,3,4]
# df = pd.DataFrame(data)

# 同构的数组数据
# data = np.zeros((2, ), dtype=[('A', 'i4'), ('B', 'f4'), ('C', 'a10')]) # 创建一个空的 2x3 数组
# data[:] = [(1, 2., 'Hello'), (2, 3., "World")] # 给这个数据填入具体数据值
# pd.DataFrame(data) # 生成 DataFrame
# pd.DataFrame(data, index=['first', 'second']) # 指定索引
# pd.DataFrame(data, columns=['C', 'A', 'B']) # 指定列名

# 从列表字典来创建DataFrame
# data = {'Name':['Tom','Jack','Steve'],'Age':[19,18,20]}
# df = pd.DataFrame(data,index = ['rank1','rank2','rank3'],columns = ['Name','Age','Sex']) # 指定行索引和列索引

# 从字典列表创建数据帧DataFrame
# data = [{'a':1,'b':2},{'a':1,'b':2,'c':3}] # 传递字典列表指定行索引
# df = pd.DataFrame(data)
# df = pd.DataFrame(data,index = ['first','second'])
# df = pd.DataFrame(data,index = ['first','second'],columns = ['a','b','c','d']) # 传递字典列表指定行索引，列索引

# 从系列Series的字典来创建DataFrame
# data = {
#     'one':pd.Series([1,2,3],index = ['a','b','c']),
#     'two':pd.Series([1,2,3,4],index = ['a','b','c','d'])
# }
# df = pd.DataFrame(data)

# 元组组成的字典
# pd.DataFrame({('a', 'b'): {('A', 'B'): 1, ('A', 'C'): 2},
#               ('a', 'a'): {('A', 'C'): 3, ('A', 'B'): 4},
#               ('a', 'c'): {('A', 'B'): 5, ('A', 'C'): 6},
#               ('b', 'a'): {('A', 'C'): 7, ('A', 'B'): 8},
#               ('b', 'b'): {('A', 'D'): 9, ('A', 'B'): 10}}) # 一个双索引的例子

# 由 系列Series 生成
# s1 = pd.Series(['a', 'b', 'c', 'd', 'e']) # 可以将多个同索引的 Series，生成 DataFrame：
# pd.DataFrame(s1)

# 混杂的结构
# 从字典里生成
# pd.DataFrame.from_dict(dict([('A', [1, 2, 3]), ('B', [4, 5, 6])]))
# 从列表、元组、ndarray 中创建
# pd.DataFrame.from_records([(1, 2., b'Hello'), (2, 3., b'World')])
# 列内容为一个字典
# pd.json_normalize(df.col)
# df.col.apply(pd.Series)


# 1.清空一个DataFrame表，保留表头
# 有两种写法

# df=df.drop(index=df.index)
# 或
# df.drop(df.index, inplace=True)

# 2.去掉重复行
# 使用pandas自带的drop_duplicates方法：
# norepeat_df = df.drop_duplicates(subset=['A_ID', 'B_ID'], keep='first')
# 去掉UNIT_ID和KPI_ID列中重复的行，并保留重复出现的行中第一次出现的行
# 补充：
# 当keep=False时，就是去掉所有的重复行
# 当keep='first'时，就是保留第一次出现的重复行
# 当keep='last'时就是保留最后一次出现的重复行。
# （注意，这里的参数是字符串，要加引号！！！）
# https://blog.csdn.net/xueruixuan/article/details/80237248

# 3.去掉NaN行
# 使用pandas自带的dropna()方法：
# 删除表中某行全部为NaN的行 nonan_df = df.dropna(axis=0, how='all') #删除表中某行含有任何NaN的行
# nonan_df = df.dropna(axis=0, how='any')
# 补充：
# 删除行的参数axis = 0
# 删除列的参数axis = 1

# 4.融合数据

# import pandas as pd

# frame=pd.DataFrame([[2,4,1,5],[3,1,4,5],[5,1,4,2]],
# columns=['b','a','d','c'],index=['20201201','20201203','20201202'])

# print(frame)
# print('dataframe根据行索引进行降序排序（排序时默认升序，调节ascending参数）)：')
# print(frame.sort_index(ascending=False))
# print('dataframe根据列索引进行排序：')
# print(frame.sort_index(axis=1))
# print('dataframe根据值进行排序：')
# print(frame.sort_values(by='a'))
# print('通过多个索引进行排序：')
# print(frame.sort_values(by=['a','c']))

# frame2=pd.DataFrame([[4,4,1,5],[6,1,4,5],[10,1,4,2],[2,2,2,2]],
# columns=['b','a','d','c'],
# index=['20201201','20201203','20201202','20201204'])

# print(frame2)
# frameAdd = frame+frame2
# print(frameAdd)
# print('======去除NAN值后数据，求平均=========')
# frameAdd_average = frameAdd/2
# nonan_df = frameAdd_average.dropna(axis=0, how='any')
# print(nonan_df)
# print('======合并的数据=========')
# mergeList = pd.concat([frame,frame2],axis=0)
# print(mergeList)

# print('======合并后的数据去重数据=========')
# norepeat_df = mergeList[~mergeList.index.duplicated(keep='last')]
# print(norepeat_df)

# print('======最终合并后的数据且去重数据=========')
# 先添加的并集数据，相加求平均的数据在后面
# add_averg_and_allData = pd.concat([norepeat_df,nonan_df],axis=0)
# 相合并后的数据
# pd_data = add_averg_and_allData[~add_averg_and_allData.index.duplicated(keep='last')]
# print(pd_data)



# Series

# 创建series
# 一个series是一个一维的数据类型，其中每一个元素都有一个标签.类似于Numpy中元素带标签的数组.其中，标签可以是数字或者字符串
# pd.Series(['a', 'b', 'c', 'd', 'e'])
# pd.Series(('a', 'b', 'c', 'd', 'e'))

# 通过一维数组方式创建
# s = pd.Series([1, 2, 5, np.nan, 6, 8])

# 从ndarray创建一个系列
# data = np.array(['a','b','c','d'])
# ser02 = pd.Series(data)

#指定索引
# data = np.array(['a','b','c','d'])
# ser02 = pd.Series(data,index=[100,101,102,103])
# ser02 = pd.Series(data,index=['name','age','sex','address'])

# 从字典dict创建一个系列
# 字典(dict)可以作为输入传递，如果没有指定索引，则按排序顺序取得字典键以构造索引. 如果传递了索引，索引中与标签对应的数据中的值将被拉出.
# data = {'a':1,'b':2,'c':3}
# ser03 = pd.Series(data)

#指定索引
# data = {'a':1,'b':2,'c':3}
# ser03 = pd.Series(data,index = ['a','b','c','d'])

#标量创建 scalar value
# 一个具体的值，如果不指定索引长度为 1，指定索引后长度为索引的数量，每个索引的值都是它.
# ser04 = pd.Series(5,index = [0,1,2,3])
# pd.Series(5., index=['a', 'b', 'c', 'd', 'e'])

# Series值的获取
# 通过方括号+索引的方式读取对应索引的数据，有可能返回多条数据
# 通过方括号+下标值的方式读取对应下标值的数据，下标值的取值范围为：[0，len(Series.values))；另外下标值也可以是负数，表示从右往左获取数据
# Series获取多个值的方式类似NumPy中的ndarray的切片操作，通过方括号+下标值/索引值+冒号(:)的形式来截取series对象中的一部分数

#检索第一个元素.
# ser05 = pd.Series([1,2,3,4,5],index = ['a','b','c','d','e'])
# ser05[1]
# ser05['a']
# ser05['d']

#检索系列中的前三个元素
# ser05 = pd.Series([1,2,3,4,5],index = ['a','b','c','d','e'])

#通过索引来获取数据
# ser05[:3]
# ser05[::2]
# ser05[4:2:-1]

# 类似ndarray
# s = pd.Series([1,2,3,4,5,6,7,8])
# s[3] # 类似列表切片
# s[2:]
# s.median() # 平均值，包括其他的数学函数
# s[s > s.median()] # 筛选大于平均值的内容
# s[[1, 2, 1]] # 指定索引的内容，括号的列表是索引
# s.dtype # 数据类型
# s.array # 返回值的数列
# s.to_numpy() # 转为 numpy 的 ndarray
# 3 in s # 逻辑运算，检测索引

#通过标签（下标值）来获取数据
# print(ser05['b':'d'])
# ser05['a':'d':2]
# ser05['e':'c':-1]
# ser05[['a','b']]

# 类似字典的操作
# s = pd.Series([14.22, 21.34, 5.18],
#               index=['中国', '美国', '日本'],
#               name='人口')
# s['中国'] # 14.22 # 根 key 进行取值，如果没有报 KeyError
# s['印度'] = 13.54 # 类似字典一样增加一个数据
# '法国' in s # False 逻辑运算，检测索引


# Series的运算
# series = pd.Series({'a':941,'b':431,'c':9327})

# 向量计算和标签对齐
# s = pd.Series([1,2,3,4])
# s + s # 同索引相加，无索引位用 NaN 补齐
# s * 2 # 同索引相乘
# s[1:] + s[:-1] # 选取部分进行计算
# np.exp(s) # 求e的幂次方


#输出大于500的值
# series[series>500]

#计算加
# series+10

#计算减
# series-100

#计算乘
# series*10

#两个系列相加
# ser01 = pd.Series([1,2,3])
# ser02 = pd.Series([4,5,6])
# ser01+ser02

# 计算各个元素的指数e的x次方  e 约等于 2.71828
# np.exp(series)
# np.abs(series)

#sign()计算各个元素的正负号: 1 正数，0：零，-1：负数
# np.sign(series)

# Series自动对齐
# 当多个series对象之间进行运算的时候，如果不同series之间具有不同的索引值，那么运算会自动对齐不同索引值的数据，如果某个series没有某个索引值，那么最终结果会赋值为NaN.

# serA = pd.Series([1,2,3],index = ['a','b','c'])
# serB = pd.Series([4,5,6],index = ['b','c','d'])
# serA+serB

# 不同维度的 pandas 对象也可以做运算，它会自动进行对应，shift 用来做对齐操作.
# s = pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2)
# 对不同维度的 pandas 对象进行减法操作
# df.sub(s, axis='index')

# 名称属性
# Series 可以指定一个名称，如无名称不返回内容（NoneType）.
# s = pd.Series([1,2,3,4], name='数字')
# s.name # '数字'
# s = s.rename("number") # 修改名称
# s2 = s.rename("number") # 修改名称并赋值给一个新变量

# 其他操作
# s = pd.Series([1,2,3,4], name='数字')
# s.add(1) # 每个元素加1 abs()
# s.add_prefix(3) # 给索引前加个3，升位
# s.add_suffix(4) # 同上，在后增加
# s.sum() # 总和
# s.count() # 数量，长度
# s.agg('std') # 聚合，仅返回标准差, 与 s.std() 相同
# s.agg(['min', 'max']) # 聚合，返回最大最小值
# s.align(s2) # 联接
# s.any() # 是否有为假的
# s.all() # 是否全是真
# s.append(s2) # 追加另外一个 Series
# s.apply(lambda x:x+1) # 应用方法
# s.empty # 是否为空
# s3 = s.copy() # 深拷贝


# Numpy

# ndarray 创建
# np.array([1, 2, 3])
# np.array((1, 2, 3)) # 同上

# np.array(((1, 2),(1, 2)))
# np.array(([1, 2],[1, 2])) # 同上

# 使用函数创建
# np.arange(10) # 10个, 不包括10，步长为 1
# np.arange(3, 10, 0.1) # 从 3 开始到时，步长为 0.1
# np.linspace(2.0, 3.0, num=5, endpoint=False) # 从 2.0 开始到 3.0，生成均匀的 5 个值，不包括终终值 3.0
# np.random.randn(6, 4)  # 返回一个 6x4 的随机数组，float 型
# np.random.randint(3,7,size=(2, 4)) # 指定范围指定形状的数组，整型
# np.zeros(6) # 6个浮点 0. # 创建值为 0 的数组
# np.zeros((5, 6), dtype=int) # 5 x 6 整型 0
# np.ones(4) # 同上
# np.empty(4) # 同上
# np.zeros_like(np.arange(6))  # 创建一份和目标结构相同的 0 值数组
# np.ones_like(np.arange(6)) # 同上
# np.empty_like(np.arange(6)) # 同上

# 数组信息
# n.shape() # 数组的形状, 返回值是一个元组
# n.shape = (4, 1) # 改变形状
# a = n.reshape((2,2)) # 改变原数组的形状创建一个新的
# n.dtype # 数据类型
# n.ndim # 维度数
# n.size # 元素数
# np.typeDict # np 所有数据类型

# 计算
# 两个数组间的操作总是应用在每个元素上的.

# np.array( [10,20,30,40] )[:3] # 支持类似列表的切片
# a = np.array( [10,20,30,40] )
# b = np.array( [1, 2, 3, 4] )
# a+b # array([11, 22, 33, 44]) 矩阵相加
# a-1 # array([ 9, 19, 29, 39])
# 4*np.sin(a)

# 以下举例数学函数，还支持非常多的数据函数
# a.max() # 40
# a.min() # 10
# a.sum() # 100
# a.std() # 11.180339887498949
# a.all() # True
# a.cumsum() # array([ 10,  30,  60, 100])
# b.sum(axis=1) # 多维可以指定方向

# 类型
# np.int64 # 有符号 64 位 int 类型
# np.float32 # 标准双精度浮点类型
# np.complex # 由128位的浮点数组成的复数类型
# np.bool # TRUE 和 FALSE 的 bool 类型
# np.object # Python 中的 object 类型
# np.string # 固定长度的 string 类型
# np.unicode # 固定长度的 unicode 类型
# np.NaN # np.float 的子类型
# np.nan


# "D:\Cache\jSoft\Python3.6\python.exe"


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

---------------numpy-----------------------
arr = np.array([1,2,3], dtype=np.float64)
np.zeros((3,6))  np.empty((2,3,2)) np.arange(15)
arr.dtype arr.ndim arr.shape
arr.astype(np.int32) #np.float64 np.string_ np.unicode_
arr * arr arr - arr 1/arr
arr= np.arange(32).reshape((8,4))
arr[1:3, : ]  #正常切片
arr[[1,2,3]]  #花式索引
arr.T   arr.transpose((...))   arr.swapaxes(...) #转置
arr.dot #矩阵内积
np.sqrt(arr)   np.exp(arr)    randn(8)＃正态分布值   np.maximum(x,y)
np.where(cond, xarr, yarr)  ＃当cond为真，取xarr,否则取yarr
arr.mean()  arr.mean(axis=1)   #算术平均数
arr.sum()   arr.std()  arr.var()   #和、标准差、方差
arr.min()   arr.max()   #最小值、最大值
arr.argmin()   arr.argmax()    #最小索引、最大索引
arr.cumsum()    arr.cumprod()   #所有元素的累计和、累计积
arr.all()   arr.any()   # 检查数组中是否全为真、部分为真
arr.sort()   arr.sort(1)   #排序、1轴向上排序
arr.unique()   #去重
np.in1d(arr1, arr2)  #arr1的值是否在arr2中
np.load() np.loadtxt() np.save() np.savez() ＃读取、保存文件
np.concatenate([arr, arr], axis=1)  ＃连接两个arr，按行的方向


---------------pandas-----------------------
ser = Series()     ser = Series([...], index=[...])  #一维数组, 字典可以直接转化为series
ser.values    ser.index    ser.reindex([...], fill_value=0)  #数组的值、数组的索引、重新定义索引
ser.isnull()   pd.isnull(ser)   pd.notnull(ser)   #检测缺失数据
ser.name=       ser.index.name=    #ser本身的名字、ser索引的名字
ser.drop('x') #丢弃索引x对应的值
ser +ser  #算术运算
ser.sort_index()   ser.order()     ＃按索引排序、按值排序
df = DataFrame(data, columns=[...], index=[...]) #表结构的数据结构，既有行索引又有列索引
df.ix['x']  #索引为x的值    对于series，直接使用ser['x']
del df['ly']  #用del删除第ly列
df.T    #转置
df.index.name df.columns.name df.values
df.drop([...])
df + df   df1.add(df2, fill_vaule=0) #算术运算
df -ser   #df与ser的算术运算
f=lambda x: x.max()-x.min()   df.apply(f)
df.sort_index(axis=1, ascending=False)   #按行索引排序
df.sort_index(by=['a','b'])   #按a、b列索引排序
ser.rank()   df.rank(axis=1)  #排序，增设一个排名值
df.sum()   df.sum(axis=1)   #按列、按行求和
df.mean(axis=1, skipna=False)   #求各行的平均值，考虑na的存在
df.idxmax()   #返回最大值的索引
df.cumsum()   #累计求和
df.describe()  ser.describe()   #返回count mean std min max等值
ser.unique()  #去重
ser.value_counts()   df.value_counts()  ＃返回一个series，其索引为唯一值，值为频率
ser.isin(['x', 'y'])  #判断ser的值是否为x,y，得到布尔值
ser.dropna() ser.isnull() ser.notnull() ser.fillna(0)  #处理缺失数据，df相同
df.unstack()   #行列索引和值互换  df.unstack().stack()
df.swaplevel('key1','key2')   #接受两个级别编号或名称，并互换
df.sortlevel(1) #根据级别1进行排序，df的行、列索引可以有两级
df.set_index(['c','d'], drop=False)    #将c、d两列转换为行,因drop为false，在列中仍保留c,d
read_csv   read_table   read_fwf    #读取文件分隔符为逗号、分隔符为制表符('\t')、无分隔符（固定列宽）
pd.read_csv('...', nrows=5) #读取文件前5行
pd.read_csv('...', chunksize=1000) #按块读取，避免过大的文件占用内存
pd.load() #pd也有load方法，用来读取二进制文件
pd.ExcelFile('...xls').parse('Sheet1')  # 读取excel文件中的sheet1
df.to_csv('...csv', sep='|', index=False, header=False) #将数据写入csv文件，以｜为分隔符，默认以，为分隔符, 禁用列、行的标签
pd.merge(df1, df2, on='key', suffixes=('_left', '_right')) #合并两个数据集,类似数据库的inner join, 以二者共有的key列作为键,suffixes将两个key分别命名为key_left、key_right
pd.merge(df1, df2, left_on='lkey', right_on='rkey') #合并，类似数据库的inner join, 但二者没有同样的列名，分别指出，作为合并的参照
pd.merge(df1, df2, how='outer') #合并，但是是outer join；how='left'是笛卡尔积，how='inner'是...;还可以对多个键进行合并
df1.join(df2, on='key', how='outer')  #也是合并
pd.concat([ser1, ser2, ser3], axis=1) #连接三个序列，按行的方向
ser1.combine_first(ser2)   df1.combine_first(df2) #把2合并到1上，并对齐
df.stack() df.unstack()  #列旋转为行、行旋转为列
df.pivot()
df.duplicated()   df.drop_duplicates() #判断是否为重复数据、删除重复数据
df[''].map(lambda x: abs(x)) #将函数映射到df的指定列
ser.replace(-999, np.nan) #将－999全部替换为nan
df.rename(index={}, columns={}, inplace=True) #修改索引，inplace为真表示就地修改数据集
pd.cut(ser, bins)  #根据面元bin判断ser的各个数据属于哪一个区段，有labels、levels属性
df[(np.abs(df)>3).any(1)] #输出含有“超过3或－3的值”的行
permutation  take    #用来进行随机重排序
pd.get_dummies(df['key'], prefix='key')  #给df的所有列索引加前缀key
df[...].str.contains()  df[...].str.findall(pattern, flags=re.IGNORECASE)  df[...].str.match(pattern, flags=...)    df[...].str.get()  #矢量化的字符串函数

----绘图
ser.plot() df.plot() #pandas的绘图工具，有参数label, ax, style, alpha, kind, logy, use_index, rot, xticks, xlim, grid等，详见page257
kind='kde' #密度图
kind='bar' kind='barh' #垂直柱状图、水平柱状图，stacked=True为堆积图
ser.hist(bins=50) #直方图
plt.scatter(x,y) #绘制x,y组成的散点图
pd.scatter_matrix(df, diagonal='kde', color='k', alpha='0.3')  #将df各列分别组合绘制散点图

----聚合分组
groupby() 默认在axis=0轴上分组，也可以在1组上分组；可以用for进行分组迭代
df.groupby(df['key1']) #根据key1对df进行分组
df['key2'].groupby(df['key1'])  #根据key1对key2列进行分组
df['key3'].groupby(df['key1'], df['key2'])  #先根据key1、再根据key2对key3列进行分组
df['key2'].groupby(df['key1']).size() #size()返回一个含有分组大小的series
df.groupby(df['key1'])['data1']  等价于 df['data1'].groupby(df['key1'])
df.groupby(df['key1'])[['data1']]  等价于  df[['data1']].groupby(df['key1'])
df.groupby(mapping, axis=1)  ser(mapping) #定义mapping字典，根据字典的分组来进行分组
df.groupby(len) #通过函数来进行分组，如根据len函数
df.groupby(level='...', axis=1)  #根据索引级别来分组
df.groupby([], as_index=False)   #禁用索引，返回无索引形式的数据
df.groupby(...).agg(['mean', 'std'])   #一次使用多个聚合函数时，用agg方法
df.groupby(...).transform(np.mean)   #transform()可以将其内的函数用于各个分组
df.groupby().apply()  #apply方法会将待处理的对象拆分成多个片段，然后对各片段调用传入的函数，最后尝试将各片段组合到一起

----透视交叉
df.pivot_table(['',''], rows=['',''], cols='', margins=True)  #margins为真时会加一列all
pd.crosstab(df.col1, df.col2, margins=True) #margins作用同上


---------------matplotlib---------------
fig=plt.figure() ＃图像所在的基对象
ax=fig.add_subplot(2,2,1)  #2*2的图像，当前选中第1个
fig, axes = plt.subplots(nrows, nclos, sharex, sharey)  #创建图像，指定行、列、共享x轴刻度、共享y轴刻度
plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)
#调整subplot之间的距离，wspace、hspace用来控制宽度、高度百分比
ax.plot(x, y, linestyle='--', color='g')   #依据x,y坐标画图，设置线型、颜色
ax.set_xticks([...]) ax.set_xticklabels([...]) #设置x轴刻度
ax.set_xlabel('...') #设置x轴名称
ax.set_title('....') ＃设置图名
ax.legend(loc='best') #设置图例， loc指定将图例放在合适的位置
ax.text(x,y, 'hello', family='monospace', fontsize=10) #将注释hello放在x,y处，字体大小为10
ax.add_patch() #在图中添加块
plt.savefig('...png', dpi=400, bbox_inches='tight') #保存图片，dpi为分辨率，bbox＝tight表示将裁减空白部分




------------------------------------------
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
#可以用来绘制地图


-----------------时间序列--------------------------
pd.to_datetime(datestrs)    #将字符串型日期解析为日期格式
pd.date_range('1/1/2000', periods=1000)    #生成时间序列
ts.resample('D', how='mean')   #采样，将时间序列转换成以每天为固定频率的, 并计算均值；how='ohlc'是股票四个指数；
＃重采样会聚合，即将短频率（日）变成长频率（月），对应的值叠加；
＃升采样会插值，即将长频率变为短频率，中间产生新值
ts.shift(2, freq='D')   ts.shift(-2, freq='D') #后移、前移2天
now+Day() now+MonthEnd()
import pytz   pytz.timezone('US/Eastern')   #时区操作，需要安装pytz
pd.Period('2010', freq='A-DEC')   ＃period表示时间区间，叫做时期
pd.PeriodIndex    #时期索引
ts.to_period('M')   #时间转换为时期
pd.rolling_mean(...)    pd.rolling_std(...)   #移动窗口函数－平均值、标准差




判断一个变量是否已经定义
方法一：try except方法：
def isset(v):
try :
type (eval(v))
except :
return  0
else :
return  1

if isset('user_name'):
print 'user_name is defined'
else
print 'user_name is not defined'

方法二：使用命名空间：
'varname' in locals().keys() #获取已定义 对象字典
'varname' in  dir()         #获取已定义 对象列表
vars().has_key('var')       #获取已定义 对象字典


判空
df.empty ，这是 DataFrame 内置的属性，可以看到虽然调用简单，但他是最耗时的
len(df)==0 ，这是通过Python内置len方法判断 DataFrame 的行数，相对来说速度比较快，是第1种的3倍
len(df.index)==0 ，这是判断 DataFrame 的行索引的值数量，这已经到达纳秒级别了，是其中最快的方式




1.识别数据类型 (1. Identifying Data Types)
Find Categorical Data
查找分类数据
list(df.columns[df.dtypes == 'object'])
列表(df.columns [df.dtypes =='object'])
But, Categorical data can exist in Numerical format. eg. , days of a month, months(1C12), waist-size (24C38).
但是，分类数据可以数字格式存在. 例如. ，每月的某几天，几个月(1C12)，腰围(24C38).
2. Distinguish between Numerical and Categorical Data
2.区分数值数据和分类数据
df.nunique().sort_values()
df.nunique().sort_values()
Categorical ― The count of unique values should be 30 or less.
分类-唯一值的计数应小于或等于30.
对数值数据进行运算 (Perform operations on numerical data)
Correlations ― should only be done on numeric variables.
关联-仅应在数字变量上进行.
uniqueCount = df.nunique()
uniqueCount = df.nunique()
numerical_columns = list (uniqueCount [ uniqueCount > 30 ].keys())
numeric_columns =列表(uniqueCount [uniqueCount> 30] .keys())
df[numerical_columns] .corr()
df [numerical_columns] .corr()
对数值数据进行可视化 (Perform Visualisations on numerical data)
Scatter plot should always be feed with numerical data each side.
散点图应始终在两侧馈送数值数据.
2.固定行和列 (2. Fixing the rows and columns)
2.1检查格式 (2.1 Check Formatting)
Check the file format
检查文件格式
pd.read_csv() / pd.read_tsv()
pd.read_csv()/ pd.read_tsv()
2. Check the separator type
2.检查分隔符类型
pd.read_csv(sep = '|')
pd.read_csv(sep ='|')
2.2固定行 (2.2 Fixing rows)
Ignore irrelevant rows ― Rows other than the column name headers , use skiprows = 2
忽略不相关的行-列名标题以外的行，请使用skiprows = 2
Delete summary rows ― We derive such insights from dataset, hence not required.
删除摘要行-我们从数据集中获得此类见解，因此不是必需的.
Delete extra rows ― column number indicator, subsection, new page.
删除多余的行-列号指示符，小节，新页面.
df = df.loc['condition']
df = df.loc ['condition']
2.3固定柱 (2.3 Fixing columns)
Add column names if missing.
如果缺少，请添加列名称.
df.columns.values[i] = 'Column_name'
df.columns.values [i] ='列名'
2. Rename columns consistently: Abbreviations, encoded columns.
2.一致地重命名列：缩写，编码列.
df.rename(columns = {'old_name' : 'new_name'} )
df.rename(columns = {'old_name'：'new_name'})
3. Align misaligned columns -> Manual fix on file.
3.对齐未对齐的列->手动修复文件.
4. Delete columns having less analytical value. Like name, (system-generated)id columns.
4.删除分析值较小的列. 像名称一样，(系统生成的)id列.
df.drop(columns = ['col1', 'col2'])
df.drop(columns = ['col1'，'col2'])
df.drop(['col1', 'col2'], axis = 1)
df.drop(['col1'，'col2']，轴= 1)
5. Split/merge columns to get more understandable data.
5.拆分/合并列以获取更多可理解的数据.
df['avg'] = (df['M1'] + df['M2']) /2 ;
df ['avg'] =(df ['M1'] + df ['M2'])/ 2;
df['fullname'] = df['firstname] + ' ' + df[lastname]
df ['fullname'] = df ['firstname] +''+ df [lastname]
3.插补/删除缺失值 (3. Imputing/removing missing values)
3.1用空值替换空值 (3.1 Replace Empty values with Nulls)
Need to replace the empty strings or object NA,XX with nan
需要用nan替换空字符串或对象NA，XX
df.replace(r'^\s*$', np.NaN, regex=True)
df.replace(r'^ \ s * $'，np.NaN，regex = True)
df.replace(r'NA', np.NaN, regex=True)
df.replace(r'NA'，np.NaN，regex = True)
df.replace(r'XX', np.NaN, regex=True)
df.replace(r'XX'，np.NaN，regex = True)
3.2检查空值 (3.2 Check Nulls)
List null values
列出空值
df.columns[df.isna().any()].tolist()
df.columns [df.isna().any()].tolist()
2. Check the Percentage of nulls
2.检查空值百分比
df.isnull().sum()/len(df)*100).sort_values(ascending = False)
df.isnull().sum()/ len(df)* 100).sort_values(升序= False)
3. Profilers ― Check Null value visualizations.
3.事件探查器―检查“空值”可视化.
3.3删除行/列 (3.3 Removing Rows/Columns)
Removing the outlier data
删除异常数据
df = df[df['field1'] < Outliers]
df = df [df ['field1'] <离群值]
2. Removing data if the target value is missing
2.如果缺少目标值，则删除数据
df1 = df[~df['target-value'].isnull()].copy()
df1 = df [?df ['target-value'].isnull()].copy()
3. High Percentage of nulls
3.高百分比的空值
df.drop(columns = ['col1', 'col2']) / df.drop(['col1', 'col2'], axis = 1)
df.drop(columns = ['col1'，'col2'])/ df.drop(['col1'，'col2']，轴= 1)
df = df.loc[:, df.isnull().mean() < .95]
df = df.loc [:, df.isnull().mean()<.95]
3.4.1缺失值类型 (3.4.1 Missing Values Types)
MCAR: It stands for Missing Completely At Random. The reason behind a missing value is not dependent on any other feature.
MCAR ：代表随机完全丢失. 缺少值的原因不依赖于任何其他功能.
MAR: It stands for Missing At Random. The reason behind a missing value may be associated with some other features.
MAR ：代表“随机失踪”. 缺少值的原因可能与某些其他功能有关.
MNAR: It stands for Missing Not At Random. There is a specific reason behind a missing value.
MNAR ：代表不随机丢失. 缺少值背后有特定原因.
3.4.2缺失值处理 (3.4.2 Missing Values Treatment)
df.[field].fillna() → fill null values
df.[field] .fillna() →填充空值
Replacing all nulls with zeros →
用零替换所有空值→
df.fillna(0)
df.fillna(0)
2. Imputing values using Mean, Median, Mode
2.使用均值，中位数，众数估算值
Mean ― continuous, if data set does not have outliers.
均值-连续(如果数据集没有异常值).
df['field1'].mean()
df ['field1'].mean()
Median ― Has outliers
中位数-有异常值
df['field1'].median()
df ['field1'].median()
Mode ― Max occurrence value, categorical.
模式-最大发生值(绝对).
df['field1'].mode()[0]
df ['field1'].mode()[0]
3. Fill with a relevant value by looking at other columns of the same row.
3.通过查看同一行的其他列来填充相关值.
df['field'] = df.apply(lambda x : transform(x), axis =1)
df ['field'] = df.apply(lambda x：transform(x)，axis = 1)
4.处理异常值 (4. Handling outliers)
Image for post
Q3 = np.percentile(df['field1'], 75)
Q3 = np.percentile(df ['field1']，75)
Q1 = np.percentile(df['field1'], 25)
Q1 = np.percentile(df ['field1']，25)
IQR = Q3 ― Q1
IQR = Q3-Q1
Outliers = Q3 + 1.5 * IQR
离群值= Q3 + 1.5 * IQR
df = df[df['field1'] < Outliers] df['field1'].plot(kind='box')
df = df [df ['field1'] <离群值] df ['field1'].plot(kind ='box')
5.标准化值 (5. Standardising the values)
Standardise Precision ― for better presentation of data. e.g. change 4.5312341 kg to 4.53 kg.
标准化精度 -更好地呈现数据. 例如，将4.5312341公斤更改为4.53公斤.
df['field'] = df['field'] .apply(lambda x : round(x,2))
df ['field'] = df ['field'] .apply(lambda x：round(x，2))
2. Scale Values/Standardise Units ― Ensure all observations under one variable are expressed in a common and consistent unit.
2. 标度值/标准化单位 -确保在一个变量下的所有观察值均以相同且一致的单位表示.
df['field'] = df['field'] .apply(lambda x : transform(x))
df ['field'] = df ['field'] .apply(lambda x：transform(x))
3. Standardise Format ― It is important to standardise the format of other elements such as date and name. e.g., change 23/10/16 to 2016/10/23.
3. 标准化格式―标准化其他元素(例如日期和名称)的格式很重要. 例如，将23/10/16更改为2016/10/23.
df['field1'] = pd.to_datetime(df['field1'], format = '%d%b%Y:%H:%M:%S. %f')
df ['field1'] = pd.to_datetime(df ['field1']，format ='％d％b％Y：％H：％M：％S.％f')
strftimes ― Check the date format from this link.
strftimes-从此链接检查日期格式.
4. Standardise Case ― String variables may take various casing styles, e.g. FULLCAPS, lowercase, Title Case, Sentence case, etc.
4. 标准化大小写 ―字符串变量可以采用各种大小写样式，例如FULLCAPS，小写字母，标题大小写，句子大小写等.
df['field1'] = df['field1'].str.title()
df ['field1'] = df ['field1'].str.title()
5. Remove Characters ― Remove extra characters such as common prefixes/suffixes, leading/trailing/multiple spaces.
5. 删除字符 -删除多余的字符，例如常见的前缀/后缀，前导/后缀/多个空格.
df['field1'] = arrests['field1'].str.replace('$', '' )
df ['field1'] =逮捕['field1'].str.replace('$'，'')
df['field1'] = df['field1'].str.strip()
df ['field1'] = df ['field1'].str.strip()
6.修正无效值 (6. Fixing invalid values)
Encode unicode properly ― In case the data is being read as junk characters, try to change the encoding.
正确编码unicode-如果数据被读取为垃圾字符，请尝试更改编码.
df= pd.read_csv('file.csv',encoding ='cp1252')
df = pd.read_csv('file.csv'，encoding ='cp1252')
2. Convert incorrect data types ― Change the incorrect data types to the correct data types for ease of analysis.
2. 转换不正确的数据类型―将不正确的数据类型更改为正确的数据类型，以便于分析.
df['field1'] = df['field1'].astype(int)
df ['field1'] = df ['field1'].astype(int)
df[toNumFieldsArray] = df[toNumFieldsArray].apply(pd.to_numeric, errors='coerce',axis=1)
df [toNumFieldsArray] = df [toNumFieldsArray] .apply(pd.to_numeric，errors ='coerce'，axis = 1)
3. Correct values that lie beyond the range ― If some values lie beyond the logical range.
3. 纠正超出范围的值-如果某些值超出逻辑范围.
df.['field1'].describe() -> min/max values
df.['field1'].describe()->最小值/最大值
df.['field1'].plot(kind = 'box')
df.['field1'].plot(kind ='box')
4. Correct values that do not belong to the list ― Remove values that do not belong to a list. eg. a data set of blood groups of individuals, strings 'E' and 'F' are invalid values.
4. 纠正不属于列表的值―删除不属于列表的值. 例如. 在个人血型数据集中，字符串“ E”和“ F”是无效值.
df['field'].value_counts()
df ['field'].value_counts()
5. Fix incorrect structure ― Values that do not follow a defined structure can be removed Eg. a phone number of 12 digits is an invalid value.
5. 修复错误的结构-可以删除不遵循定义的结构的值，例如. 12位电话号码是无效值.
df['len'] = df['filed1'].apply(lambda x : len(str(x)))
df ['len'] = df ['filed1'].apply(lambda x：len(str(x)))
6. Validate internal rules ― Internal rules, if present, should be correct and consistent. eg. a product's date of delivery cannot be less than date of purchase.
6. 验证内部规则-内部规则(如果存在)应正确且一致. 例如. 产品的交货日期不能少于购买日期.
df[df['field1']>df['field2']]
df [df ['field1']> df ['field2']]
7.过滤数据 (7. Filtering the data)
Deduplicate data ― Remove identical rows and rows in which some columns are identical.
重复数据删除―删除相同的行和某些列相同的行.
df.drop_duplicates(subset =”field1",keep = first/last, inplace = True)
df.drop_duplicates(subset =“ field1”，keep = first / last，inplace = True)
2. Filter rows ― Filter rows by segment and date period to obtain only rows that are relevant to the analysis.
2. 筛选行―按细分和日期期间筛选行，以仅获取与分析相关的行.
df = df.loc['condition']
df = df.loc ['condition']
3. Filter columns ― Filter columns that are relevant to the analysis.
3. 过滤器列-与分析相关的过滤器列.
df_derived = df.filter(regex = “^COMMON_EXP”, axis = 1)
df_derived = df.filter(regex =“ ^ COMMON_EXP”，轴= 1)
4. Binning Data ― converting a numerical data to categorical.
4. 合并数据 -将数字数据转换为分类数据.
df['field-group'] = pd.cut(df['field1'] , bins=np.linspace (min,max,bin_count))
df ['field-group'] = pd.cut(df ['field1']，bins = np.linspace(min，max，bin_count))

构造函数

DataFrame([data, index, columns, dtype, copy]) #构造数据框


属性和数据
DataFrame.axes                                #index: 行标签；columns: 列标签
DataFrame.as_matrix([columns])                #转换为矩阵
DataFrame.dtypes                              #返回数据的类型
DataFrame.ftypes                              #返回每一列的 数据类型float64:dense
DataFrame.get_dtype_counts()                  #返回数据框数据类型的个数
DataFrame.get_ftype_counts()                  #返回数据框数据类型float64:dense的个数
DataFrame.select_dtypes([include, include])   #根据数据类型选取子数据框
DataFrame.values                              #Numpy的展示方式
DataFrame.axes                                #返回横纵坐标的标签名
DataFrame.ndim                                #返回数据框的纬度
DataFrame.size                                #返回数据框元素的个数
DataFrame.shape                               #返回数据框的形状
DataFrame.memory_usage()                      #每一列的存储

类型转换
DataFrame.astype(dtype[, copy, errors])       #转换数据类型
DataFrame.copy([deep])                        #deep深度复制数据
DataFrame.isnull()                            #以布尔的方式返回空值
DataFrame.notnull()                           #以布尔的方式返回非空值

索引和迭代
DataFrame.head([n])                           #返回前n行数据
DataFrame.at                                  #快速标签常量访问器
DataFrame.iat                                 #快速整型常量访问器
DataFrame.loc                                 #标签定位，使用名称
DataFrame.iloc                                #整型定位，使用数字
DataFrame.insert(loc, column, value)          #在特殊地点loc[数字]插入column[列名]某列数据
DataFrame.iter()                              #Iterate over infor axis
DataFrame.iteritems()                         #返回列名和序列的迭代器
DataFrame.iterrows()                          #返回索引和序列的迭代器
DataFrame.itertuples([index, name])           #Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple.
DataFrame.lookup(row_labels, col_labels)      #Label-based “fancy indexing” function for DataFrame.
DataFrame.pop(item)                           #返回删除的项目
DataFrame.tail([n])                           #返回最后n行
DataFrame.xs(key[, axis, level, drop_level])  #Returns a cross-section (row(s) or column(s)) from the Series/DataFrame.
DataFrame.isin(values)                        #是否包含数据框中的元素
DataFrame.where(cond[, other, inplace, …])    #条件筛选
DataFrame.mask(cond[, other, inplace, …])     #Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other.
DataFrame.query(expr[, inplace])              #Query the columns of a frame with a boolean expression.

二元运算
DataFrame.add(other[,axis,fill_value])        #加法，元素指向
DataFrame.sub(other[,axis,fill_value])        #减法，元素指向
DataFrame.mul(other[, axis,fill_value])       #乘法，元素指向
DataFrame.div(other[, axis,fill_value])       #小数除法，元素指向
DataFrame.truediv(other[, axis, level, …])    #真除法，元素指向
DataFrame.floordiv(other[, axis, level, …])   #向下取整除法，元素指向
DataFrame.mod(other[, axis,fill_value])       #模运算，元素指向
DataFrame.pow(other[, axis,fill_value])       #幂运算，元素指向
DataFrame.radd(other[, axis,fill_value])      #右侧加法，元素指向
DataFrame.rsub(other[, axis,fill_value])      #右侧减法，元素指向
DataFrame.rmul(other[, axis,fill_value])      #右侧乘法，元素指向
DataFrame.rdiv(other[, axis,fill_value])      #右侧小数除法，元素指向
DataFrame.rtruediv(other[, axis, …])          #右侧真除法，元素指向
DataFrame.rfloordiv(other[, axis, …])         #右侧向下取整除法，元素指向
DataFrame.rmod(other[, axis,fill_value])      #右侧模运算，元素指向
DataFrame.rpow(other[, axis,fill_value])      #右侧幂运算，元素指向
DataFrame.lt(other[, axis, level])            #类似Array.lt
DataFrame.gt(other[, axis, level])            #类似Array.gt
DataFrame.le(other[, axis, level])            #类似Array.le
DataFrame.ge(other[, axis, level])            #类似Array.ge
DataFrame.ne(other[, axis, level])            #类似Array.ne
DataFrame.eq(other[, axis, level])            #类似Array.eq
DataFrame.combine(other,func[,fill_value, …]) #Add two DataFrame objects and do not propagate NaN values, so if for a
DataFrame.combine_first(other)                #Combine two DataFrame objects and default to non-null values in frame calling the method.

函数应用&分组&窗口
DataFrame.apply(func[, axis, broadcast, …])   #应用函数
DataFrame.applymap(func)                      #Apply a function to a DataFrame that is intended to operate elementwise, i.e.
DataFrame.aggregate(func[, axis])             #Aggregate using callable, string, dict, or list of string/callables
DataFrame.transform(func, *args, **kwargs)    #Call function producing a like-indexed NDFrame
DataFrame.groupby([by, axis, level, …])       #分组
DataFrame.rolling(window[, min_periods, …])   #滚动窗口
DataFrame.expanding([min_periods, freq, …])   #拓展窗口
DataFrame.ewm([com, span, halflife,  …])      #指数权重窗口

描述统计学
DataFrame.abs()                               #返回绝对值
DataFrame.all([axis, bool_only, skipna])      #Return whether all elements are True over requested axis
DataFrame.any([axis, bool_only, skipna])      #Return whether any element is True over requested axis
DataFrame.clip([lower, upper, axis])          #Trim values at input threshold(s).
DataFrame.clip_lower(threshold[, axis])       #Return copy of the input with values below given value(s) truncated.
DataFrame.clip_upper(threshold[, axis])       #Return copy of input with values above given value(s) truncated.
DataFrame.corr([method, min_periods])         #返回本数据框成对列的相关性系数
DataFrame.corrwith(other[, axis, drop])       #返回不同数据框的相关性
DataFrame.count([axis, level, numeric_only])  #返回非空元素的个数
DataFrame.cov([min_periods])                  #计算协方差
DataFrame.cummax([axis, skipna])              #Return cumulative max over requested axis.
DataFrame.cummin([axis, skipna])              #Return cumulative minimum over requested axis.
DataFrame.cumprod([axis, skipna])             #返回累积
DataFrame.cumsum([axis, skipna])              #返回累和
DataFrame.describe([percentiles,include, …])  #整体描述数据框
DataFrame.diff([periods, axis])               #1st discrete difference of object
DataFrame.eval(expr[, inplace])               #Evaluate an expression in the context of the calling DataFrame instance.
DataFrame.kurt([axis, skipna, level, …])      #返回无偏峰度Fisher's  (kurtosis of normal == 0.0).
DataFrame.mad([axis, skipna, level])          #返回偏差
DataFrame.max([axis, skipna, level, …])       #返回最大值
DataFrame.mean([axis, skipna, level, …])      #返回均值
DataFrame.median([axis, skipna, level, …])    #返回中位数
DataFrame.min([axis, skipna, level, …])       #返回最小值
DataFrame.mode([axis, numeric_only])          #返回众数
DataFrame.pct_change([periods, fill_method])  #返回百分比变化
DataFrame.prod([axis, skipna, level, …])      #返回连乘积
DataFrame.quantile([q, axis, numeric_only])   #返回分位数
DataFrame.rank([axis, method, numeric_only])  #返回数字的排序
DataFrame.round([decimals])                   #Round a DataFrame to a variable number of decimal places.
DataFrame.sem([axis, skipna, level, ddof])    #返回无偏标准误
DataFrame.skew([axis, skipna, level, …])      #返回无偏偏度
DataFrame.sum([axis, skipna, level, …])       #求和
DataFrame.std([axis, skipna, level, ddof])    #返回标准误差
DataFrame.var([axis, skipna, level, ddof])    #返回无偏误差

从新索引&选取&标签操作
DataFrame.add_prefix(prefix)                  #添加前缀
DataFrame.add_suffix(suffix)                  #添加后缀
DataFrame.align(other[, join, axis, level])   #Align two object on their axes with the
DataFrame.drop(labels[, axis, level, …])      #返回删除的列
DataFrame.drop_duplicates([subset, keep, …])  #Return DataFrame with duplicate rows removed, optionally only
DataFrame.duplicated([subset, keep])          #Return boolean Series denoting duplicate rows, optionally only
DataFrame.equals(other)                       #两个数据框是否相同
DataFrame.filter([items, like, regex, axis])  #过滤特定的子数据框
DataFrame.first(offset)                       #Convenience method for subsetting initial periods of time series data based on a date offset.
DataFrame.head([n])                           #返回前n行
DataFrame.idxmax([axis, skipna])              #Return index of first occurrence of maximum over requested axis.
DataFrame.idxmin([axis, skipna])              #Return index of first occurrence of minimum over requested axis.
DataFrame.last(offset)                        #Convenience method for subsetting final periods of time series data based on a date offset.
DataFrame.reindex([index, columns])           #Conform DataFrame to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index.
DataFrame.reindex_axis(labels[, axis, …])     #Conform input object to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index.
DataFrame.reindex_like(other[, method, …])    #Return an object with matching indices to myself.
DataFrame.rename([index, columns])            #Alter axes input function or functions.
DataFrame.rename_axis(mapper[, axis, copy])   #Alter index and / or columns using input function or functions.
DataFrame.reset_index([level, drop, …])       #For DataFrame with multi-level index, return new DataFrame with labeling information in the columns under the index names, defaulting to 'level_0', 'level_1', etc.
DataFrame.sample([n, frac, replace, …])       #返回随机抽样
DataFrame.select(crit[, axis])                #Return data corresponding to axis labels matching criteria
DataFrame.set_index(keys[, drop, append ])    #Set the DataFrame index (row labels) using one or more existing columns.
DataFrame.tail([n])                           #返回最后几行
DataFrame.take(indices[, axis, convert])      #Analogous to ndarray.take
DataFrame.truncate([before, after, axis ])    #Truncates a sorted NDFrame before and/or after some particular index value.

处理缺失值
DataFrame.dropna([axis, how, thresh, …])      #Return object with labels on given axis omitted where alternately any
DataFrame.fillna([value, method, axis, …])    #填充空值
DataFrame.replace([to_replace, value, …])     #Replace values given in 'to_replace' with 'value'.

从新定型&排序&转变形态
DataFrame.pivot([index, columns, values])     #Reshape data (produce a “pivot” table) based on column values.
DataFrame.reorder_levels(order[, axis])       #Rearrange index levels using input order.
DataFrame.sort_values(by[, axis, ascending])  #Sort by the values along either axis
DataFrame.sort_index([axis, level, …])        #Sort object by labels (along an axis)
DataFrame.nlargest(n, columns[, keep])        #Get the rows of a DataFrame sorted by the n largest values of columns.
DataFrame.nsmallest(n, columns[, keep])       #Get the rows of a DataFrame sorted by the n smallest values of columns.
DataFrame.swaplevel([i, j, axis])             #Swap levels i and j in a MultiIndex on a particular axis
DataFrame.stack([level, dropna])              #Pivot a level of the (possibly hierarchical) column labels, returning a DataFrame (or Series in the case of an object with a single level of column labels) having a hierarchical index with a new inner-most level of row labels.
DataFrame.unstack([level, fill_value])        #Pivot a level of the (necessarily hierarchical) index labels, returning a DataFrame having a new level of column labels whose inner-most level consists of the pivoted index labels.
DataFrame.melt([id_vars, value_vars, …])      #“Unpivots” a DataFrame from wide format to long format, optionally
DataFrame.T                                   #Transpose index and columns
DataFrame.to_panel()                          #Transform long (stacked) format (DataFrame) into wide (3D, Panel) format.
DataFrame.to_xarray()                         #Return an xarray object from the pandas object.
DataFrame.transpose(*args, **kwargs)          #Transpose index and columns

Combining& joining&merging
DataFrame.append(other[, ignore_index, …])    #追加数据
DataFrame.assign(**kwargs)                    #Assign new columns to a DataFrame, returning a new object (a copy) with all the original columns in addition to the new ones.
DataFrame.join(other[, on, how, lsuffix, …])  #Join columns with other DataFrame either on index or on a key column.
DataFrame.merge(right[, how, on, left_on, …]) #Merge DataFrame objects by performing a database-style join operation by columns or indexes.
DataFrame.update(other[, join, overwrite, …]) #Modify DataFrame in place using non-NA values from passed DataFrame.

时间序列
DataFrame.asfreq(freq[, method, how, …])      #将时间序列转换为特定的频次
DataFrame.asof(where[, subset])               #The last row without any NaN is taken (or the last row without
DataFrame.shift([periods, freq, axis])        #Shift index by desired number of periods with an optional time freq
DataFrame.first_valid_index()                 #Return label for first non-NA/null value
DataFrame.last_valid_index()                  #Return label for last non-NA/null value
DataFrame.resample(rule[, how, axis, …])      #Convenience method for frequency conversion and resampling of time series.
DataFrame.to_period([freq, axis, copy])       #Convert DataFrame from DatetimeIndex to PeriodIndex with desired
DataFrame.to_timestamp([freq, how, axis])     #Cast to DatetimeIndex of timestamps, at beginning of period
DataFrame.tz_convert(tz[, axis, level, copy]) #Convert tz-aware axis to target time zone.
DataFrame.tz_localize(tz[, axis, level, …])   #Localize tz-naive TimeSeries to target time zone.

作图
DataFrame.plot([x, y, kind, ax, ….])          #DataFrame plotting accessor and method
DataFrame.plot.area([x, y])                   #面积图Area plot
DataFrame.plot.bar([x, y])                    #垂直条形图Vertical bar plot
DataFrame.plot.barh([x, y])                   #水平条形图Horizontal bar plot
DataFrame.plot.box([by])                      #箱图Boxplot
DataFrame.plot.density(**kwds)                #核密度Kernel Density Estimate plot
DataFrame.plot.hexbin(x, y[, C, …])           #Hexbin plot
DataFrame.plot.hist([by, bins])               #直方图Histogram
DataFrame.plot.kde(**kwds)                    #核密度Kernel Density Estimate plot
DataFrame.plot.line([x, y])                   #线图Line plot
DataFrame.plot.pie([y])                       #饼图Pie chart
DataFrame.plot.scatter(x, y[, s, c])          #散点图Scatter plot
DataFrame.boxplot([column, by, ax, …])        #Make a box plot from DataFrame column optionally grouped by some columns or
DataFrame.hist(data[, column, by, grid, …])   #Draw histogram of the DataFrame's series using matplotlib / pylab.

转换为其他格式
DataFrame.from_csv(path[, header, sep, …])    #Read CSV file (DEPRECATED, please use pandas.read_csv() instead).
DataFrame.from_dict(data[, orient, dtype])    #Construct DataFrame from dict of array-like or dicts
DataFrame.from_items(items[,columns,orient])  #Convert (key, value) pairs to DataFrame.
DataFrame.from_records(data[, index, …])      #Convert structured or record ndarray to DataFrame
DataFrame.info([verbose, buf, max_cols, …])   #Concise summary of a DataFrame.
DataFrame.to_pickle(path[, compression, …])   #Pickle (serialize) object to input file path.
DataFrame.to_csv([path_or_buf, sep, na_rep])  #Write DataFrame to a comma-separated values (csv) file
DataFrame.to_hdf(path_or_buf, key, **kwargs)  #Write the contained data to an HDF5 file using HDFStore.
DataFrame.to_sql(name, con[, flavor, …])      #Write records stored in a DataFrame to a SQL database.
DataFrame.to_dict([orient, into])             #Convert DataFrame to dictionary.
DataFrame.to_excel(excel_writer[, …])         #Write DataFrame to an excel sheet
DataFrame.to_json([path_or_buf, orient, …])   #Convert the object to a JSON string.
DataFrame.to_html([buf, columns, col_space])  #Render a DataFrame as an HTML table.
DataFrame.to_feather(fname)                   #write out the binary feather-format for DataFrames
DataFrame.to_latex([buf, columns, …])         #Render an object to a tabular environment table.
DataFrame.to_stata(fname[, convert_dates, …]) #A class for writing Stata binary dta files from array-like objects
DataFrame.to_msgpack([path_or_buf, encoding]) #msgpack (serialize) object to input file path
DataFrame.to_sparse([fill_value, kind])       #Convert to SparseDataFrame
DataFrame.to_dense()                          #Return dense representation of NDFrame (as opposed to sparse)
DataFrame.to_string([buf, columns, …])        #Render a DataFrame to a console-friendly tabular output.
DataFrame.to_clipboard([excel, sep])          #Attempt to write text representation of object to the system clipboard



# import datetime
# import sys
# import tushare as ts
# import backtrader as bt
# import twilio
# from twilio.rest import Client
# import yagmail
# import keyring
# import schedule
# import imbox
# from imbox import Imbox
# import sklearn
# from factor_analyzer import FactorAnalyzer
# import requests
# import arrow
# import pandas.io.data as web
# import pandas_datareader.data as web
# import mpl_finance as mpf
# import yfinance as yf
# import baostock as bs
# import json
# from log import logger
# import smtplib
# from email.mime.text import MIMEText
# from email.mime.multipart import MIMEMultipart
# from email.header import Header
# from email.mime.image import MIMEImage
# import poplib
# from email.parser import Parser
# from email.utils import parseaddr
# import urllib.request
# import urllib.parse
# import urllib3
# import jsonpath
# import ipywidgets as widgets
# from IPython.display import display
# from ipywidgets import Layout, Button, Box, FloatText, Textarea, Dropdown, Label, IntSlider
# from termcolor import colored, cprint
# import threading
# from timeit import timeit
# from apscheduler.schedulers.blocking import BlockingScheduler
# from apscheduler.schedulers.background import BackgroundScheduler
# from sklearn import datasets#引入数据集,sklearn包含众多数据集
# from sklearn.model_selection import train_test_split#将数据分为测试集和训练集
# from sklearn.neighbors import KNeighborsClassifier#利用邻近点方式训练数据
# from sklearn.linear_model import LinearRegression#引入线性回归模型
# from sklearn import preprocessing
# from sklearn.datasets.samples_generator import make_classification
# from sklearn.svm import SVC
# from sklearn.datasets import load_iris
# from sklearn.externals import joblib
# import pickle
# from joblib import dump, load
# import socket
# from pytdx.exhq import TdxExHq_API, TDXParams
# from pytdx.hq import TdxHq_API
# import struct
# import subprocess
# import seaborn as sns
# from scipy.stats import bartlett

一、生成数据表 
1、首先导入pandas库，一般都会用到numpy库，所以我们先导入备用：
import numpy as np
import pandas as pd
2、导入CSV或者xlsx文件：

df = pd.DataFrame(pd.read_csv('name.csv',header=1))
df = pd.DataFrame(pd.read_excel('name.xlsx'))
#pandas还可以读取一下文件： 
read_csv, read_excel, read_hdf, read_sql, read_json, read_msgpack (experimental), read_html, read_gbq (experimental), read_stata, read_sas, read_clipboard, read_pickle; 
#相应的写入： 
to_csv, to_excel, to_hdf, to_sql, to_json, to_msgpack (experimental), to_html, to_gbq (experimental), to_stata, to_clipboard, to_pickle.

（1）pandas.read_csv()参数整理:
读取CSV（逗号分割）文件到DataFrame，也支持文件的部分导入和选择迭代。
filepath_or_buffer : str，pathlib。str, pathlib.Path, py._path.local.LocalPath or any object with a read() method (such as a file handle or StringIO)
可以是URL，可用URL类型包括：http, ftp, s3和文件。
sep : str, default ','
指定分隔符。如果不指定参数，则会尝试使用逗号分隔。分隔符长于一个字符并且不是'\s+',将使用python的语法分析器。并且忽略数据中的逗号。正则表达式例子：'\r\t'
delimiter : str, default None 
定界符，备选分隔符（如果指定该参数，则sep参数失效）。
delim_whitespace : boolean, default False. 
指定空格(例如' '或者' ')是否作为分隔符使用，等效于设定sep='\s+'。如果这个参数设定为Ture那么delimiter 参数失效。在新版本0.18.1支持
header : int or list of ints, default 'infer'
指定行数用来作为列名，数据开始行数。如果文件中没有列名，则默认为0，否则设置为None。如果明确设定header=0 就会替换掉原来存在列名。header参数可以是一个list例如：[0,1,3]，
这个list表示将文件中的这些行作为列标题（意味着每一列有多个标题），介于中间的行将被忽略掉
注意：如果skip_blank_lines=True 那么header参数忽略注释行和空行，所以header=0表示第一行数据而不是文件的第一行。
names : array-like, default None
用于结果的列名列表，如果数据文件中没有列标题行，就需要执行header=None。默认列表中不能出现重复，除非设定参数mangle_dupe_cols=True。
index_col : int or sequence or False, default None
用作行索引的列编号或者列名，如果给定一个序列则有多个行索引。
如果文件不规则，行尾有分隔符，则可以设定index_col=False 来是的pandas不适用第一列作为行索引。
usecols : array-like, default None
返回一个数据子集，该列表中的值必须可以对应到文件中的位置（数字可以对应到指定的列）或者是字符传为文件中的列名。
例如：usecols有效参数可能是 [0,1,2]或者是 ['foo', 'bar', 'baz']。使用这个参数可以加快加载速度并降低内存消耗。
as_recarray : boolean, default False
不赞成使用：该参数会在未来版本移除。请使用pd.read_csv(...).to_records()替代。
返回一个Numpy的recarray来替代DataFrame。如果该参数设定为True。将会优先squeeze参数使用。并且行索引将不再可用，索引列也将被忽略。
squeeze : boolean, default False　　
如果文件值包含一列，则返回一个Series
prefix : str, default None
在没有列标题时，给列添加前缀。例如：添加'X' 成为 X0, X1, ...
mangle_dupe_cols : boolean, default True
重复的列，将'X'...'X'表示为'X.0'...'X.N'。如果设定为false则会将所有重名列覆盖。
dtype : Type name or dict of column -> type, default None
每列数据的数据类型。例如 {'a': np.float64, 'b': np.int32}
engine : {'c', 'python'}, optional
Parser engine to use. The C engine is faster while the python engine is currently more feature-complete.
使用的分析引擎。可以选择C或者是python。C引擎快但是Python引擎功能更加完备。
converters : dict, default None
列转换函数的字典。key可以是列名或者列的序号。
true_values : list, default None
Values to consider as True
false_values : list, default None
Values to consider as False
skipinitialspace : boolean, default False
忽略分隔符后的空白（默认为False，即不忽略）.
skiprows : list-like or integer, default None
需要忽略的行数（从文件开始处算起），或需要跳过的行号列表（从0开始）。
skipfooter : int, default 0
从文件尾部开始忽略。 (c引擎不支持)
skip_footer : int, default 0
不推荐使用：建议使用skipfooter ，功能一样。
nrows : int, default None
需要读取的行数（从文件头开始算起）。
na_values : scalar, str, list-like, or dict, default None
一组用于替换NA/NaN的值。如果传参，需要制定特定列的空值。默认为'1.#IND', '1.#QNAN', 'N/A', 'NA', 'NULL', 'NaN', 'nan'`.
keep_default_na : bool, default True
如果指定na_values参数，并且keep_default_na=False，那么默认的NaN将被覆盖，否则添加。
na_filter : boolean, default True
是否检查丢失值（空字符串或者是空值）。对于大文件来说数据集中没有空值，设定na_filter=False可以提升读取速度。
verbose : boolean, default False
是否打印各种解析器的输出信息，例如：“非数值列中缺失值的数量”等。
skip_blank_lines : boolean, default True
如果为True，则跳过空行；否则记为NaN。
parse_dates : boolean or list of ints or names or list of lists or dict, default False
boolean. True -> 解析索引
list of ints or names. e.g. If [1, 2, 3] -> 解析1,2,3列的值作为独立的日期列；
list of lists. e.g. If [[1, 3]] -> 合并1,3列作为一个日期列使用
dict, e.g. {'foo' : [1, 3]} -> 将1,3列合并，并给合并后的列起名为"foo"
infer_datetime_format : boolean, default False
如果设定为True并且parse_dates 可用，那么pandas将尝试转换为日期类型，如果可以转换，转换方法并解析。在某些情况下会快5~10倍。
keep_date_col : boolean, default False
如果连接多列解析日期，则保持参与连接的列。默认为False。
date_parser : function, default None
用于解析日期的函数，默认使用dateutil.parser.parser来做转换。Pandas尝试使用三种不同的方式解析，如果遇到问题则使用下一种方式。
1.使用一个或者多个arrays（由parse_dates指定）作为参数；
2.连接指定多列字符串作为一个列作为参数；
3.每行调用一次date_parser函数来解析一个或者多个字符串（由parse_dates指定）作为参数。
dayfirst : boolean, default False
DD/MM格式的日期类型
iterator : boolean, default False
返回一个TextFileReader 对象，以便逐块处理文件。
chunksize : int, default None
文件块的大小， See IO Tools docs for more informationon iterator and chunksize.
compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'
直接使用磁盘上的压缩文件。如果使用infer参数，则使用 gzip, bz2, zip或者解压文件名中以'.gz', '.bz2', '.zip', or 'xz'这些为后缀的文件，否则不解压。
如果使用zip，那么ZIP包中国必须只包含一个文件。设置为None则不解压。
新版本0.18.1版本支持zip和xz解压
thousands : str, default None
千分位分割符，如“，”或者“."
decimal : str, default '.'
字符中的小数点 (例如：欧洲数据使用'，').
float_precision : string, default None
Specifies which converter the C engine should use for floating-point values. The options are None for the ordinary converter, high for the high-precision converter, and round_trip for the round-trip converter.
lineterminator : str (length 1), default None
行分割符，只在C解析器下使用。
quotechar : str (length 1), optional
引号，用作标识开始和解释的字符，引号内的分割符将被忽略。
quoting : int or csv.QUOTE_* instance, default 0
控制csv中的引号常量。可选 QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3)
doublequote : boolean, default True
双引号，当单引号已经被定义，并且quoting 参数不是QUOTE_NONE的时候，使用双引号表示引号内的元素作为一个元素使用。
escapechar : str (length 1), default None
当quoting 为QUOTE_NONE时，指定一个字符使的不受分隔符限值。
comment : str, default None
标识着多余的行不被解析。如果该字符出现在行首，这一行将被全部忽略。这个参数只能是一个字符，空行（就像skip_blank_lines=True）注释行被header和skiprows忽略一样。
例如如果指定comment='#' 解析'#empty\na,b,c\n1,2,3' 以header=0 那么返回结果将是以'a,b,c'作为header。
encoding : str, default None
指定字符集类型，通常指定为'utf-8'. List of Python standard encodings
dialect : str or csv.Dialect instance, default None
如果没有指定特定的语言，如果sep大于一个字符则忽略。具体查看csv.Dialect 文档
tupleize_cols : boolean, default False
Leave a list of tuples on columns as is (default is to convert to a Multi Index on the columns)
error_bad_lines : boolean, default True
如果一行包含太多的列，那么默认不会返回DataFrame ，如果设置成false，那么会将改行剔除（只能在C解析器下使用）。
warn_bad_lines : boolean, default True
如果error_bad_lines =False，并且warn_bad_lines =True 那么所有的“bad lines”将会被输出（只能在C解析器下使用）。
low_memory : boolean, default True
分块加载到内存，再低内存消耗中解析。但是可能出现类型混淆。确保类型不被混淆需要设置为False。或者使用dtype 参数指定类型。
注意使用chunksize 或者iterator 参数分块读入会将整个文件读入到一个Dataframe，而忽略类型（只能在C解析器中有效）
buffer_lines : int, default None
不推荐使用，这个参数将会在未来版本移除，因为他的值在解析器中不推荐使用
compact_ints : boolean, default False
不推荐使用，这个参数将会在未来版本移除
如果设置compact_ints=True ，那么任何有整数类型构成的列将被按照最小的整数类型存储，是否有符号将取决于use_unsigned 参数
use_unsigned : boolean, default False
不推荐使用：这个参数将会在未来版本移除
如果整数列被压缩(i.e. compact_ints=True)，指定被压缩的列是有符号还是无符号的。
memory_map : boolean, default False
如果使用的文件在内存内，那么直接map文件使用。使用这种方式可以避免文件再次进行IO操作。
（2）pandas.read_csv()实例：

#read_csv会自动加上行索引，即使原数据集有行索引
obj=pd.read_csv('f:/ceshi.csv')
#获取对应的列值，从列名获取值，header必须从0开始或者默认
obj['列名']
#header=None时，即指明原始文件数据没有列索引，这样read_csv为自动加上列索引，除非你给定列索引的名字。
obj_2=pd.read_csv('f:/ceshi.csv',header=None,names=range(2,5))
#header=0，表示文件第0行（即第一行，python，索引从0开始）为列索引，这样加names会替换原来的列索引。
obj_2=pd.read_csv('f:/ceshi.csv',index_col=0)
#index_col为指定数据中哪一列作为Dataframe的行索引，也可以可指定多列，形成层次索引，默认为None,即不指定行索引，这样系统会自动加上行索引（0-）
obj_2=pd.read_csv('f:/ceshi.csv',index_col=[0,2])
#usecols:可以指定原数据集中，所使用的列。在本例中，共有4列，当usecols=[0,1,2,3]时，即选中所有列，之后令第一列为行索引，当usecols=[1,2,3]时，即从第二列开始，之后令原始数据集的第二列为行索引。
obj_2=pd.read_csv('f:/ceshi.csv',index_col=0,usecols=[0,1,2,3])
#nrows：可以给出从原始数据集中的所读取的行数，目前只能从第一行开始到nrows行
obj_2=pd.read_csv('f:/ceshi.csv',index_col=0,nrows=3)

3、用pandas创建数据表：

df = pd.DataFrame({"id":[1001,1002,1003,1004,1005,1006], 
"date":pd.date_range('20130102', periods=6),
"city":['Beijing ', 'SH', ' guangzhou ', 'Shenzhen', 'shanghai', 'BEIJING '],
"age":[23,44,54,32,34,32],
"category":['100-A','100-B','110-A','110-C','210-A','130-F'],
"price":[1200,np.nan,2133,5433,np.nan,4432]},
columns =['id','date','city','category','age','price'])

二、数据表信息查看 
1、维度查看：
df.shape
2、数据表基本信息（维度、列名称、数据格式、所占空间等）：
df.info()
3、每一列数据的格式：
df.dtypes
4、某一列格式：
df['B'].dtype
5、空值：
df.isnull()
6、查看某一列空值：
df['B'].isnull()
7、查看某一列的唯一值：
df['B'].unique()
8、查看数据表的值： 
df.values 
9、查看列名称：
df.columns
10、查看前10行数据、后10行数据：
df.head() #默认前10行数据
df.tail()    #默认后10 行数据
三、数据表清洗 
1、用数字0填充空值：
df.fillna(value=0)
2、使用列prince的均值对NA进行填充：
df['prince'].fillna(df['prince'].mean())
3、清楚city字段的字符空格：
df['city']=df['city'].map(str.strip)
4、大小写转换：
df['city']=df['city'].str.lower()
5、更改数据格式：
df['price'].astype('int')       
6、更改列名称：
df.rename(columns={'category': 'category-size'}) 
7、删除后出现的重复值：
df['city'].drop_duplicates()
8、删除先出现的重复值：
df['city'].drop_duplicates(keep='last')
9、数据替换：
df['city'].replace('sh', 'shanghai')
四、数据预处理
df1=pd.DataFrame({"id":[1001,1002,1003,1004,1005,1006,1007,1008], 
"gender":['male','female','male','female','male','female','male','female'],
"pay":['Y','N','Y','Y','N','Y','N','Y',],
"m-point":[10,12,20,40,40,40,30,20]})
1、数据表合并
df_inner=pd.merge(df,df1,how='inner')  # 匹配合并，交集
df_left=pd.merge(df,df1,how='left')        #
df_right=pd.merge(df,df1,how='right')
df_outer=pd.merge(df,df1,how='outer')  #并集
2、设置索引列
df_inner.set_index('id')
3、按照特定列的值排序：
df_inner.sort_values(by=['age'])
4、按照索引列排序：
df_inner.sort_index()
5、如果prince列的值>3000，group列显示high，否则显示low：
df_inner['group'] = np.where(df_inner['price'] > 3000,'high','low')
6、对复合多个条件的数据进行分组标记
df_inner.loc[(df_inner['city'] == 'beijing') & (df_inner['price'] >= 4000), 'sign']=1
7、对category字段的值依次进行分列，并创建数据表，索引值为df_inner的索引列，列名称为category和size
pd.DataFrame((x.split('-') for x in df_inner['category']),index=df_inner.index,columns=['category','size']))
8、将完成分裂后的数据表和原df_inner数据表进行匹配
df_inner=pd.merge(df_inner,split,right_index=True, left_index=True)
五、数据提取 
主要用到的三个函数：loc,iloc和ix，loc函数按标签值进行提取，iloc按位置进行提取，ix可以同时按标签和位置进行提取。 
1、按索引提取单行的数值
df_inner.loc[3]
2、按索引提取区域行数值
df_inner.iloc[0:5]
3、重设索引
df_inner.reset_index()
4、设置日期为索引
df_inner=df_inner.set_index('date')
5、提取4日之前的所有数据
df_inner[:'2013-01-04']
6、使用iloc按位置区域提取数据
df_inner.iloc[:3,:2] #冒号前后的数字不再是索引的标签名称，而是数据所在的位置，从0开始，前三行，前两列。
7、适应iloc按位置单独提起数据
df_inner.iloc[[0,2,5],[4,5]] #提取第0、2、5行，4、5列
8、使用ix按索引标签和位置混合提取数据
df_inner.ix[:'2013-01-03',:4] #2013-01-03号之前，前四列数据
9、判断city列的值是否为北京
df_inner['city'].isin(['beijing'])
10、判断city列里是否包含beijing和shanghai，然后将符合条件的数据提取出来
df_inner.loc[df_inner['city'].isin(['beijing','shanghai'])] 
11、提取前三个字符，并生成数据表
pd.DataFrame(category.str[:3])
六、数据筛选 
使用与、或、非三个条件配合大于、小于、等于对数据进行筛选，并进行计数和求和。 
1、使用“与”进行筛选
df_inner.loc[(df_inner['age'] > 25) & (df_inner['city'] == 'beijing'), ['id','city','age','category','gender']]
2、使用“或”进行筛选
df_inner.loc[(df_inner['age'] > 25) | (df_inner['city'] == 'beijing'), ['id','city','age','category','gender']].sort(['age']) 
3、使用“非”条件进行筛选
df_inner.loc[(df_inner['city'] != 'beijing'), ['id','city','age','category','gender']].sort(['id']) 
4、对筛选后的数据按city列进行计数
df_inner.loc[(df_inner['city'] != 'beijing'), ['id','city','age','category','gender']].sort(['id']).city.count()
5、使用query函数进行筛选
df_inner.query('city == ["beijing", "shanghai"]')
6、对筛选后的结果按prince进行求和
df_inner.query('city == ["beijing", "shanghai"]').price.sum()
七、数据汇总 主要函数是groupby和pivote_table 
1、对所有的列进行计数汇总
df_inner.groupby('city').count()
2、按城市对id字段进行计数
df_inner.groupby('city')['id'].count()
3、对两个字段进行汇总计数
df_inner.groupby(['city','size'])['id'].count()
4、对city字段进行汇总，并分别计算prince的合计和均值
df_inner.groupby('city')['price'].agg([len,np.sum, np.mean]) 
八、数据统计
数据采样，计算标准差，协方差和相关系数 
1、简单的数据采样
df_inner.sample(n=3) 
2、手动设置采样权重
weights = [0, 0, 0, 0, 0.5, 0.5]
df_inner.sample(n=2, weights=weights) 
3、采样后不放回
df_inner.sample(n=6, replace=False) 
4、采样后放回
df_inner.sample(n=6, replace=True)
5、 数据表描述性统计
df_inner.describe().round(2).T #round函数设置显示小数位，T表示转置
6、计算列的标准差
df_inner['price'].std()
7、计算两个字段间的协方差
df_inner['price'].cov(df_inner['m-point'])
8、数据表中所有字段间的协方差
df_inner.cov()
9、两个字段的相关性分析
df_inner['price'].corr(df_inner['m-point']) #相关系数在-1到1之间，接近1为正相关，接近-1为负相关，0为不相关
10、数据表的相关性分析
df_inner.corr()
九、数据输出 
分析后的数据可以输出为xlsx格式和csv格式 
1、写入Excel
df_inner.to_excel('excel_to_python.xlsx', sheet_name='bluewhale_cc') 
2、写入到CSV
df_inner.to_csv('excel_to_python.csv') 



  
pandas 常用函数清单
文件读取
df = pd.read_csv(path='file.csv')
参数：header=None  用默认列名，0，1，2，3...
     names=['A', 'B', 'C'...] 自定义列名
     index_col='A'|['A', 'B'...]  给索引列指定名称，如果是多重索引，可以传list
     skiprows=[0,1,2] 需要跳过的行号，从文件头0开始，skip_footer从文件尾开始
     nrows=N 需要读取的行数，前N行
     chunksize=M 返回迭代类型TextFileReader，每M条迭代一次，数据占用较大内存时使用
     sep=':'数据分隔默认是','，根据文件选择合适的分隔符，如果不指定参数，会自动解析
     skip_blank_lines=False 默认为True，跳过空行，如果选择不跳过，会填充NaN
     converters={'col1', func} 对选定列使用函数func转换，通常表示编号的列会使用（避免转换成int）
     
dfjs = pd.read_json('file.json')  可以传入json格式字符串
dfex = pd.read_excel('file.xls', sheetname=[0,1..]) 读取多个sheet页，返回多个df的字典
数据预处理
df.duplicated()           返回各行是否是上一行的重复行
df.drop_duplicates()      删除重复行，如果需要按照列过滤，参数选填['col1', 'col2',...]
df.fillna(0)              用实数0填充na
df.dropna()               axis=0|1  0-index 1-column
                          how='all'|'any' all-全部是NA才删  any-只要有NA就全删
del df['col1']            直接删除某一列              
df.drop(['col1',...], axis=1, inplace=True)   删除指定列，也可以删除行                          
df.column = col_lst       重新制定列名
df.rename(index={'row1':'A'},   重命名索引名和列名
          columns={'col1':'A1'})  
df.replace(dict)          替换df值，前后值可以用字典表，{1:'A', '2':'B'}

def get_digits(str):
    m = re.match(r'(\d+(\.\d+)?)', str.decode('utf-8'))
    if m is not None:   
        return float(m.groups()[0])
    else:
        return 0
df.apply(get_digits)      DataFrame.apply，只获取小数部分，可以选定某一列或行
df['col1'].map(func)      Series.map，只对列进行函数转换

pd.merge(df1, df2, on='col1', 
         how='inner'，sort=True) 合并两个DataFrame，按照共有的某列做内连接（交集），outter为外连接（并集），结果排序
         
pd.merge(df1, df2, left_on='col1', 
         right_on='col2')   df1 df2没有公共列名，所以合并需指定两边的参考列


pd.concat([sr1, sr2, sr3,...], axis=0) 多个Series堆叠成多行，结果仍然是一个Series
pd.concat([sr1, sr2, sr3,...], axis=1) 多个Series组合成多行多列，结果是一个DataFrame，索引取并集，没有交集的位置填入缺省值NaN
 
df1.combine_first(df2)   用df2的数据补充df1的缺省值NaN，如果df2有更多行，也一并补上

df.stack()              列旋转成行，也就是列名变为索引名，原索引变成多层索引，结果是具有多层索引的Series，实际上是把数据集拉长

df.unstack()            将含有多层索引的Series转换为DataFrame，实际上是把数据集压扁，如果某一列具有较少类别，那么把这些类别拉出来作为列
df.pivot()              实际上是unstack的应用，把数据集压扁

pd.get_dummies(df['col1'], prefix='key') 某列含有有限个值，且这些值一般是字符串，例如国家，借鉴位图的思想，可以把k个国家这一列量化成k列，每列用0、1表示
数据筛选
df.columns             列名，返回Index类型的列的集合
df.index               索引名，返回Index类型的索引的集合
df.shape               返回tuple，行x列
df.head(n=N)           返回前N条
df.tail(n=M)           返回后M条
df.values              值的二维数组，以numpy.ndarray对象返回
df.index               DataFrame的索引，索引不可以直接赋值修改
df.reindex(index=['row1', 'row2',...]
           columns=['col1', 'col2',...]) 根据新索引重新排序
df[m:n]                切片，选取m~n-1行
df[df['col1'] > 1]     选取满足条件的行
df.query('col1 > 1')   选取满足条件的行
df.query('col1==[v1,v2,...]') 
df.ix[:,'col1']        选取某一列
df.ix['row1', 'col2']  选取某一元素
df.ix[:,:'col2']       切片选取某一列之前（包括col2）的所有列
df.loc[m:n]            获取从m~n行（推荐）
df.iloc[m:n]           获取从m~n-1行
df.loc[m:n-1,'col1':'coln']   获取从m~n行的col1~coln列


sr=df['col']           取某一列，返回Series
sr.values              Series的值，以numpy.ndarray对象返回
sr.index               Series的索引，以index对象返回
数据运算与排序
df.T                   DataFrame转置
df1 + df2              按照索引和列相加，得到并集，NaN填充
df1.add(df2, fill_value=0) 用其他值填充
df1.add/sub//mul/div   四则运算的方法
df - sr                DataFrame的所有行同时减去Series
df * N                 所有元素乘以N
df.add(sr, axis=0)     DataFrame的所有列同时减去Series


sr.order()             Series升序排列
df.sort_index(axis=0, ascending=True) 按行索引升序
df.sort_index(by=['col1', 'col2'...])  按指定列优先排序
df.rank()              计算排名rank值
数学统计
sr.unique             Series去重
sr.value_counts()     Series统计频率，并从大到小排序，DataFrame没有这个方法
sr.describe()         返回基本统计量和分位数

df.describe()         按各列返回基本统计量和分位数
df.count()            求非NA值得数量
df.max()              求最大值
df.min()              求最大值
df.sum(axis=0)        按各列求和
df.mean()             按各列求平均值
df.median()           求中位数
df.var()              求方差
df.std()              求标准差
df.mad()              根据平均值计算平均绝对利差
df.cumsum()           求累计和
sr1.corr(sr2)         求相关系数
df.cov()              求协方差矩阵
df1.corrwith(df2)     求相关系数

pd.cut(array1, bins)  求一维数据的区间分布
pd.qcut(array1, 4)    按指定分位数进行区间划分，4可以替换成自定义的分位数列表   

df['col1'].groupby(df['col2']) 列1按照列2分组，即列2作为key
df.groupby('col1')    DataFrame按照列1分组
grouped.aggreagte(func) 分组后根据传入函数来聚合
grouped.aggregate([f1, f2,...]) 根据多个函数聚合，表现成多列，函数名为列名
grouped.aggregate([('f1_name', f1), ('f2_name', f2)]) 重命名聚合后的列名
grouped.aggregate({'col1':f1, 'col2':f2,...}) 对不同的列应用不同函数的聚合，函数也可以是多个


df.pivot_table(['col1', 'col2'], 
               rows=['row1', 'row2'], 
               aggfunc=[np.mean, np.sum]
               fill_value=0,
               margins=True)  根据row1, row2对col1， col2做分组聚合，聚合方法可以指定多种，并用指定值替换缺省值
               
          
pd.crosstab(df['col1'], df['col2']) 交叉表，计算分组的频率
  



导入库包
import pandas as pd  # 最新为 1.4.1 版本 (2022-02-12)
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

# 导入数据
pd.read_csv('file.csv', name=['列名','列名2']) # 从 CSV 文件导入数据
pd.read_table(filename, header=0) # 从限定分隔符的文本文件导入数据
pd.read_excel('file.xlsx', sheet_name=' 表1', header=0) # Excel 导入，指定 sheet 和表头
pd.read_sql(query, connection_object) # 从 SQL 表/库导入数据
pd.read_json(json_string) # 从 JSON 格式的字符串导入数据
pd.read_html(url) # 解析 URL、字符串或者 HTML 文件，抽取其中的 tables 表格
pd.read_clipboard() # 从你的粘贴板获取内容，并传给 read_table()
pd.DataFrame(dict) # 从字典对象导入数据，Key 是列名，Value是数据
from io import StringIO
pd.read_csv(StringIO(web_data.text)) # 导入字符串

# 导出输出数据

df.to_csv('filename.csv')  # 导出数据到CSV文件
df.to_excel('filename.xlsx', index=True) # 导出数据到Excel文件
df.to_sql(table_name, connection_object) # 导出数据到 SQL 表
df.to_json(filename)  # 以Json格式导出数据到文本文件
df.to_html()  # 显示 HTML 代码
df.to_markdown() # 显示 markdown 代码
df.to_string() # 显示格式化字符
df.to_latex(index=False) # LaTeX tabular, longtable
df.to_dict('split') # 字典, 格式 list/series/records/index
df.to_clipboard(sep=',', index=False) # 存入系统剪贴板

# 将两个表格输出到一个excel文件里面,导出到多个 sheet
writer=pd.ExcelWriter('new.xlsx')
df_1.to_excel(writer,sheet_name='第一个', index=False)
df_2.to_excel(writer,sheet_name='第二个', index=False)
writer.save() # 必须运行writer.save()，不然不能输出到本地

# 写法2
with pd.ExcelWriter('new.xlsx') as writer:
    df1.to_excel(writer, sheet_name='第一个')
    df2.to_excel(writer, sheet_name='第二个')
# 用 xlsxwriter 导出支持合并单元格、颜色、图表等定制功能
# https://xlsxwriter.readthedocs.io/working_with_pandas.html
创建测试对象
# 创建20行5列的随机数组成的 DataFrame 对象
pd.DataFrame(np.random.rand(20,5))
# 从可迭代对象 my_list 创建一个 Series 对象
pd.Series(my_list)
# 增加一个日期索引
df.index = pd.date_range('1900/1/30', periods=df.shape[0])
# 创建随机数据集
df = pd.util.testing.makeDataFrame()
# 创建随机日期索引数据集
df = pd.util.testing.makePeriodFrame()
df = pd.util.testing.makeTimeDataFrame()
# 创建随机混合类型数据集
df = pd.util.testing.makeMixedDataFrame()
查看、检查、统计、属性
df.head(n) # 查看 DataFrame 对象的前n行
df.tail(n) # 查看 DataFrame 对象的最后n行
df.sample(n) # 查看 n 个样本，随机
df.shape # 查看行数和列数
df.info() # 查看索引、数据类型和内存信息
df.describe() # 查看数值型列的汇总统计
df.dtypes # 查看各字段类型
df.axes # 显示数据行和列名
df.mean() # 返回所有列的均值
df.mean(1) # 返回所有行的均值，下同
df.corr() # 返回列与列之间的相关系数
df.count() # 返回每一列中的非空值的个数
df.max() # 返回每一列的最大值
df.min() # 返回每一列的最小值
df.median() # 返回每一列的中位数
df.std() # 返回每一列的标准差
df.var() # 方差
s.mode() # 众数
s.prod() # 连乘
s.cumprod() # 累积连乘,累乘
df.cumsum(axis=0) # 累积连加,累加
s.nunique() # 去重数量，不同值的量
df.idxmax() # 每列最大的值的索引名
df.idxmin() # 最小
df.columns # 显示所有列名
df.team.unique() # 显示列中的不重复值
# 查看 Series 对象的唯一值和计数, 计数占比: normalize=True
s.value_counts(dropna=False)
# 查看 DataFrame 对象中每一列的唯一值和计数
df.apply(pd.Series.value_counts)
df.duplicated() # 重复行
df.drop_duplicates() # 删除重复行
# set_option、reset_option、describe_option 设置显示要求
pd.get_option()
# 设置行列最大显示数量，None 为不限制
pd.options.display.max_rows = None
pd.options.display.max_columns = None
df.col.argmin() # 最大值[最小值 .argmax()] 所在位置的自动索引
df.col.idxmin() # 最大值[最小值 .idxmax()] 所在位置的定义索引
# 累计统计
ds.cumsum() # 前边所有值之和
ds.cumprod() # 前边所有值之积
ds.cummax() # 前边所有值的最大值
ds.cummin() # 前边所有值的最小值
# 窗口计算(滚动计算)
ds.rolling(x).sum() #依次计算相邻x个元素的和
ds.rolling(x).mean() #依次计算相邻x个元素的算术平均
ds.rolling(x).var() #依次计算相邻x个元素的方差
ds.rolling(x).std() #依次计算相邻x个元素的标准差
ds.rolling(x).min() #依次计算相邻x个元素的最小值
ds.rolling(x).max() #依次计算相邻x个元素的最大值
数据清理
df.columns = ['a','b','c'] # 重命名列名
df.columns = df.columns.str.replace(' ', '_') # 列名空格换下划线
df.loc[df.AAA >= 5, ['BBB', 'CCC']] = 555 # 替换数据
df['pf'] = df.site_id.map({2: '小程序', 7:'M 站'}) # 将枚举换成名称
pd.isnull() # 检查DataFrame对象中的空值，并返回一个 Boolean 数组
pd.notnull() # 检查DataFrame对象中的非空值，并返回一个 Boolean 数组
df.drop(['name'], axis=1, inplace=True) # 删除列
df.drop([0, 10], axis=0) # 删除行
del df['name'] # 删除列
df.dropna() # 删除所有包含空值的行
df.dropna(axis=1) # 删除所有包含空值的列
df.dropna(axis=1,thresh=n) # 删除所有小于 n 个非空值的行
df.fillna(x) # 用x替换DataFrame对象中所有的空值
df.fillna(value={'prov':'未知'}) # 指定列的空值替换为指定内容
s.astype(float) # 将Series中的数据类型更改为 float 类型
df.index.astype('datetime64[ns]') # 转化为时间格式
s.replace(1, 'one') # 用 'one' 代替所有等于 1 的值
s.replace([1, 3],['one','three']) # 用'one'代替 1，用 'three' 代替 3
df.rename(columns=lambda x: x + 1) # 批量更改列名
df.rename(columns={'old_name': 'new_name'}) # 选择性更改列名
df.set_index('column_one') # 更改索引列
df.rename(index=lambda x: x + 1) # 批量重命名索引
# 重新命名表头名称
df.columns = ['UID', '当前待打款金额', '认证姓名']
df['是否设置提现账号'] = df['状态'] # 复制一列
df.loc[:, ::-1] # 列顺序反转
df.loc[::-1] # 行顺序反转, 下方为重新定义索引
df.loc[::-1].reset_index(drop=True)
数据处理：Filter、Sort
# 保留小数位，四舍六入五成双
df.round(2) # 全部
df.round({'A': 1, 'C': 2}) # 指定列
df['Name'] = df.Name # 取列名的两个方法
df[df.index == 'Jude'] # 按索引查询要用 .index
df[df[col] > 0.5] # 选择col列的值大于0.5的行
# 多条件查询
df[(df['team'] == 'A') &
   ( df['Q1'] > 80) &
   df.utype.isin(['老客', '老访客'])]
# 筛选为空的内容
df[df.order.isnull()]
# 类似 SQL where in
df[df.team.isin('A','B')]
df[(df.team=='B') & (df.Q1 == 17)]
df[~(df['team'] == 'A') | ( df['Q1'] > 80)] # 非，或
df[df.Name.str.contains('张')] # 包含字符
df.sort_values(col1) # 按照列col1排序数据，默认升序排列
df.col1.sort_values() # 同上, -> s
df.sort_values(col2, ascending=False) # 按照列 col1 降序排列数据
# 先按列col1升序排列，后按col2降序排列数据
df.sort_values([col1,col2], ascending=[True,False])
df2 = pd.get_dummies(df, prefix='t_') # 将枚举的那些列带枚举转到列上
s.set_index().plot()
# 多索引处理
dd.set_index(['utype', 'site_id', 'p_day'], inplace=True)
dd.sort_index(inplace=True) # 按索引排序
dd.loc['新访客', 2, '2019-06-22'].plot.barh() # loc 中按顺序指定索引内容
# 前100行, 不能指定行，如：df[100]
df[:100]
# 只取指定行
df1 = df.loc[0:, ['设计师ID', '姓名']]
# 将ages平分成5个区间并指定 labels
ages = np.array([1,5,10,40,36,12,58,62,77,89,100,18,20,25,30,32])
pd.cut(ages, [0,5,20,30,50,100],
       labels=[u"婴儿",u"青年",u"中年",u"壮年",u"老年"])

daily_index.difference(df_work_day.index) # 取出差别
# 格式化
df.index.name # 索引的名称 str
df.columns.tolist()
df.values.tolist()
df.总人口.values.tolist()
data.apply(np.mean) # 对 DataFrame 中的每一列应用函数 np.mean
data.apply(np.max,axis=1) # 对 DataFrame 中的每一行应用函数 np.max
df.insert(1, 'three', 12, allow_duplicates=False) # 插入列 (位置、列名、[值])
df.pop('class') # 删除列
# 增加一行
df.append(pd.DataFrame({'one':2,
                        'two':3,
                        'three': 4.4},
                       index=['f']),
          sort=True)
# 指定新列
iris.assign(sepal_ratio=iris['SepalWidth'] / iris['SepalLength']).head()
df.assign(rate=lambda df: df.orders/df.uv)
# shift 函数是对数据进行平移动的操作
df['增幅'] = df['国内生产总值'] - df['国内生产总值'].shift(-1)
df.tshift(1) # 时间移动，按周期
# 和上相同，diff 函数是用来将数据进行移动之后与原数据差
# 异数据，等于 df.shift()-df
df['增幅'] = df['国内生产总值'].diff(-1)
# 留存数据，因为最大一般为数据池
df.apply(lambda x: x/x.max(), axis=1)

# 取 best 列中值为列名的值写到 name 行上
df['value'] = df.lookup(df['name'], df['best'])

s.where(s > 1, 10) # 满足条件下数据替换（10，空为 NaN）
s.mask(s > 0) # 留下满足条件的，其他的默认为 NaN
# 所有值加 1 (加减乘除等)
df + 1 / df.add(1)
# 管道方法，链式调用函数，f(df)=df.pipe(f)
def gb(df, by):
    result = df.copy()
    result = result.groupby(by).sum()
    return result 
# 调用
df.pipe(gb, by='team')
# 窗口计算 '2s' 为两秒
df.rolling(2).sum()
# 在窗口结果基础上的窗口计算
df.expanding(2).sum()
# 超出（大于、小于）的值替换成对应值
df.clip(-4, 6)
# AB 两列想加增加 C 列
df['C'] = df.eval('A+B')
# 和上相同效果
df.eval('C = A + B', inplace=True)
# 数列的变化百分比
s.pct_change(periods=2)
# 分位数, 可实现时间的中间点
df.quantile(.5)
# 排名 average, min,max,first，dense, 默认 average
s.rank()
# 数据爆炸，将本列的类列表数据和其他列的数据展开铺开
df.explode('A')
# 枚举更新
status = {0:'未执行', 1:'执行中', 2:'执行完毕', 3:'执行异常'}
df['taskStatus'] = df['taskStatus'].apply(status.get)
df.assign(金额=0) # 新增字段
df.loc[('bar', 'two'), 'A'] # 多索引查询
df.query('i0 == "b" & i1 == "b"') # 多索引查询方法 2
# 取多索引中指定级别的所有不重复值
df.index.get_level_values(2).unique()
# 去掉为零小数，12.00 -> 12
df.astype('str').applymap(lambda x: x.replace('.00', ''))
# 插入数据，在第三列加入「两倍」列
df.insert(3, '两倍', df['值']*2)
# 枚举转换
df['gender'] = df.gender.map({'male':'男', 'female':'女'})
# 增加本行之和列
df['Col_sum'] = df.apply(lambda x: x.sum(), axis=1)
# 对指定行进行加和
col_list= list(df)[2:] # 取请假范围日期
df['总天数'] = df[col_list].sum(axis=1) # 计算总请假天数
# 对列求和，汇总
df.loc['col_sum'] = df.apply(lambda x: x.sum())
# 按指定的列表顺序显示
df.reindex(order_list)
# 按指定的多列排序
df.reindex(['col_1', 'col_5'], axis="columns")
数据选取
df[col] # 根据列名，并以Series的形式返回列
df[[col1, col2]] # 以DataFrame形式返回多列
df.loc[df['team'] == 'B',['name']] # 按条件查询，只显示name 列
s.iloc[0] # 按位置选取数据
s.loc['index_one'] # 按索引选取数据
df.loc[0,'A':'B'] #  A到 B 字段的第一行 
df.loc[2018:1990, '第一产业增加值':'第三产业增加值']
df.loc[0,['A','B']] # d.loc[位置切片, 字段]
df.iloc[0,:] # 返回第一行, iloc 只能是数字
df.iloc[0,0] # 返回第一列的第一个元素
dc.query('site_id > 8 and utype=="老客"').head() # 可以 and or / & |
 # 迭代器及使用
for idx,row in df.iterrows(): row['id']
# 迭代器对每个元素进行处理
df.loc[i,'链接'] = f'http://www.gairuo.com/p/{slug}.html'
for i in df.Name:print(i) # 迭代一个列
# 按列迭代，[列名, 列中的数据序列 S（索引名 值)]
for label, content in df.items():print(label, content)
# 按行迭代，迭代出整行包括索引的类似列表的内容，可row[2]取
for row in df.itertuples():print(row)
df.at[2018, '总人口'] # 按行列索引名取一个指定的单个元素
df.iat[1, 2] # 索引和列的编号取单个元素
s.nlargest(5).nsmallest(2) # 最大和最小的前几个值
df.nlargest(3, ['population', 'GDP'])
df.take([0, 3]) # 指定多个行列位置的内容
# 按行列截取掉部分内容，支持日期索引标签
ds.truncate(before=2, after=4)
# 将 dataframe 转成 series
df.iloc[:,0]
float(str(val).rstrip('%')) # 百分数转数字
df.reset_index(inplace=True) # 取消索引
数据处理 GroupBy 透视
df.groupby(col) # 返回一个按列col进行分组的Groupby对象
df.groupby([col1,col2]) # 返回一个按多列进行分组的Groupby对象
df.groupby(col1)[col2] # 返回按列col1进行分组后，列col2的均值
# 创建一个按列col1进行分组，并计算col2和col3的最大值的数据透视表
df.pivot_table(index=col1,
               values=[col2,col3],
               aggfunc=max,
               as_index=False)
# 同上
df.pivot_table(index=['site_id', 'utype'],
               values=['uv_all', 'regist_num'],
               aggfunc=['max', 'mean'])
df.groupby(col1).agg(np.mean) # 返回按列col1分组的所有列的均值
# 按列将其他列转行
pd.melt(df, id_vars=["day"], var_name='city', value_name='temperature')
# 交叉表是用于统计分组频率的特殊透视表
pd.crosstab(df.Nationality,df.Handedness)
# groupby 后排序，分组 agg 内的元素取固定个数
(
    df[(df.p_day >= '20190101')]
    .groupby(['p_day', 'name'])
    .agg({'uv':sum})
    .sort_values(['p_day','uv'], ascending=[False, False])
    .groupby(level=0).head(5) # 每天取5个页面
    .unstack()
    .plot()
)
# 合并查询经第一个看（max, min, last, size:数量）
df.groupby('结算类型').first()
# 合并明细并分组统计加总（'max', `mean`, `median`,
# `prod`, `sum`, `std`,`var`, 'nunique'）,'nunique'为去重的列表
df1 = df.groupby(by='设计师ID').agg({'结算金额':sum})
df.groupby(by=df.pf).ip.nunique() # groupby distinct, 分组+去重数
df.groupby(by=df.pf).ip.value_counts() # groupby 分组+去重的值及数量
df.groupby('name').agg(['sum', 'median', 'count'])
数据合并
# 合并拼接行
# 将df2中的行添加到df1的尾部
df1.append(df2)
# 指定列合并成一个新表新列
ndf = (df['提名1']
       .append(df['提名2'], ignore_index=True)
       .append(df['提名3'], ignore_index=True))
ndf = pd.DataFrame(ndf, columns=(['姓名']))
# 将df2中的列添加到df1的尾部
df.concat([df1, df2], axis=1)

# 合并文件的各行
df1 = pd.read_csv('111.csv', sep='\t')
df2 = pd.read_csv('222.csv', sep='\t')
excel_list = [df1, df2]
# result = pd.concat(excel_list).fillna('')[:].astype('str')
result = pd.concat(excel_list)[]
result.to_excel('333.xlsx', index=False)

# 合并指定目录下所有的 excel (csv) 文件
import glob
files = glob.glob("data/cs/*.xls")
dflist = []
for i in files:
    dflist.append(pd.read_excel(i, usecols=['ID', '时间', '名称']))

df = pd.concat(dflist)

# 合并增加列
# 对df1的列和df2的列执行SQL形式的join
df1.join(df2,on=col1,how='inner')
# 用 key 合并两个表
df_all = pd.merge(df_sku, df_spu, 
                  how='left',
                  left_on=df_sku['product_id'],
                  right_on=df_spu['p.product_id'])
时间处理 时间序列
# 时间索引
df.index = pd.DatetimeIndex(df.index)
# 时间只保留日期
df['date'] = df['time'].dt.date
# 将指定字段格式化为时间类型
df["date"] = pd.to_datetime(df['时间'])
# 转化为北京时间
df['time'] = df['time'].dt.tz_convert('Asia/Shanghai')
# 转为指定格式，可能会失去秒以后的精度
df['time'] = df['time'].dt.strftime("%Y-%m-%d %H:%M:%S")
dc.index = pd.to_datetime(dc.index, format='%Y%m%d', errors='ignore')
# 时间，参与运算
pd.DateOffset(days=2)
# 当前时间
pd.Timestamp.now()
pd.to_datetime('today')
# 判断时间是否当天
pd.datetime.today().year == df.start_work.dt.year
df.time.astype('datetime64[ns]').dt.date == pd.to_datetime('today')
# 定义个天数
import datetime
days = lambda x: datetime.timedelta(days=x)
days(2)
# 同上，直接用 pd 包装的
pd.Timedelta(days=2)
# unix 时间戳
pd.to_datetime(ted.film_date, unit='ms')
# 按月（YMDHminS）采集合计数据
df.set_index('date').resample('M')['quantity'].sum()
df.set_index('date').groupby('name')['ext price'].resample("M").sum()
# 按天汇总，index 是 datetime 时间类型
df.groupby(by=df.index.date).agg({'uu':'count'})
# 按周汇总
df.groupby(by=df.index.weekday).uu.count()
# 按月进行汇总
df.groupby(['name', pd.Grouper(key='date', freq='M')])['ext price'].sum()
# 按月进行汇总
df.groupby(pd.Grouper(key='day', freq='1M')).sum()
# 按照年度，且截止到12月最后一天统计 ext price 的 sum 值
df.groupby(['name', pd.Grouper(key='date', freq='A-DEC')])['ext price'].sum()
# 按月的平均重新采样
df['Close'].resample('M').mean()
# https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases
# 取时间范围，并取工作日
rng = pd.date_range(start="6/1/2016",end="6/30/2016",freq='B')
# 重新定时数据频度，按一定补充方法
df.asfreq('D', method='pad')
# 时区，df.tz_convert('Europe/Berlin')
df.time.tz_localize(tz='Asia/Shanghai')
# 转北京时间
df['Time'] = df['Time'].dt.tz_localize('UTC').dt.tz_convert('Asia/Shanghai')
# 查看所有时区
from pytz import all_timezones
print (all_timezones)
# 时长，多久，两个时间间隔时间，时差
df['duration'] = pd.to_datetime(df['end']) - pd.to_datetime(df['begin'])
# 指定时间进行对比
df.Time.astype('datetime64[ns]') < pd.to_datetime('2019-12-11 20:00:00', format='%Y-%m-%d %H:%M:%S')
常用备忘
# 解决科学计数法问题
df = pd.read_csv('111.csv', sep='\t').fillna('')[:].astype('str')
# 和订单量相关性最大到小显示
dd.corr().total_order_num.sort_values(ascending=False)

# 解析列表、json 字符串
import ast
ast.literal_eval("[{'id': 7, 'name': 'Funny'}]")

# Series apply method applies a function to
# every element in a Series and returns a Series
ted.ratings.apply(str_to_list).head()
# lambda is a shorter alternative
ted.ratings.apply(lambda x: ast.literal_eval(x))
# an even shorter alternative is to apply the 
# function directly (without lambda)
ted.ratings.apply(ast.literal_eval)
# 索引 index 使用 apply()
df.index.to_series().apply()
样式显示
# https://pbpython.com/styling-pandas.html
df['per_cost'] = df['per_cost'].map('{:,.2f}%'.format)  # 显示%比形式
# 指定列表（值大于0）加背景色
df.style.applymap(lambda x: 'background-color: grey' if x>0 else '',
                  subset=pd.IndexSlice[:, ['B', 'C']])

# 最大值最小值加背景色
df.style.highlight_max(color='lightgreen').highlight_min(color='#cd4f39')
df.style.format('{:.2%}', subset=pd.IndexSlice[:, ['B']]) # 显示百分号

# 指定各列的样式
format_dict = {'sum':'${0:,.0f}',
                       'date': '{:%Y-%m}',
                       'pct_of_total': '{:.2%}'
                       'c': str.upper}

# 一次性样式设置
(df.style.format(format_dict) # 多种样式形式
    .hide_index()
    # 指定列按颜色深度表示值大小, cmap 为 matplotlib colormap
    .background_gradient(subset=['sum_num'], cmap='BuGn')
    # 表格内作横向 bar 代表值大小
    .bar(color='#FFA07A', vmin=100_000, subset=['sum'], align='zero')
    # 表格内作横向 bar 代表值大小
    .bar(color='lightgreen', vmin=0, subset=['pct_of_total'], align='zero')
    # 下降（小于0）为红色, 上升为绿色
    .bar(color=['#ffe4e4','#bbf9ce'], vmin=0, vmax=1, subset=['增长率'], align='zero')
    # 给样式表格起个名字
    .set_caption('2018 Sales Performance')
    .hide_index())

# 按条件给整行加背景色（样式）
def background_color(row):
    if row.pv_num >= 10000:
        return ['background-color: red'] * len(row)
    elif row.pv_num >= 100:
        return ['background-color: yellow'] * len(row)
    return [''] * len(row)
# 使用
df.style.apply(background_color, axis=1)
表格中的直方图，sparkline 图形
import sparklines
import numpy as np
def sparkline_str(x):
    bins=np.histogram(x)[0]
    sl = ''.join(sparklines.sparklines(bins))
    return sl
sparkline_str.__name__ = "sparkline"
# 画出趋势图，保留两位小数
df.groupby('name')['quantity', 'ext price'].agg(['mean', sparkline_str]).round(2)

# sparkline 图形
# https://hugoworld.wordpress.com/2019/01/26/sparklines-in-jupyter-notebooks-ipython-and-pandas/
def sparkline(data, figsize=(4, 0.25), **kwargs):
    """
    creates a sparkline
    """

    # Turn off the max column width so the images won't be truncated
    pd.set_option('display.max_colwidth', -1)

    # Turning off the max column will display all the data
    # if gathering into sets / array we might want to restrict to a few items
    pd.set_option('display.max_seq_items', 3)

    #Monkey patch the dataframe so the sparklines are displayed
    pd.DataFrame._repr_html_ = lambda self: self.to_html(escape=False)

    from matplotlib import pyplot as plt
    import base64
    from io import BytesIO

    data = list(data)

    *_, ax = plt.subplots(1, 1, figsize=figsize, **kwargs)
    ax.plot(data)
    ax.fill_between(range(len(data)), data, len(data)*[min(data)], alpha=0.1)
    ax.set_axis_off()

    img = BytesIO()
    plt.savefig(img)
    plt.close()
    return '<img src="data:image/png;base64, {}" />'.format(base64.b64encode(img.getvalue()).decode())

# 使用
df.groupby('name')['quantity', 'ext price'].agg(['mean', sparkline])
df.apply(sparkline, axis=1) # 仅支持横向数据画线，可做 T 转置
可视化
kind : str
- 'line' : line plot (default)
- 'bar' : vertical bar plot
- 'barh' : horizontal bar plot
- 'hist' : histogram
- 'box' : boxplot
- 'kde' : Kernel Density Estimation plot
- 'density' : same as 'kde'
- 'area' : area plot
- 'pie' : pie plot
常用方法：

df88.plot.bar(y='rate', figsize=(20, 10)) # 图形大小，单位英寸
df_1[df_1.p_day > '2019-06-01'].plot.bar(x='p_day', y=['total_order_num','order_user'], figsize=(16, 6)) # 柱状图
# 每条线一个站点，各站点的 home_remain, stack的意思是堆叠，堆积
# unstack 即“不要堆叠”
(df[(df.p_day >= '2019-05-1') & (df.utype == '老客')].groupby(['p_day', 'site_id'])['home_remain'].sum().unstack().plot.line())
#  折线图，多条, x 轴默认为 index
dd.plot.line(x='p_day', y=['uv_all', 'home_remain'])
dd.loc['新访客', 2].plot.scatter(x='order_user', y='paid_order_user') # 散点图
dd.plot.bar(color='blue') # 柱状图, barh 为横向柱状图
sns.heatmap(dd.corr()) # 相关性可视化
#  刻度从0开始，指定范围 ylim=(0,100), x 轴相同
s.plot.line(ylim=0)

# 折线颜色 https://matplotlib.org/examples/color/named_colors.html
# 样式( '-','--','-.',':' )
# 折线标记 https://matplotlib.org/api/markers_api.html
# grid=True 显示刻度 etc: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html
s.plot.line(color='green', linestyle='-', marker='o')

# 两个图绘在一起
[df['数量'].plot.kde(), df['数量'].plot.hist()]

# 对表中的数据按颜色可视化
import seaborn as sns
cm = sns.light_palette("green", as_cmap=True)
df.style.background_gradient(cmap=cm, axis=1)

# 将数据转化为二维数组
[i for i in zip([i.strftime('%Y-%m-%d') for i in s.index.to_list()], s.to_list())]

# 和 plot 用法一样 https://hvplot.pyviz.org/user_guide/Plotting.html
import hvplot.pandas

# 打印 Sqlite 建表语句
print(pd.io.sql.get_schema(fdf, 'table_name'))
Jupyter notebooks 问题
# jupyter notebooks plt 图表配置
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (15.0, 8.0) # 固定显示大小
plt.rcParams['font.family'] = ['sans-serif'] # 显示中文问题
plt.rcParams['font.sans-serif'] = ['SimHei'] # 显示中文问题

# 输出单行全部变量
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = 'all'

# jupyter notebooks 页面自适应宽度
from IPython.core.display import display, HTML
display(HTML("<style>.container { width:100% !important; }</style>"))
# 背景白色 <style>#notebook_panel {background: #ffffff;}</style>

# jupyter notebooks 嵌入页面内容
from IPython.display import IFrame
IFrame('https://arxiv.org/pdf/1406.2661.pdf', width=800, height=450)

# Markdown 一个 cell 不支持多张粘贴图片
# 一个文件打印打开只显示一张图片问题解决
# /site-packages/notebook/static/notebook/js/main.min.js var key 处
# 33502、33504 行
key = utils.uuid().slice(2,6)+encodeURIandParens(blob.name);
key = utils.uuid().slice(2,6)+Object.keys(that.attachments).length;
# https://github.com/ihnorton/notebook/commit/55687c2dc08817da587977cb6f19f8cc0103bab1

# 多行输出
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = 'all' #默认为'last'

# 执行 shell 命令: ! <命令语句>

# 在线可视化工具
https://plot.ly/create


Python pandas库159个常用方法使用说明
Pandas库专为数据分析而设计，它是使Python成为强大而高效的数据分析环境的重要因素。
一、Pandas数据结构
1、import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
2、S1=pd.Series(['a','b','c']) series是一组数据与一组索引（行索引）组成的数据结构
3、S1=pd.Series(['a','b','c'],index=(1,3,4)) 指定索引
4、S1=pd.Series({1:'a',2:'b',3:'c'}) 用字典形式指定索引
5、S1.index() 返回索引
6、S1.values() 返回值
7、Df=pd.DataFrame(['a','b','c']) dataframe是一组数据与两组索引（行列索引）组成的数据结构
8、Df=pd.DataFrame([[a,A],[b,B],[c,C]],columns=['小写','大写']，index=['一','二','三'])
Columms 为列索引，index为行索引
9、pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pyspider 清华镜像
10、data={'小写':['a','b','c'],'大写':['A','B','C']} 传入字典
Df=Pd.DataFrame(data)
11、Df.index() df.columns()
二、读取数据
12、df=pd.read_excel(r'C:\user\...xlsx',sheet_name='sheet1') 或
Pd.read_excel(r'C:\user\...xlsx',sheet_name=0) 读取excel表
13、Pd.read_excel(r'C:\user\...xlsx',index_col=0,header=0)
index_col指定行索引，header指定列索引
14、pd.read_excel(r'C:\user\...xlsx',usecols=[0,1]) 导入指定列,不能有index_col和header
15、pd.read_tablel(r'C:\user\...txt'，sep=' ') 导入txt文件,sep指定分隔符是什么
16、df.head(2) 展示前两行，默认展示前5行
17、df.shape 显示数据几行几列，不包含行和列索引
18、http://df.info() 可查看表中数据的类型
19、df.describe() 可获得表中数值类型指端的分布值（和、平均值、方差等）
三、数据预处理
20、http://df.info() 可显示表中哪个数据为空
21、df.isnull() 方法可以判断哪个值是缺失值，如果缺失返回True，否则为False
22、df.dropna() 默认删除含缺失值的行
23、df.dropna(how='all') 删除全为空值的行，不全为空值的行不会删除
24、df.fillna(0) 用0填充所有空值
25、df.fillna({'性别':'男','年龄':'30'}) 对性别列中空值填充男，年龄填充30
26、df.drop_duplicates() 默认对所有值进行重复值检查，保留第一行的值
27、df.drop_duplicates(subset='性别') 对性别列中重复值查询保留第一行
28、df.drop_duplicates(subset=['性别','公司']，keep='last') 对性别和公司两列查重
keep设置默认为first（保留第一个），可设置为last（保留最后一个） 或False(不部不保留)
29、df['ID'].dtype 查看ID列的数据类型
30、df['ID'].astype('float') 将ID列的数据类型转换为float类型
31、数据类型：int、float、object、string、unicode、datetime
32、df['ID'][1] ID列的第二个数据
33、df.columns=['大写','小写','中文'] 为无索引表添加列索引
34、df.index=[1,2,3] 添加行索引
35、df.set_index('编号') 指明要用的列作为行索列
36、df.rename(index={'订单编号':'新订单编号','客户姓名':'新客户姓名'}) 对行索引进行重新命名
37、df.rename(columns={1:'一',2:'二'}) 对列索引进行重新命名
38、df.reset_index() 默认将全部index转化为column
39、df.reset_index(level=0) 将0级索引转化为column
40、df.reset_index(drop=True) 删除原有索引
四、数据选择
41、df[['ID','姓名']] 多个列名要装入list
42、df.iloc[[1,3],[2,4]] 用行列编号选择数据
43、df.iloc[1,1] 选取表中的第3行2列数据，第一行默认为列索引
44、df.iloc[:,0:4] #获取第1列到第4列的值
45、df.loc['一'] #loc用行名选取的行数据，格式是Series，但可以用列表形式访问
46、df.loc['一'][0] 或 df.loc['一']['序号']
47、df.iloc[1]#iloc用行编号选取行数据
48、df.iloc[[1,3]]#多行编号选取行数据，要用list封装，不然变成行列选取
49、df.iloc[1:3]#选择第二行和第四行
50、df[df['年龄']<45] #加判断条件返回符合条件的全部数据，不局限年龄列
51、df[(df['年龄']<45)&(df['ID']<4)] #判断多条件选择数据
52、df.iloc[[1,3],[2,4]] 相当于df.loc[['一','二'],['年龄','ID']] #loc是名，iloc是编号
53、df[df['年龄']<45][['年龄','ID']]#先通过年龄条件选择行，再通过不同索引指定列
54、df.iloc[1:3,2:4]#切片索引
五、数值操作
55、df['年龄'].replace(100,33)#对年龄列中的100替换成33
56、df.replace(np.NaN,0)#相当于fillna(),其中np.NaN是python中缺省值的表示方式
57、df.replace([A,B],C)#多对一替换，A、B替换成C
58、df.replace({'A':'a','B':'b','C':'c'})#多对多替换
59、df.sort_values(by=['申请单编号'],ascending=False)#申请单编号列降序排列，Ture升序排列（默认）
60、df.sort_values(by=['申请单编号'],na_position='first')#申请单编号列升序排列，缺失值排在第一位
默认缺失值在最后一位last
61、df.sort_values(by=['col1','col2'],ascending=[False,True])#多列排序
62、df['销量'].rank(method='first')#销量排名（不是排序），method有first\min\max\average
63、df.drop(['销量','ID'],axis=1)#删除列,直接是列名
64、df.drop(df.columns[[4,5]],axis=1)#删除列,是编号
65、df.drop(colums=['销量','ID'])#此种方式删除列，可以不写axis=1
66、df.drop(['a','b'],axis=0)#删除行,直接是列名
67、df.drop(df.index[[4,5]],axis=0)#删除行,是编号
68、df.drop(index=['a','b'])#此种方式删除行，可以不写axis=0
69、df['ID'].value_counts()#对ID列中数据出现的次数进行统计
70、df['ID'].value_counts(normalize=Ture,sort=False)#对ID列中数据出现的次数占比进行统计，并降序排序
71、df['ID'].unique()#获取列的唯一值
72、df['年龄'].isin(['a',11])#查看这列中是否包含a或11
73、pd.cut(df['ID'],bins=[0,3,6,10])#用bins指明切分区间
74、pd.qcut(df['ID'],3)#ID列切分成3个部分，每部分数据个数尽量一致
75、df.insert(2,'商品',['书','笔','计算器'])#插入第三列
76、df['商品']=['书','笔','计算器'])#插新列，在表的最后面
77、df.T行列互换
78、df.tack()#把表格型数据转化成树形数据
79、df.set_index(['ID','姓名']).stack().reset_index()#宽表转换成长表，先将共同列设置成行索引，再对其他列
进行转化成树形数据，再重置行索引
80、df.melt(id_vars=['ID','姓名'],var_name='year',value_name='sale')#id_var参数指明宽表转换成长表时保持不
变的列，var_name参数表示原来的列索引转化为行索引对应的列名，value_name表示新索引对应值的列名
81、df['C1'].apply(lambda x:x+1)#相当于map(),只是需要和lambda配合
82、df.applymap(lambda x:x+1),对表中的所有数据执行相同函数运算
六、数据运算
83、df['ID']+Df['ID']#可进行加减乘除
84、df['ID']>Df['ID']#可进行> < == !=等比较运算
85、df.count()#统计每列的非空值的个数
86、df.count(axis=1)#统计每行的非空值的个数
87、df['ID'].count()#统计指定列的非空值的个数
88、df.sum(axis=1)#每列/行求和结果
89、df.mean(axis=1)#每列/行求均值
90、df.max(axis=1)#每列/行求最大值
91、df.min(axis=1)#每列/行求最小值
92、df.median(axis=1)#每列/行求中间值
93、df.mode(axis=1)#每列/行中出现最多的值
94、df.var(axis=1)#每列/行求方差
95、df.std(axis=1)#每列/行求标准差
96、df.quantile(0.25)#求1/4分位数，可以0.5、0.75等分位数
97、df.corr()#求整个DataFrame表中的相关性
七、时间序列
98、from datetime import datetime
99、datatime.now()#返回现在的时间年月日时分秒
100、datatime.now().year#返回年，可以.month\.day
101、datatime.now().weekday()-1#返回周几
102、datatime.now().isocalendar()#返回周数
103、 （2018，41，7）#2018年的第41周第7天
104、datatime.now().date()#只返回年月日
105、datatime.now().time()#只返回时间
106、datatime.now().strftime('%Y-%m-%d %H:%M:%S')#返回2020-03-13 09:09:12
107、from dateutil.parer import parse
108、 parse(str_time)#将字符串的时间转化成为时间格式
109、pd.Datetimeindex(['2020-02-03',2020-03-05'])#设置时间索引
110、data['2018']#获取2018年的数据
111、data['2018-01']#获取2018年1月的数据
112、data['2018-01-05':'2018-01-15']#获取这个时段的数据
113、非时间索引的表格处理
114、df[df['成交时间']==datetime(2018,08,05)]
115、df[df['成交时间']>datetime(2018,08,05)]
116、df[(df['成交时间']>datetime(2018,08,05))&(df['成交时间'] <datetime(2018,08,15))]
117、cha=datatime(2018,5,21,19,50)-datatime(2018,5,18,17,50)
118、 cha.days#返回天的时间差
119、 cha.seconds#返回秒的时间差
120、 cha.seconds/3600#返回小时的时间差
121、datatime(2018,5,21,19,50)+timedelta(days=1)#往后移一天
122、datatime(2018,5,21,19,50)+timedelta(seconds=20)#往后移20秒
123、datatime(2018,5,21,19,50)-timedelta(days=1)#往前移一天
八、数据透视表
124、df.groupby('客户分类').count()#客户分类后求数运算
125、df.groupby('客户分类').sum()#客户分类后求和运算
126、df.groupby('客户分类','区域分类').sum()#多列分类后求和运算
127、df.groupby('客户分类','区域分类')['ID'].sum()#多列分类后ID求和运算
128、df['ID']#DataFrame取出一列就是Series类型
129、df.groupby(df['ID']).sum（） 相当于 df.groupby('ID').sum（）
130、df.groupby('客户分类').aggregate(['sum','count']# aggregate可实现多种汇总方式
131、df.groupby('客户分类').aggregate({'ID'：'count','销量'： 'sum'})
132、# aggregate可针对不同列做不同的汇总运算
133、df.groupby('客户分类').sum().reset_index()#分组汇总后再重置索引，变为标准DataFrame
134、pd.pivot_table(data,values,index,columms,aggfunc,fill_value,margins,dropna,margins_name)
135、数据透视表，data:数据表df,values:值，index:行索引，columns:列索引，aggfunc:values的计算类型，fill_value:对空值的填充方式；margins:是否有合计列；margins_name:合计列的列名
136、pd.pivot_table(df,values=['ID','销量'],index='客户分类',columms='区域',aggfunc={'ID'：'count','销量'：'sum'}),fill_value=0,margins=Ture,dropna=None,margins_name='总计')
九、多表格拼接
137、pd.merge(df1,df2)#默认自动寻找两个表中的公共列进行拼接
138、pd.merge(df1,df2,on=“学号“)#on来指定连接列，连接列要是公共列
139、pd.merge(df1,df2,on=['学号','姓名']#on来指定连接列，连接列要是公共列
140、pd.merge(df1,df2,left_on='学号'right_on='编号') #由公共列，但类名不同时用左右键指定
141、pd.merge(df1,df2,left_index='学号'right_index='编号')#两表公共列都是索引列时
142、pd.merge(df1,df2,left_index='学号'right_on='编号')#公共列一个时索引列一个时普通列
143、pd.merge(df1,df2,on='学号',how='inner')#返回公共列中对应的公共值拼接（内连接）
144、pd.merge(df1,df2,on='学号',how='left')#返回公共列中对应的左表值（左连接）
145、pd.merge(df1,df2,on='学号',how='right')#返回公共列中对应的右表值（右连接）
146、pd.merge(df1,df2,on='学号',how='outer')#返回公共列中对应的所有值（外连接）
147、pd.concat([df1,df2])#两个结构相同的表纵向连接，保留原索引值
148、pd.concat([df1,df2]，ignore_index=True)#两个结构相同的表纵向连接，重新设置索引值
149、pd.concat([df1,df2]，ignore_index=True).drop_duplicates()#拼接后去掉重复值
十、导出文件
150、df.to_excel(excel_writer=r'C:\users\zhoulifu\Desktop\测试.xlsx')#导出文件格式.xlsx用to_excel方法，通过excel_writer参数来实现
151、df.to_excel(excel_writer=r'C:\users\zhoulifu\Desktop\测试.xlsx',sheet_name='文档')
152、df.to_excel(excel_writer=r'C:\users\zhoulifu\Desktop\测试.xlsx',sheet_name='文档'，index=False)#导出是去掉索引
153、df.to_excel(excel_writer=r'C:\users\zhoulifu\Desktop\测试.xlsx',sheet_name='文档'，index=False,columns=['ID','销量','姓名'])#设置导出的列
154、df.to_excel(excel_writer=r'C:\users\zhoulifu\Desktop\测试.xlsx',sheet_name='文档'，index=False,columns=['ID','销量','姓名'],encoding='utf-8')#设置导出的列
155、df.to_excel(excel_writer=r'C:\users\zhoulifu\Desktop\测试.xlsx',sheet_name='文档'，index=False,columns=['ID','销量','姓名'],encoding='utf-8',na_rep=0)#缺失值填充
156、writer=pd.ExcelWriter(excelpath,engine='xlsxwirter')#导出多个文件至一个文件的多个sheet
157、df1.to_excel(writer,sheet_name='表一')
158、df2.to_excel(writer,sheet_name='表二')
159、writer.save()

1.包导入
一般我们需要做如下导入，numpy和pandas一般需要联合使用：
import pandas as pd
import numpy as np
本文采用如下缩写：
df：Pandas DataFrame对象
s： Pandas Series对象
2.数据导入
pd.read_csv(filename)：从CSV文件导入数据
pd.read_table(filename)：从限定分隔符的文本文件导入数据
pd.read_excel(filename)：从Excel文件导入数据
pd.read_sql(query, connection_object)：从SQL表/库导入数据
pd.read_json(json_string)：从JSON格式的字符串导入数据
pd.read_html(url)：解析URL、字符串或者HTML文件
pd.read_clipboard()：从粘贴板获取内容
pd.DataFrame(dict)：从字典对象导入数据
举例：
#从文本文件中读取数据
df=pd.read_csv('data.csv')    #数据自带列名
#等效于
#sep指分隔符，对于不是固定的分隔符时，可以编写正则表达式作为分隔符
df=pd.read_table('data.csv',sep=',')
#若数据未带列名，可默认：
df1=pd.read_csv('data1.csv',header=None)# header=0保留列属性，header=None不读列属性
#自定义列名：
df2=pd.read_csv('data1.csv',names=['a','b','c','d','messgae'])
#指定message列(第4列)为列索引，index_col为整数或序列
df3=pd.read_csv('data.csv',index_col=4)
3.数据导出
df.to_csv(filename)：导出数据到CSV文件
df.to_excel(filename)：导出数据到Excel文件
df.to_sql(table_name, connection_object)：导出数据到SQL表
df.to_json(filename)：以Json格式导出数据到文本文件
4.遍历
方式1
for index, row in df.iterrows():
print row["c1"], row["c2"]
方式2
for row in df.itertuples(index=True, name='Pandas'):
print getattr(row, "c1"), getattr(row, "c2")
方式3
for i in range(0, len(df)):
print df.iloc[i]['c1'], df.iloc[i]['c2']
5.创建对象
pd.DataFrame(np.random.rand(20,5))：创建20行5列的随机数组成的DataFrame对象
pd.Series(my_list)：从可迭代对象my_list创建一个Series对象
df.index = pd.date_range('1900/1/30', periods=df.shape[0])：增加一个日期索引
index和reindex联合使用很有用处，index可作为索引并且元素乱排序之后，所以跟着元素保持不变，因此，当重拍元素时，只需要对index进行才重排即可:reindex。
举例：
a = pd.Series([9, 8, 7, 6], index = ['a','b','c','d']) 构造一个Series对象a，前面是值，后面是索引
a = pd.Series({'a':9,'b':8,'c':7}) 通过字典创建
标准正太分布数据：s = np.random.normal(0,1,10000)
时间数据：dates = pd.date_range('20171021',periods=6)
随机矩阵：df1 = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))
字典创建：
df = pd.DataFrame({
'A':pd.series([1,2,np.nan,6,8]),
'B':pd.Timestamp('20130102'),
'C':pd.Series(1,index=list(range(4)),dtype='float32'),
'D':np.array([3]*4,dtype='int32'),
'E':pd.Categorical(['test','train','trat','train']),
'F':'foo'
})
6.数据查看
df.head(n)：查看DataFrame对象的前n行
df.tail(n)：查看DataFrame对象的最后n行
df.shape()：查看行数和列数
df.info()：查看索引、数据类型和内存信息
df.describe()：查看数值型列的汇总统计
s.value_counts(dropna=False)：查看Series对象的唯一值和计数
df.apply(pd.Series.value_counts)：查看DataFrame对象中每一列的唯一值和计数
apply的用处很多，比如可以通过跟lambda函数联合，完成很多功能：将包含某个部分的元素挑出来等等。
df.values.tolist()获得值
df.columns.values.tolist()获得列名
举例：cities['Is wide and has saint name'] = (cities['Area square miles'] > 50) & cities['City name'].apply(lambda name: name.startswith('San'))
7.数据选取
df[col]：根据列名，并以Series的形式返回列
df[[col1, col2]]：以DataFrame形式返回多列
s.iloc[0]：按位置选取数据
s.loc['index_one']：按索引选取数据
df.loc基于标签，df.iloc基于索引（从0开始），ix基于标签或索引
df.iloc[0,:]：返回第一行
df.iloc[3] #第四行
df.iloc[:,3] #第四列
df.iloc[3:5,0:2] #第四到六行，第一到三列
df.iloc[[4,5,6],[0,1,2]] #第四到六行，第一到三列
df[] #这是对行进行切片
举例：
df.loc[0:10] #切片方式查看前10个元素
df.loc(83, "列名")#定位到某个元素，行列分别是：83为行数和列名
8.数据清洗
df.columns = ['a','b','c']：重命名列名
pd.isnull()：检查DataFrame对象中的空值，并返回一个Boolean数组
pd.notnull()：检查DataFrame对象中的非空值，并返回一个Boolean数组
df.dropna()：删除所有包含空值的行
df.fillna(x)：用x替换DataFrame对象中所有的空值
s.astype(float)：将Series中的数据类型更改为float类型
s.replace(1,'one')：用'one'代替所有等于1的值
df.rename(columns=lambda x: x + 1)：批量更改列名
df.set_index('column_one')：更改索引列
举例：
df.dropna(how='any')#去掉包含缺失值的行
df.fillna(value=5)#对缺失值进行填充
pd.isnull(df)#查看该列为空(NaN)的所有元素，是空的返回true，不空返回false
9.数据处理：
Filter, Sort, GroupBy
df[df[col] > 0.5]：选择col列的值大于0.5的行
df.sort_values(col1)：按照列col1排序数据，默认升序排列
df.groupby(col)：返回一个按列col进行分组的Groupby对象
df.groupby(col1).agg(np.mean)：返回按列col1分组的所有列的均值
df.pivot_table(index=col1, values=[col2,col3], aggfunc=max)：创建一个按列col1进行分组，并计算col2和col3的最大值的数据透视表
df.apply(function) #通过自定义函数，应用于df中
例如：data.apply(np.mean)：对DataFrame中的每一列应用函数np.mean
sort_values("列名", inplace=True) #对文件该列进行排序，inplace=True表示排序后覆盖此列
10.数据合并
df1.append(df2)：将df2中的行添加到df1的尾部
df.concat([df1, df2],axis=1)：将df2中的列添加到df1的尾部
df1.join(df2,on=col1,how='inner')：对df1的列和df2的列执行SQL形式的join
11.数据统计
df.describe()：查看数据值列的汇总统计
df.mean()：返回所有列的均值
df.corr()：返回列与列之间的相关系数
df.count()：返回每一列中的非空值的个数
df.max()：返回每一列的最大值
df.min()：返回每一列的最小值
df.median()：返回每一列的中位数
df.std()：返回每一列的标准差
Pandas支持的数据类型
int 整型
float 浮点型
bool 布尔类型
object 字符串类型
category 种类
datetime 时间类型
12.其它：
df.astypes: 数据格式转换
df.value_counts:相同数值的个数统计
df.hist(): 画直方图
df.get_dummies: one-hot编码，将类型格式的属性转换成矩阵型的属性。比如：三种颜色RGB，红色编码为[1 0 0]



pd.set_option('colheader_justify', 'center')   # FOR TABLE <th>
html_string = '''
<html>
  <head><title>HTML Pandas Dataframe with CSS</title></head>
  <link rel="stylesheet" type="text/css" href="df_style.css"/>
  <body>
    {table}
  </body>
</html>.
'''
# OUTPUT AN HTML FILE
with open('myhtml.html', 'w') as f:
    f.write(html_string.format(table=demo_df.to_html(classes='mystyle')))
    
    
    
$ pip install sshtunnel

import sshtunnel
 
with sshtunnel.SSHTunnelForwarder(
        ('192.168.1.216', 22), # ssh端口22
        ssh_password='实际密码',
        ssh_username='实际用户名',
        remote_bind_address=('127.0.0.1', 3306)) as server: # mysql端口3306
    
 

一、Python的惯用命名方法
常用的命名方法包括驼峰命名法（Camel Case）、匈牙利命名法（Hungarian Notation）、帕斯卡命名法（Pascal）、下划线命名法（Snake Case）。
根据Python之父、荷兰程序猿Guido van Rossum的建议，Python中变量的命名应当以Snake Case为主。具体命名方式大致分为以下三种。
（一）绝大多熟情况下建议使用Snake Case命名，即全部字母均小写，用下划线代替单词间的空格
具体包括：
1.模块名
public：module_name
internal：_module_name
例如，我们可以把一个模块的文件命名为my_module.py。
这里要注意，虽然有一些比较老的模块仍然用的是MyModule这种命名方法，但是现在已经不推荐这样命名了。因为，当一个模块和一个类同名的时候，我们就搞不清楚到底指的是类还是模块了。
例如，我可能会搞混，到底写的是
import StringIO
还是
from StringIO import StringIO
2.包名
Public：package_name
3.方法名
Public：method_name()
Internal：_method_name()（被保护的）
4.函数名
Public：function_name()
Internal: _function_name()
5.全局变量名/类的变量名
Public：global_var_name
Internal: _global_var_name
6.实例对象名
Public：instance_var_name
Internal: _instance_var_name（被保护的）
7.函数的参数名
Public：function_parameter_name
8.局部变量名
Public：local_var_name
（二）少部分情况下，建议使用大驼峰命名
具体包括：
1.类名
Public：ClassName
Internal: _ClassName
2.报错名
Public：ExceptionName
（三）只有一种情况下，全部字母都大写
那就是：
1.全局常量名/类的常量名
GLOBAL_CONSTANT_NAME
Internal: _GLOBAL_CONSTANT_NAME
二、应当避免的变量命名
（一）避免由单独一个字母组成的变量名
很多初学者，甚至有一些水平比较低的老师，都喜欢用a、b、c这种单个字母来给变量命名，这是不可取的。
因为这种变量名里面不包含任何信息，变量稍微多一点，程序稍微长一点，很容易就搞不清楚谁是谁、每个变量是干什么的了。
但是，要除开几个特殊的、约定俗成的情况：
1.计数器或者迭代器（例如，i、j、k、v等）
例如，
for i in range(100):
    print(i)
2.字母e在作为 try/except 语句中的异常标识符的时候。
例如，下面的代码可以准确的定位错误类型与错误明细：
a = [1,2,3]
try:
    a[3]
except Exception as e:  # 注意e在这里的用法
    print('错误类型是',e.__class__.__name__)
    print('错误明细是',e)
3.字母f在with语句中作为文件句柄的时候
例如，如果我们要读取一个文件：
with open('/path/to/file', 'r') as f:
    print(f.read())
for line in f.readlines():
    print(line.strip()) # 把末尾的'\n'删掉
现在我们知道了，用单个字母做变量名肯定是不行的。那一个变量的名字要取得多详细才合适呢？
通常来说，一个变量名的详细程度应当与这个变量在程序中出现的范围成正比。
例如，如果我们整个程序总共就不到10行代码，那么，我们用i来给一个变量命名是完全OK的。而且，此时通常就用一个单词做一个变量名。如果在这么短的程序中，每个变量都像this_is_a_variable_name一样，这么长，反而会显得很不协调、很奇怪。
如果我们的程序存在比较复杂的嵌套关系，而且一个变量在总共大几百行代码中的多个位置均有出现，那么我们就不能用i来给一个变量命名了，这是我们就要让这个变量的名字包含足够多的信息，我们才能有效地知道，它是谁，从哪儿来，到哪儿去，要干什么。
（二）避免在任何包名或者模块名中使用中划线（-）。
（三）避免在一个变量的开头和结尾加双下划线
例如：__this_name__。
因为这种类型的变量名是由Python所保留的，用户不能自己定义这种变量名。
（四）不要存在冒犯性的词汇
否则会引起阅读代码的人的不适。如果是开源的代码，那么在代码传播的时候，会存在不好的影响。如果你是为商业公司写代码，那么你有可能影响你的前途，或者商业公司的声誉。
（五）不必要地把变量类型加入到变量名中
例如，id_to_name_dict，就是不合适的命名方法。
我们要知道，Python是动态类型语言。
三、文件名中的下划线
（一）
Internal意味着在一个模块的内部，或者是在类中被保护的或者是私有的。
（二）
在变量名前加单个下划线 (_) 可以保护模块中的变量和函数，因为linters会标记受保护的成员权限。
这个符号达到的效果是，只有类对象和子类对象自己能访问到这些变量，且不能用'from module import'导入函数。
（三）
在实例变量或方法前添加双下划线（即__，又名“dunder”），可以有效地使变量或方法成为它所属的类的私有变量或者私有方法（通过使用name mangling）。
它的效果是只有类对象自己能访问，连子类对象也不能访问到这个数据。
但是，我们不鼓励在变量或者方法前加双下划线，因为它会影响可读性和可测试性，而且并不能做到真正私有。
（四）
下划线也可能出现在以test开头的unittest method的名称中，用以分隔这个名称中的各个逻辑组件，即使这些组件本身使用了类似于CapWords的命名方法。
例如，test<MethodUnderTest>_<state>就是一个可行的命名样式。
例如，testPop_EmptyStack就是根据上面的样式，命名出的一个可接受的名称。它以test开头，Pop和EmptyStack是两个逻辑组件，他们本身都是CapWorks式的命名方法，但是两个组件之间仍然用_分隔开。
我们要注意，命名test methods是没有唯一的正确方法的。
四、其他注意事项
（一）谨慎地使用缩写
此外，我们还要注意，函数名（function name）、变量名、文件名应当是描述性的，要尽量避免缩写。
不过，少数几个常见的、公认的缩写，我们是可以直接使用的：
temp-->tmp
flag-->flg
statistic-->stat
increment-->inc
message-->msg
特别地，不要使用有歧义的缩写或者是你的项目以外的人不熟悉的缩写，同时也不要简单地通过删除一个单词中的字母来创造缩写。
有歧义，一般就指的是一种缩写可能对应多个单词。
另外我还见过一种奇葩的歧义，在这里分享一下，供大家一笑。
我一个学长在做teaching fellow的时候，曾经收到学生交的作业，这个学生在给作业文件命名的时候，把assignment简写成ass。因此，ta的作业下载下来全是，xxx_ass1.py、xxx_ass2.pdf、xxx_ass3.jpg，其中xxx是一个人名（逃
所以，这也是我们要谨慎使用缩写的原因。
（二）
我们应将所有相关的类和最高层级函数放在一个模块中。这一点和Java不一样，Python程序的一个模块中可以有多个类，而不是一个模块中仅能有一个类。
（三）
对于包含大量数学计算的代码，我们通常倾向于使用一些较短的、在参考文献或算法中已经建立、并惯用的符号表示来命名，尽管这些变量名通常会违本文的Python样式指南。
在定义这些变量名的时候，我们要在注释或文档字符串（docstring）中引用所有命名约定的来源。
如果命名约定的来源不可访问了，那么我们要清晰地记录下命名约定的内容具体是什么。
（四）
对于公共的API，我们更倾向于使用符合PEP8的descriptive_names，在脱离代码的上下文语境的时候，这个更有用。
（五）
在创建python脚本的时候，我们要永远使用.py作为文件名扩展。
永远不要在文件名中使用中划线（-）。
如果我们想要在没有文件扩展名的情况下去访问可执行文件，那么需要使用symbolic link或包含 exec "$0.py" "$@" 的简单bash包装器。 一、Python的惯用命名方法
常用的命名方法包括驼峰命名法（Camel Case）、匈牙利命名法（Hungarian Notation）、帕斯卡命名法（Pascal）、下划线命名法（Snake Case）。
根据Python之父、荷兰程序猿Guido van Rossum的建议，Python中变量的命名应当以Snake Case为主。具体命名方式大致分为以下三种。
（一）绝大多熟情况下建议使用Snake Case命名，即全部字母均小写，用下划线代替单词间的空格
具体包括：
1.模块名
public：module_name
internal：_module_name
例如，我们可以把一个模块的文件命名为my_module.py。
这里要注意，虽然有一些比较老的模块仍然用的是MyModule这种命名方法，但是现在已经不推荐这样命名了。因为，当一个模块和一个类同名的时候，我们就搞不清楚到底指的是类还是模块了。
例如，我可能会搞混，到底写的是
import StringIO
还是
from StringIO import StringIO
2.包名
Public：package_name
3.方法名
Public：method_name()
Internal：_method_name()（被保护的）
4.函数名
Public：function_name()
Internal: _function_name()
5.全局变量名/类的变量名
Public：global_var_name
Internal: _global_var_name
6.实例对象名
Public：instance_var_name
Internal: _instance_var_name（被保护的）
7.函数的参数名
Public：function_parameter_name
8.局部变量名
Public：local_var_name
（二）少部分情况下，建议使用大驼峰命名
具体包括：
1.类名
Public：ClassName
Internal: _ClassName
2.报错名
Public：ExceptionName
（三）只有一种情况下，全部字母都大写
那就是：
1.全局常量名/类的常量名
GLOBAL_CONSTANT_NAME
Internal: _GLOBAL_CONSTANT_NAME
二、应当避免的变量命名
（一）避免由单独一个字母组成的变量名
很多初学者，甚至有一些水平比较低的老师，都喜欢用a、b、c这种单个字母来给变量命名，这是不可取的。
因为这种变量名里面不包含任何信息，变量稍微多一点，程序稍微长一点，很容易就搞不清楚谁是谁、每个变量是干什么的了。
但是，要除开几个特殊的、约定俗成的情况：
1.计数器或者迭代器（例如，i、j、k、v等）
例如，
for i in range(100):
    print(i)
2.字母e在作为 try/except 语句中的异常标识符的时候。
例如，下面的代码可以准确的定位错误类型与错误明细：
a = [1,2,3]
try:
    a[3]
except Exception as e:  # 注意e在这里的用法
    print('错误类型是',e.__class__.__name__)
    print('错误明细是',e)
3.字母f在with语句中作为文件句柄的时候
例如，如果我们要读取一个文件：
with open('/path/to/file', 'r') as f:
    print(f.read())
for line in f.readlines():
    print(line.strip()) # 把末尾的'\n'删掉
现在我们知道了，用单个字母做变量名肯定是不行的。那一个变量的名字要取得多详细才合适呢？
通常来说，一个变量名的详细程度应当与这个变量在程序中出现的范围成正比。
例如，如果我们整个程序总共就不到10行代码，那么，我们用i来给一个变量命名是完全OK的。而且，此时通常就用一个单词做一个变量名。如果在这么短的程序中，每个变量都像this_is_a_variable_name一样，这么长，反而会显得很不协调、很奇怪。
如果我们的程序存在比较复杂的嵌套关系，而且一个变量在总共大几百行代码中的多个位置均有出现，那么我们就不能用i来给一个变量命名了，这是我们就要让这个变量的名字包含足够多的信息，我们才能有效地知道，它是谁，从哪儿来，到哪儿去，要干什么。
（二）避免在任何包名或者模块名中使用中划线（-）。
（三）避免在一个变量的开头和结尾加双下划线
例如：__this_name__。
因为这种类型的变量名是由Python所保留的，用户不能自己定义这种变量名。
（四）不要存在冒犯性的词汇
否则会引起阅读代码的人的不适。如果是开源的代码，那么在代码传播的时候，会存在不好的影响。如果你是为商业公司写代码，那么你有可能影响你的前途，或者商业公司的声誉。
（五）不必要地把变量类型加入到变量名中
例如，id_to_name_dict，就是不合适的命名方法。
我们要知道，Python是动态类型语言。
三、文件名中的下划线
（一）
Internal意味着在一个模块的内部，或者是在类中被保护的或者是私有的。
（二）
在变量名前加单个下划线 (_) 可以保护模块中的变量和函数，因为linters会标记受保护的成员权限。
这个符号达到的效果是，只有类对象和子类对象自己能访问到这些变量，且不能用'from module import'导入函数。
（三）
在实例变量或方法前添加双下划线（即__，又名“dunder”），可以有效地使变量或方法成为它所属的类的私有变量或者私有方法（通过使用name mangling）。
它的效果是只有类对象自己能访问，连子类对象也不能访问到这个数据。
但是，我们不鼓励在变量或者方法前加双下划线，因为它会影响可读性和可测试性，而且并不能做到真正私有。
（四）
下划线也可能出现在以test开头的unittest method的名称中，用以分隔这个名称中的各个逻辑组件，即使这些组件本身使用了类似于CapWords的命名方法。
例如，test<MethodUnderTest>_<state>就是一个可行的命名样式。
例如，testPop_EmptyStack就是根据上面的样式，命名出的一个可接受的名称。它以test开头，Pop和EmptyStack是两个逻辑组件，他们本身都是CapWorks式的命名方法，但是两个组件之间仍然用_分隔开。
我们要注意，命名test methods是没有唯一的正确方法的。
四、其他注意事项
（一）谨慎地使用缩写
此外，我们还要注意，函数名（function name）、变量名、文件名应当是描述性的，要尽量避免缩写。
不过，少数几个常见的、公认的缩写，我们是可以直接使用的：
temp-->tmp
flag-->flg
statistic-->stat
increment-->inc
message-->msg
特别地，不要使用有歧义的缩写或者是你的项目以外的人不熟悉的缩写，同时也不要简单地通过删除一个单词中的字母来创造缩写。
有歧义，一般就指的是一种缩写可能对应多个单词。
另外我还见过一种奇葩的歧义，在这里分享一下，供大家一笑。
我一个学长在做teaching fellow的时候，曾经收到学生交的作业，这个学生在给作业文件命名的时候，把assignment简写成ass。因此，ta的作业下载下来全是，xxx_ass1.py、xxx_ass2.pdf、xxx_ass3.jpg，其中xxx是一个人名（逃
所以，这也是我们要谨慎使用缩写的原因。
（二）
我们应将所有相关的类和最高层级函数放在一个模块中。这一点和Java不一样，Python程序的一个模块中可以有多个类，而不是一个模块中仅能有一个类。
（三）
对于包含大量数学计算的代码，我们通常倾向于使用一些较短的、在参考文献或算法中已经建立、并惯用的符号表示来命名，尽管这些变量名通常会违本文的Python样式指南。
在定义这些变量名的时候，我们要在注释或文档字符串（docstring）中引用所有命名约定的来源。
如果命名约定的来源不可访问了，那么我们要清晰地记录下命名约定的内容具体是什么。
（四）
对于公共的API，我们更倾向于使用符合PEP8的descriptive_names，在脱离代码的上下文语境的时候，这个更有用。
（五）
在创建python脚本的时候，我们要永远使用.py作为文件名扩展。
永远不要在文件名中使用中划线（-）。
如果我们想要在没有文件扩展名的情况下去访问可执行文件，那么需要使用symbolic link或包含 exec "$0.py" "$@" 的简单bash包装器。