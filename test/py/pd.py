
'''

pip install jupyter
http://pypi.douban.com/simple/
pip install  -i https://pypi.tuna.tsinghua.edu.cn/simple  jupyter
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas
python  -m pip install --upgrade pip
python  -m pip install  -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip
pip install jupyter_contrib_nbextensions
pip install   -i https://pypi.tuna.tsinghua.edu.cn/simple  jupyter_contrib_nbextensions
工作目录
jupyter notebook --generate-config
Writing default config to: C:\Users\Administrator\.jupyter\jupyter_notebook_conf
ig.py
c.NotebookApp.notebook_dir
D:\\Doc\\Test\\ipynb
jupyter notebook

Nbextensions标签
pip uninstall jupyter_contrib_nbextensions
pip uninstall jupyter_nbextensions_configurator

pip install jupyter_contrib_nbextensions
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple jupyter_contrib_nbextensions
jupyter contrib nbextension install --user
pip install jupyter_nbextensions_configurator
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple  jupyter_nbextensions_configurator

pip install -i https://pypi.tuna.tsinghua.edu.cn/simple lxml
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tushare --upgrade
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple numpy
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple matplotlib
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple plotly
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple sklearn  scikit-learn
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple scipy
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple seaborn
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple StatsModels
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple keras
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple TensorFlow
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas_datareader

pip install baostock -i https://pypi.tuna.tsinghua.edu.cn/simple/ --trusted-host pypi.tuna.tsinghua.edu.cn
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple akshare


http://127.0.0.1:8888/?token=a5188b94f0e62a8e340196dcb6580406868ae55abeffa60d

"D:\Cache\jSoft\Python3.8\python.exe" -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple arrow

'''

new_label = {'SecurityID':'代码', 'Symbol':'名称', 'Increase':'涨幅%', 'New':'现价', 'Chg':'涨跌', 'BuyPx':'买价', 'SellPx':'卖价', 'Vol':'总量', 'NowVol':'现量', 'Accer':'涨速%', 'Tor':'换手%', 'Open':'今开', 'High':'最高', 'Low':'最低', 'Pre':'昨收', 'PE':'市盈(动)', 'Amt':'总金额', 'VR':'量比', 'Industry':'细分行业', 'Area':'地区', 'Amp':'振幅%', 'AvgPx':'均价', 'InVol':'内盘', 'OutVol':'外盘', 'CR':'内外比', 'BuyVol':'买量', 'SellVol':'卖量', 'UnknownVol':'未匹配量', 'TradableShare':'流通股(亿)', 'Cmv':'流通市值', 'TmvAB':'AB股总市值', 'DOP':'强弱度%', 'Activation':'活跃度', 'VolPO':'笔均量', 'TorPO':'笔换手', 'UpDays':'连涨天', 'Increase3':'3日涨幅%', 'PEttm':'市盈(TTM)', 'PEstatic':'市盈(静)', 'OpenR':'开盘%', 'High':'最高%', 'LowR':'最低%', 'IncreaseAvg':'均涨幅%', 'IncreaseEntity':'实体涨幅%', 'BackWave':'回头波%', 'AttackWave':'攻击波%', 'FinancialDate':'财务更新', 'IpoDate':'上市日期', 'TotalStockIssue':'总股本(亿)', 'ShareAB':'B/A股(亿)', 'ShareH':'H股(亿)', 'FundAsset':'总资产(亿)', 'NetAssets':'净资产(亿)', 'MinorityInterest':'少数股权(亿)', 'ROA':'资产负债率%', 'LiquefiableAssets ':'流动资产(亿)', 'FixedAssets':'固定资产(亿)', 'IntangibleAssets)':'无形资产(亿)', 'CurrentLiability ':'流动负债(亿)', 'CapitalReserve':'资本公积金(亿)', 'Inventory':'存货(亿)', 'AccountsReceivable':'应收帐款(亿)', 'OperatingIncome -':'营业收入(亿)', 'OperatingCost':'营业成本(亿)', 'OperatingProfit':'营业利润(亿)', 'InvestmentIncome':'投资收益(亿)', 'TNP':'利润总额(亿)', 'NPAT':'税后利润(亿)', 'NetProfit ':'净利润(亿)', 'UDP':'未分利润(亿)', 'OCF':'经营现金流(亿)', 'TCF':'总现金流(亿)', 'ShareHolders':'股东人数', 'SharePC':'人均持股', 'MVPC':'人均市值', 'ProfitR':'利润同比%', 'RevenueR':'收入同比%', 'PB':'市净率', 'PCF':'市现率', 'PSR ':'市销率', 'EPS':'每股收益', 'BVPS':'每股净资', 'ANAV':'调整后净资', 'AFPS':'每股公积', 'UDPPS':'每股未分配', 'ER':'权益比%', 'NPR':'净益率%', 'GM':'毛利率%', 'OROA':'营业利润率%', 'CROE':'净利润率%', 'Code':'交易代码', 'ChoiceDate':'自选日', 'ChoicePx':'自选价', 'ChoiceYield':'自选收益%', 'AmtOpen':'开盘金额', 'SealR':'封成比', 'AmtSeal':'封单额', 'Increase20':'20日涨幅%', 'IncreaseYear':'年初至今%', 'Increase60':'60日涨幅%', 'OpneTUNZ':'开盘换手Z', 'TUNZ':'换手Z', 'CmvZ':'流通市值Z', 'LiquidRZ':'流通比例Z', 'NewDAvg':'现均差%', 'MidForm':'中期形态', 'ShortForm':'短期形态', 'LongForm':'长期形态', 'Beta':'贝塔系数', 'IndicatorTips':'近日指标提示', 'NonNetProfit':'扣非净利润(亿)', 'DividendYield ':'股息率%', 'CashFlowPS':'每股现金流', 'RDcost':'研发费用(亿)', 'Employee':'员工人数'}




import numpy as np
import pandas as pd
df = pd.read_pickle('AStock20200911')

df = pd.read_pickle('AStock20200911A')

df.head(2)

df.columns.values.tolist()

pd.set_option('Display.max_rows',None)  # 展示全部行
pd.set_option('Display.max_columns',None)   # 展示全部列

# df.rename(columns=new_col, inplace = True)

for i, v in df.dtypes.items():
if v == "object":
print(i,v)


# False :  '',(),[],{},None,set(), 0
# print ('True' if True else 'False')

#df遍历 效率
# DF['eee'] = DF['aaa'].values * DF['bbb'].values  #131us
# DF['eee'] = DF['aaa'] * DF['bbb']  #263us
# DF['eee'] = [ a*b for a,b in zip(DF['aaa'],DF['bbb']) ] #1.1ms
# DF['eee'] = DF[['aaa','bbb']].apply(lambda x: x.aaa * x.bbb, axis=1) #90ms   frame=data.apply(lambda x:x*2)
# DF['eee'] = DF.apply(lambda x: x.aaa * x.bbb, axis=1) #89ms

# kdj_position=df['K']>df['D']
# df.loc[kdj_position[(kdj_position == True) & (kdj_position.shift() == False)].index, 'KDJ_金叉死叉'] = '金叉'

# for index, row in df.iteritems():
# print(index) # 输出列名
# for row in df.iteritems():
# print(row[0], row[1], row[2]) # 输出各列

# for row in df.itertuples(index=True, name='Pandas'):
# print getattr(row, "c1"), getattr(row, "c2")

# for index, rows in DF.iterrows(): #342ms
# rows['eee'] = rows['aaa'] * rows['bbb']

# for i in range(len(DF)): #58ms
# DF.iat[i,4] = DF.iat[i,0] * DF.iat[i,1]

# for i in range(len(DF)): #2s
# DF.iloc[i,4] = DF.iloc[i,0] * DF.iloc[i,1]

# 数据整体描述
# df.shape # 行数 列数 返回表示DataFrame的维度的元组.
# df.team.dtype # 某个字段team的类型
# df.dtypes # 列数据类型
# df.dtypes.value_counts() # 各类型有多少个字段
# df.columns.values.tolist() # 列字段表
# df.ndim  # 轴/数组维度大小
# df.index # 行索引
# df.columns # 列索引
# df.values # 对象值，二维ndarray数组 NDFrame的Numpy表示
# df.size # 元素数
# df.empty # 如果NDFrame完全为空[无项目]，则返回为True; 如果任何轴的长度为0; 注意有空值不认为是空
# df.axes # 获取行及列索引 返回一个列，行轴标签和列轴标签作为唯一的成员
# df.T  # index 与 columns 对调

# df.head(10) # 显示前10行，默认是5行
# df.tail() # 显示末尾几行，默认是5
# df.info() # 相关系数，如行数，列数，列索引、列非空值个数，列类型，内存占用
# df.describe() # 列计算汇总快速统计结果，计数、均值、标准差、最大值、四分数、最小值
# df.sample(3) # 查看 n 个样本，随机
# dfs.keys()  # Series 的索引, DataFrame 的列名

# s.name # 'Q1'
# s.array # 值组成的数组 <PandasArray>
# s.dtype # 类型，dtype('int64')
# s.hasnans # False 是否有空


# 统计函数 简单分析
# df.mean() # 返回所有列的均值，若加上参数
# df.mean(1)则对每一行求平均值
# df.corr() # 返回列与列之间的相关系数
# df.count() # 返回每一列中的非空值的个数 非NA值的数量
# df.max() # 返回每一列的最大值
# df.min() # 返回每一列的最小值
# df.median() # 返回每一列的中位数
# df.std() # 返回每一列的标准差
# df.var() # 样本值的方差
# df.mode() # 众数
# df.sum() # 值的总和
# df.sum(0, skipna=False) # 否要排除缺失数据
# df.sum(level='blooded') # 索引级别
# df.sum(level=0)
# df.sum(min_count=1) # 如果有空值总共算几
# df.media() # 值的算术中位数（50%分位数)
# df.quantile() # 计算样本的分位数（0到 1）   (不同 % 的值)
# df.argmin() # 计算能够获取到最小值的索引位置（整数)
# df.argmax() # 计算能够获取到最大值的索引位置（整数)
# df.idxmin() # 计算能够获取到最小值的索引值
# df.idxmax() # 计算能够获取到最大值的索引值
# df.mad() # 根据平均值计算平均绝对离差
# df.skew() # 样本值的偏度（三阶矩）
# df.kurt() # 样本值的峰度（四阶矩）
# df.cumsum() # 样本值的累计和 df.cumsum(axis=0) # 累积连加,累加
# df.cummin() # 根样本值的累计最小
# df.cummax() # 样本值的累计最大值
# df.cumprod() # 样本值的累计积,累乘
# df.diff() # 计算一阶差分（对时间序列很有用)
# df.pct_change() # 计算百分数变化
# df.abs() # 绝对值
# df.sem() # 平均值的标准误差
# df.prod() # 连乘
# df.mad() # 平均绝对偏差
# df.nunique() # 去重数量，不同值的量

# 位置差值
# df.diff() 本行与前一行的差值（即当前值比上一行增加了多少）
# df.diff(axis=1) # 向右一列  无前一行的本行值为 NaN
# df.diff(2)
# df.diff(-1) # 新的本行为本行减去后一行
# df.shift() # 整体下移一行，最顶的一行为 NaN，不做任何计算，移动后目标位置的类型无法接收收的为 NaN.
# df.shift(3) # 移三行
# df.Q1.head().shift(-1) # 整体上移一行，最底的一行为 NaN
# df.shift(axis=1) # 向右移动一位
# df.shift(3, axis=1) # 移三位
# df.shift(-1, axis=1) # 向左移动一位
# df.Q1 - df.Q1.shift() # 实现了 df.Q1.diff()


# df=df.reset_index(drop=True)
# df.reindex(index=range(df.shape[0]))
# df["index"] = range(len(df))
# df = df.set_index(["index"])

# point_x = [A_x, B_x, C_x, D_x]
# point_y = [A_y, B_y, C_y, D_y]
# points_tulpe = list(zip(point_x, point_y))
# print(points_tulpe)

# df1 = df.values.tolist()


# 生成序号 df.rank()
# df.rank() 可以生成数据的排序值，典例的例子如学生的成绩表，给出排名.

# 排名, 将值变了序号数
# df.rank()
# df.rank(axis=1) # 横向排名
# 相同值的排名处理：
# method='average' 并列第1 计算(1+2)/2=都是1.5，下个是3
# method='max':并列第1，显示2，下个 3
# method='min':并列第1，显示1，下个3
# method='dense':并列第1，显示1，下个 2
# method='first':按索引顺序看谁在索引前
# df.Q1.rank(method='max')
# df.rank(na_option='bottom') # 把空值放在最后
# df.rank(pct=True) # 以百分比形式返回

# 常用函数
# df.all() # 返回所有列all()值的Series
# df.any()

# 用表达式计算生成列.仅支持列，不是太安全
# df.eval('Q2Q3 = Q2 + Q3')
# df.eval('Q2Q3 = Q2 + Q3', inplace=True) # 替换生效

# 四舍五入
# df.round(2) # 指定字段指定保留小数位，如有
# df.round({'Q1': 2, 'Q2': 0})
# df.round(-1) # 保留10位

# 每个列的去重值的数量
# df.nunique()
# s.nunique() # 本列的去重值

# 真假检测
# df.isna() # 值的真假值替换
# df.notna() # 与上相反

# 对 df 整体所有元素做加减乘除等计算：
# df + 1 # 等运算
# df.add() # 加
# df.sub() # 减
# df.mul() # 乘
# df.div() # 除
# df.truediv() # Divide DataFrames (float division).
# df.floordiv() # Divide DataFrames (integer division).
# df.mod() # 模，除后的余数
# df.pow() # 指数幂
# df.dot(df2) # 矩阵运算

# Series 专门函数

# 不重复的值及数量
# s.value_counts()
# s.value_counts(normalize=True) # 重复值的频率
# s.value_counts(sort=False) # 不按频率排序
# s.unique() # 去重的值 array
# s.is_unique # 是否有重复

# 最大最小值
# s.nlargest() # 最大的前5个
# s.nlargest(15) # 最大的前15个
# s.nsmallest() # 最小的前5个
# s.nsmallest(15) # 最小的前15个
# s.pct_change() # 计算与前一行的变化百分比
# s.pct_change(periods=2) # 前两行
# s1.cov(s2) # 两个序列的协方差


# 函数应用
# df.apply(np.cumsum) # 累加

# 导入
# df =pd.read_csv("Counts.csv", header=0)
# pd.read_csv('foo.csv')  # 从 csv 文件读取数据

# 保存到 csv 文件
# df.to_csv('foo.csv')

# 读取 excel 文件
# pd.read_excel('foo.xlsx','Sheet1', index_col=None, na_values=['NA'])
# df.to_excel('foo.xlsx', sheet_name='Sheet1')


# 选择行

# 可以通过将行标签传递给loc函数或者ix函数来选择行
# df.loc['a']
# df.loc[:,'two']
# df.ix['a']

# 按整数位置选择
# 可以通过将整数位置传递给iloc函数来选择行.参考以下示例代码 -
# df.iloc[2]

# 行切片
# 可以使用:运算符选择多行.参考以下示例代码 -
# df[2:4]

# 用人工索引选取
# df[df.index == '000001'] # 指定索引

# 用自然索引选择，类似列表的切片
# df[0:3] # 取前三行,
# df[0:10:2] # 前10个，每两个取一个
# df.iloc[:10,:] # 前10个

# 指定行列
# df.loc['Ben', 'Q1':'Q4'] # 只看 Ben 的四个季度成绩
# df.loc['Eorge':'Alexander', 'team':'Q4'] # 指定行区间

# 行添加
# df.ix['e'] = [22,33,444]
# df.loc['e'] = [22,33,444]
# df = df.append(data2)

# 行删除
# df06 = df06.drop('e')

# 保留表头 清空 DataFrame 空表
# df=df.drop(index=df.index)
# df.drop(df.index, inplace=True)

# 列选择
# df['one']

# 查看指定列
# df['Chg']
# df.Chg # 同上，如果列名符合 python 变量名要求，可使用
# df.set_index('SecurityID', inplace=True) # 建立索引并生效

# 选择多列
# df[['Increase', 'Chg']] # 只看这两列，注意括号
# df.loc[:, ['Increase', 'Chg']] # 和上边效果一样

# 增加列
# df['one'] = 1 # 增加一个固定值的列
# df['total'] = df.Q1 + df.Q2 + df.Q3 + df.Q4 # 增加总成绩列
# # 指定一些列相加增加一个新列
# df['total'] = df.loc[:,'Q1':'Q4'].apply(lambda x:sum(x), axis=1)
# df['total'] = df.sum(axis=1) # 可以把所有为数字的列相加
# df['avg'] = df.total/4 # 增加平均成绩列
# df['three'] = [7,8,9,10] # 直接通过列名进行修改

# 列删除
# del(df['three']) # 使用del删除列
# df.pop('two') # 使用pop删除

# 方法链创建新列
# 方法链（method chains）是一种预计算，并没有改变原变量的数据，同时使用起来非常方便，不需要频繁给变量赋值，后期的数据分析极力推荐这种思路，后期会专门讲解.用 括号 () 是为了方便在多条语句的情况下方便换行对齐.
# 定义一个名为 rate 的新列，并给定计算公式
# (df.assign(rate=df['one']/df['two']).head())
# 可以用 lambda 进行计算，变量 x 是指整个 df
# df.assign(rate=lambda x:x['one']/x['two']).head()
# 可指定多个
# df.assign(rate=lambda x:x['one']/x['two'],
#           rate2=lambda x:x['one']+x['two']).head()


# 选择数据 常用操作
# Operation 	   Syntax 	       Result       e.g.
# 选择列 	        df[col] 	   Series        # df['one']
# 按索引选择行 	    df.loc[label] 	Series       # df.loc['a']
# 按数字索引选择行 	df.iloc[loc] 	Series       # df.iloc[3]
# 使用切片选择行 	df[5:10] 	    DataFrame    # df[1:3]
# 用表达式筛选行 	df[bool_vec] 	DataFrame    # df[df.one > 1]

# 按标签 .loc
# df.loc() 的格式为 df.loc[<索引表达式>, <列表达式>]，表达式支持以下形式：

# 单个标签:
# df.loc[0] # 选择索引为 0 的行
# df.loc[8] # 代表索引，如果是字符需要加引号

# 单个列表标签：
# df.loc[[0,5,10]] # 指定索引 0，5，10 的行
# df.loc[['Eli', 'Ben']] # 如果索引是 name
# df.loc[[False, True]*50] # 为真的列显示，隔一个显示一个 # 真假选择，长度要和索引一样

# 带标签的切片（包括起始和停止）：
# df.loc[0:5] # 索引切片, 代表0-5行，包括5
# df.loc['2010':'2014'] # 如果索引是时间可以用字符查询
# df.loc[:] # 所有 # 本方法支持 Series

# 列筛选，必须有行筛选：
# dft.loc[:, ['Q1', 'Q2']] # 所有行，Q1 和 Q2两列
# dft.loc[:, ['Q1', 'Q2']] # 所有行，Q1 和 Q2两列
# dft.loc[:10, 'Q1':] # 0-10 行，Q1后边的所有列

# 按位置 .iloc
# df.iloc 与 df.loc 相似，但只能用自然索引（行和列的 0 - n 索引），不能用标签.
# df.iloc[:3]
# df.iloc[:]
# df.iloc[2:20:3]
# s.iloc[:3]

# 取具体值 .at
# 类似于 loc, 但仅取一个具体的值，结构为 at[<索引>,<列名>]：
# df.at[4, 'Q1'] # 65 # 注：索引是字符需要加引号
# df.at['lily', 'Q1'] # 65 假定索引是 name
# df.at[0, 'name'] # 'Liver'
# df.loc[0].at['name'] # 'Liver'
# df.set_index('name').at['Eorge', 'team'] # 'C' # 指定列的值对应其他列的值
# df.set_index('name').team.at['Eorge'] # 'C'
# df.team.at[3] # 'C' # 指定列的对应索引的值

# 同样 iat 和 iloc 一样，仅支持数字索引：
# df.iat[4, 2] # 65
# df.loc[0].iat[1] # 'E'

# .get 可以做类似字典的操作，如果无值给返回默认值（例中是0）：
# df.get('name', 0) # 是 name 列
# df.get('nameXXX', 0) # 0, 返回默认值
# s.get(3, 0) # 93, Series 传索引返回具体值
# df.name.get(99, 0) # 'Ben'

# df1 = pd.DataFrame([['Snow','M',22],['Tyrion','M',32],['Sansa','F',18],['Arya','F',14]], columns=['name','gender','age'])

# print("--------更换单个值----------")
# loc和iloc 可以更换单行、单列、多行、多列的值
# df1.loc[0,'age']=25      # 思路：先用loc找到要更改的值，再用赋值（=）的方法实现更换值
# df1.iloc[0,2]=25         # iloc：用索引位置来查找

# at 、iat只能更换单个值
# df1.at[0,'age']=25      # iat 用来取某个单值,参数只能用数字索引
# df1.iat[0,2]=25         # at 用来取某个单值,参数只能用index和columns索引名称
# print(df1)


# 条件选择 表达式筛选

# 单一条件 [] 切片里可以使用表达式进行筛选
# df[df.Q1 > 90] # Q1 列大于90的
# df[df.team == 'C'] # team 列为 'C' 的
# df[df.index == 'Oscar'] # 指定索引即原数据中的 name
# df[df['Q1'] == 8] # Q1 等于8
# df[~(df['Q1'] == 8)] # 不等于8
# df[df.name == 'Ben'] # 姓名为Ben
# df.loc[df['Q1'] > 90, 'Q1':]  # Q1 大于90，只显示 Q1
# df.loc[(df.Q1 > 80) & (df.Q2 < 15)] # and 关系
# df.loc[(df.Q1 > 90) | (df.Q2 < 90)] # or 关系
# df[df.Q1 > df.Q2]

# 组合条件
# df[(df['Q1'] > 90) & (df['team'] == 'C')] # and 关系
# df[df['team'] == 'C'].loc[df.Q1>90] # 多重筛选

# 逻辑判断和函数：
# df.eq() # 等于相等 ==
# df.ne() # 不等于 !=
# df.le() # 小于等于 >=
# df.lt() # 小于 <
# df.ge() # 大于等于 >=
# df.gt() # 大于 >
# 都支持  axis{0 or 'index', 1 or 'columns'}, default 'columns'
# df[df.Q1.ne(89)] # Q1 不等于8
# df.loc[df.Q1.gt(90) & df.Q2.lt(90)] # and 关系 Q1>90 Q2<90

# 其他函数：
# isin
# df[df.team.isin(['A','B'])] # 包含 AB 两组的
# df[df.isin({'team': ['C', 'D'], 'Q1':[36,93]})] # 复杂查询，其他值为 NaN

# 函数筛选
# df[lambda df: df['Q1'] == 8] # Q1为8的
# df.loc[lambda df: df.Q1 == 8, 'Q1':'Q2'] # Q1为8的, 显示 Q1 Q2

# where 和 mask
# s.where(s > 90) # 不符合条件的为 NaN
# s.where(s > 90, 0) # 不符合条件的为 0
# np.where(s>80, True, False) # np.where, 大于80是真否则是假
# np.where(df.num>=60, '合格', '不合格')
# s.mask(s > 90) # 符合条件的为 NaN
# s.mask(s > 90, 0) # 符合条件的为 0

# 例：能被整除的显示，不能的显示相反数
# m = df.loc[:,'Q1':'Q4'] % 3 == 0
# df.loc[:,'Q1':'Q4'].where(m, -df.loc[:,'Q1':'Q4'])

# 行列相同数量，返回一个 array
# df.lookup([1,3,4], ['Q1','Q2','Q3']) # array([36, 96, 61])
# df.lookup([1], ['Q1']) # array([36])

# query
# df.query('Q1 > Q2 > 90') # 直接写类型 sql where 语句
# df.query('Q1 + Q2 > 180')
# df.query('Q1 == Q2')
# df.query('(Q1<50) & (Q2>40) and (Q3>90)')
# df.query('Q1 > Q2 > Q3 > Q4')
# df.query('team != "C"')
# df.query('team not in ("E","A","B")')
# df.query('B == `team name`') # 对于名称中带有空格的列，可以使用反引号引起来

# 支持传入变量，如：大于平均分40分的
# a = df.Q1.mean()
# df.query('Q1 > @a+40')
# df.query('Q1 > `Q2`+@a')

# df.eval() 用法与 df.query 类似
# df[df.eval("Q1 > 90 > Q3 > 10")]
# df[df.eval("Q1 > `Q2`+@a")]

# filter
# 使用 filter 可以对行名和列名进行筛选.
# df.filter(items=['Q1', 'Q2']) # 选择两列
# df.filter(regex='Q', axis=1) # 列名包含Q的
# df.filter(regex='e$', axis=1) # 以 e 结尾的
# df.filter(regex='1$', axis=0) # 正则, 索引名包含1的
# df.filter(like='2', axis=0) # 索引中有2的
# df.filter(regex='^2', axis=0).filter(like='Q', axis=1) # 索引中2开头列名有Q的

# 索引选择器 pd.IndexSlice
# pd.IndexSlice 的使用方法类似于df.loc[] 切片中的方法，常用在多层索引中，以及需要指定应用范围（subset 参数）的函数中，特别是在链式方法中.
# df.loc[pd.IndexSlice[:, ['Q1', 'Q2']]]
# idx = pd.IndexSlice # 变量化使用
# df.loc[idx[:, ['Q1', 'Q2']]]
# df.loc[idx[:, 'Q1':'Q4'], :] # 多索引

# 按数据类型
# 可以只选择或者排除指定类型数据：
# df.select_dtypes(include=['float64']) # 选择 float64 型数据
# df.select_dtypes(include='bool')
# df.select_dtypes(include=['number']) # 只取数字型
# df.select_dtypes(exclude=['int']) # 排除 int 类型
# df.select_dtypes(exclude=['datetime64'])

# 理解筛选原理
# df[<表达式>] 里边的表达式如果单独拿出来，可以看到：
# df.Q1.gt(90)
# out:
# 0     False
# 1     False
# 2     False
# 3      True
# 4     False
#       ...
# Name: Q1, Length: 100, dtype: bool
# 会有一个由真假值组成的数据，筛选后的结果就是为 True 的内容.


# 排序
# df.sort_values(by='Q1') # 按 Q1 列数据升序排列
# df.sort_values(by='Q1', ascending=False) # 降序

# df.sort_values(['team', 'Q1'], ascending=[True, False]) # team 升，Q1 降序

# 分组聚合
# 我们可以实现类似 SQL groupby 那样的数据透视功能：

# df.groupby('team').sum() # 按团队分组对应列相加
# df.groupby('team').mean() # 按团队分组对应列求平均
# 不同列不同的计算方法
# df.groupby('team').agg({'Q1': sum,  # 总和
#                         'Q2': 'count', # 总数
#                         'Q3':'mean', # 平均
#                         'Q4': max}) # 最大值

# 数据转换
# df.groupby('team').sum().T
# df.groupby('team').sum().stack()
# df.groupby('team').sum().unstack()

# 字符处理
# s =pd.Series(['A','B','C','Aaba','Baca', np.nan,'CABA','dog','cat'])
# s.str.lower()

# 文本处理
# 操作方法 .str.
# 方法属性 s.str.lower() etc.
# 切分替换 .str.split('_').str.get(1) .str.replace('^.a|dog', 'XX-XX ', case=False)
# 连接 .str.cat(sep=',')
# 索引 s.str[0]
# 提取子串 .str.extract("(?P[a-zA-Z])")


# 合并
# 使用 concat() 连接 pandas 对象:=
# df =pd.DataFrame(np.random.randn(10,4))
# pieces =[df[:3], df[3:7], df[7:]]
# pd.concat(pieces)

# join 合并：=
# left =pd.DataFrame({'key': ['foo','foo'],'lval': [1,2]})
# right =pd.DataFrame({'key': ['foo','foo'],'rval': [4,5]})
# pd.merge(left, right, on='key')

# 追加
# 在 dataframe 数据后追加行
# df =pd.DataFrame(np.random.randn(8,4), columns=['A','B','C','D'])
# s =df.iloc[3]
# df.append(s, ignore_index=True)
# maTac= maTac.append(maTac2)

# 分组
# 分组常常意味着可能包含以下的几种的操作中一个或多个# 依据一些标准分离数据 对组单独地应用函数 将结果合并到一个数据结构中
# df =pd.DataFrame({'A': ['foo','bar','foo','bar','foo','bar','foo','foo'],
#                   'B': ['one','one','two','three','two','two','one','three'],
#                   'C': np.random.randn(8),
#                   'D': np.random.randn(8)})
# df1 = df.groupby('A').sum()
# df2 = df.groupby(['A','B']).sum()

# 分组时，组内运算
# 代表运算的字符串包括'sum'、'mean'、'min'、'max'、'count'
# pd3 = pd3.groupby('a').agg('sum').reset_index()

# 或者自定义函数
# 或自定义函数不需要参数，则x是serise，如果x有自定参数，则x为DataFrame
# def funname(x,name):
#     print(name)
#     print(type(x),'\n',x)
#     return 2

# pd3 = pd3.groupby('a').agg(funname,'aaa').reset_index()

# 数据透视表
# df =pd.DataFrame({'A': ['one','one','two','three']*3,
#    'B': ['A','B','C']*4,
#    'C': ['foo','foo','foo','bar','bar','bar']*2,
#    'D': np.random.randn(12),
#    'E': np.random.randn(12)})

# 生成数据透视表
# pd.pivot_table(df, values='D', index=['A','B'], columns=['C'])

# 时间序列
# pandas 拥有既简单又强大的频率变换重新采样功能，下面的例子从 1次/秒 转换到了 1次/5分钟：
# rng =pd.date_range('1/1/2012', periods=100, freq='S')
# ts =pd.Series(np.random.randint(0,500,len(rng)), index=rng)
# ts.resample('5Min', how='sum')

# 本地化时区表示
# rng =pd.date_range('3/6/2012 00:00', periods=5, freq='D')
# ts =pd.Series(np.random.randn(len(rng)), rng)
# ts_utc =ts.tz_localize('UTC')

# 转换为周期
# ps =ts.to_period()

# 转换为时间戳
# ps.to_timestamp()

# 分类
# df =pd.DataFrame({"id":[1,2,3,4,5,6],"raw_grade":['a','b','b','a','a','e']})

# 将 raw_grades 转换成 Categoricals 类型
# df["grade"]=df["raw_grade"].astype("category")


# 重命名分类
# df["grade"]=df["grade"].cat.set_categories(["very bad","bad","medium","good","very good"])

# 根据分类的顺序对数据进行排序
# df.sort("grade")


# 数据类型转换

# 数据初始化时指定
# df = pd.DataFrame(data, dtype='float32') # 对所的字段指定类型
# 每个字段分别指定
# df = pd.read_excel(data, dtype={'team': 'string', 'Q1': 'int32'})

# 自动推定类型
# Pandas 可以用以下方法智能地推定各列的数据类型，以下方法不妨一试：

# 自动转换合适的数据类型
# df.convert_dtypes() # 推荐！新的方法，支持 string 类型
# df.infer_objects()

# 按大体类型推定
# m = ['1', 2, 3]
# s = pd.to_numeric(s) # 转成数字
# pd.to_datetime(m) # 转成时间
# pd.to_timedelta(m) # 转成时差
# pd.to_datetime(m, errors='coerce') # 错误处理
# pd.to_numeric(m, errors='ignore')
# pd.to_numeric(m errors='coerce').fillna(0) # 兜底填充
# pd.to_datetime(df[['year', 'month', 'day']]) # 组合成日期

# 最低期望
# pd.to_numeric(m, downcast='integer') # smallest signed int dtype
# array([1, 2, 3], dtype=int8)
# pd.to_numeric(m, downcast='signed') # same as 'integer'
# array([1, 2, 3], dtype=int8)
# pd.to_numeric(m, downcast='unsigned') # smallest unsigned int dtype
# array([1, 2, 3], dtype=uint8)
# pd.to_numeric(m, downcast='float') # smallest float dtype
# array([1., 2., 3.], dtype=float32)

# 应用函数
# df.apply(pd.to_timedelta)

# 类型转换 astype()
# 这也是最常见的数据类型转换方式，各数据类型的介绍可参阅：

# df.dtypes # 查看数据类型
# df.index.astype('int64') # 索引类型转换
# df.astype('int32') # 所有数据转换为 int32
# df.astype({'col1': 'int32'}) # 指定字段转指定类型
# s.astype('int64')
# s.astype('int64', copy=False) # 不与原数据关联
# s.astype(np.uint8)
# df['name'].astype('object')
# data['Q4'].astype('float')
# s.astype('datetime64[ns]')
# data['状态'].astype('bool')

# 转换为时间
# pd.to_datetime() 和 s.astype('datetime64[ns]') 是最简单的时间转换方法.

# 将 89.3% 这样的文本转为浮点数字
# data.rate.apply(lambda x: x.replace('%', '')).astype('float') / 100


# 数据排序

# 索引排序
# sort_index() 可将索引重新排序，意味着每行数据的位置跟着索引而变化.

# s.sort_index() # 升序排列
# df.sort_index() # df 也是按索引进行排序
# df.team.sort_index()
# s.sort_index(ascending=False) # 降序排列
# s.sort_index(inplace=True) # 排序后生效，改变原数据
# s.sort_index(ignore_index=True) # 索引重新0-(n-1) 排, 很有用，可以得到它的排序号
# s.sort_index(na_position='first') # 空值在前，另 'last'
# s.sort_index(level=1) # 如果多层，排一级
# s.sort_index(level=1, sort_remaining=False) # 这层不排

# 行索引排序，表头排序
# df.sort_index(axis=1) # 会把列按列名顺序排列

# 数据值排序
# 数据值的排序主要使用 sort_values()，数值按大小顺序，字符按字母顺序.
# s.sort_values() # 升序
# s.sort_values(ascending=False) # 降序
# s.sort_values(inplace=True) # 修改生效
# s.sort_values(na_position='first') # 空值在前

# df.sort_values(by=['team']) # df 按指定字段顺序
# df.sort_values('Q1')
# df.sort_values(by=['team', 'Q1']) # 按多个字段，先排 team, 在同 team 内再看 Q1
# df.sort_values(by=['team', 'Q1'], ascending=False) # 全降序
# df.sort_values(by=['team', 'Q1'], ascending=[True, False]) # 对应指定team升Q1降
# df.sort_values('team', ignore_index=True) # 索引重新0-(n-1) 排

# 索引和值同时排序
# 有些时间就需要索引和值混合排序，比如先按名字排序同序的再按团队排：
# df.set_index('name', inplace=True)
# df.index.names = ['s_name']
# df.sort_values(by=['s_name', 'team'])  #也适用于多层索引.
# df.set_index('name').sort_values('team').sort_index() # 以下方法也可以实现上述需求，不过要注意顺序

# 以下多层索引示例，a 为一级 a1 为 a 下边的二级索引
# df1.sort_values(by=('a', 'a1'))

# 其他
# 也可以用 nsmallest() 和 nlargest() 来实现排序（只支持数字）：
# s.nsmallest(3) # 最小的三个
# s.nlargest(3) # 最大的三个

# 指定列
# df.nlargest(3, 'Q1')
# df.nlargest(5, ['Q1', 'Q2'])
# df.nsmallest(5, ['Q1', 'Q2'])



# 画图
# df['Q1'].plot() # Q1 成绩的折线分布
# df.loc['Ben','Q1':'Q4'].plot() # ben 四个季度的成绩变化
# df.loc[ 'Ben','Q1':'Q4'].plot.bar() # 柱状图
# df.loc[ 'Ben','Q1':'Q4'].plot.barh() # 横向柱状图
# df.groupby('team').sum().T.plot() # 各 Team
# df.groupby('team').count().Q1.plot.pie() # 各组人数对比

# 直方图
# s = pd.Series(np.random.randint(0,7, size=10))
# s.value_counts()
# ts =pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))
# ts =ts.cumsum()
# ts.plot()

# DataFrame

# df = pd.DataFrame(data=None, index = None, columns = None)
# data: 具体数据，结构化或者同构的 ndarray、可迭代对象、字典或者 DataFrame
# index: 索引，类似数组的对象，支持解包，如果没有指定会自动生成 RangeIndex (0, 1, 2, …, n)
# columns: 列索引，表头，如果没有指定会自动生成 RangeIndex (0, 1, 2, …, n)
# 此外还可以 dtype 指定数据类型，如果未指定，系统会自动推断.

# 创建DateFrame
# df = pd.DataFrame({'国家': ['中国', '美国', '日本'],
#                    '地区': ['亚洲', '北美', '亚洲'],
#                    '人口': [14.33, 3.29, 1.26],
#                    'GDP': [14.22, 21.34, 5.18],
#                   })

# dates =pd.date_range('20130101', periods=6)  # 创建日期索引序
# df =pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))  # 创建Dataframe，其中 index 决定索引序列，columns 决定列名

# 字典创建 DataFrame  Series  一维 ndarray、字典、列表
# df =pd.DataFrame({'A' : 1.,
#    'B': pd.Timestamp('20130102'),
#    'C': pd.Series(1,index=list(range(4)),dtype='float32'),
#    'D': np.array([3]*4,dtype='int32'),
#    'E': pd.Categorical(["test","train","test","train"]),
#    'F':'foo' })

# 从列表创建DataFrame
# data = [1,2,3,4]
# df = pd.DataFrame(data)

# 同构的数组数据
# data = np.zeros((2, ), dtype=[('A', 'i4'), ('B', 'f4'), ('C', 'a10')]) # 创建一个空的 2x3 数组
# data[:] = [(1, 2., 'Hello'), (2, 3., "World")] # 给这个数据填入具体数据值
# pd.DataFrame(data) # 生成 DataFrame
# pd.DataFrame(data, index=['first', 'second']) # 指定索引
# pd.DataFrame(data, columns=['C', 'A', 'B']) # 指定列名

# 从列表字典来创建DataFrame
# data = {'Name':['Tom','Jack','Steve'],'Age':[19,18,20]}
# df = pd.DataFrame(data,index = ['rank1','rank2','rank3'],columns = ['Name','Age','Sex']) # 指定行索引和列索引

# 从字典列表创建数据帧DataFrame
# data = [{'a':1,'b':2},{'a':1,'b':2,'c':3}] # 传递字典列表指定行索引
# df = pd.DataFrame(data)
# df = pd.DataFrame(data,index = ['first','second'])
# df = pd.DataFrame(data,index = ['first','second'],columns = ['a','b','c','d']) # 传递字典列表指定行索引，列索引

# 从系列Series的字典来创建DataFrame
# data = {
#     'one':pd.Series([1,2,3],index = ['a','b','c']),
#     'two':pd.Series([1,2,3,4],index = ['a','b','c','d'])
# }
# df = pd.DataFrame(data)

# 元组组成的字典
# pd.DataFrame({('a', 'b'): {('A', 'B'): 1, ('A', 'C'): 2},
#               ('a', 'a'): {('A', 'C'): 3, ('A', 'B'): 4},
#               ('a', 'c'): {('A', 'B'): 5, ('A', 'C'): 6},
#               ('b', 'a'): {('A', 'C'): 7, ('A', 'B'): 8},
#               ('b', 'b'): {('A', 'D'): 9, ('A', 'B'): 10}}) # 一个双索引的例子

# 由 系列Series 生成
# s1 = pd.Series(['a', 'b', 'c', 'd', 'e']) # 可以将多个同索引的 Series，生成 DataFrame：
# pd.DataFrame(s1)

# 混杂的结构
# 从字典里生成
# pd.DataFrame.from_dict(dict([('A', [1, 2, 3]), ('B', [4, 5, 6])]))
# 从列表、元组、ndarray 中创建
# pd.DataFrame.from_records([(1, 2., b'Hello'), (2, 3., b'World')])
# 列内容为一个字典
# pd.json_normalize(df.col)
# df.col.apply(pd.Series)


# 1.清空一个DataFrame表，保留表头
# 有两种写法

# df=df.drop(index=df.index)
# 或
# df.drop(df.index, inplace=True)

# 2.去掉重复行
# 使用pandas自带的drop_duplicates方法：
# norepeat_df = df.drop_duplicates(subset=['A_ID', 'B_ID'], keep='first')
# 去掉UNIT_ID和KPI_ID列中重复的行，并保留重复出现的行中第一次出现的行
# 补充：
# 当keep=False时，就是去掉所有的重复行
# 当keep=‘first’时，就是保留第一次出现的重复行
# 当keep=’last’时就是保留最后一次出现的重复行。
# （注意，这里的参数是字符串，要加引号！！！）
# https://blog.csdn.net/xueruixuan/article/details/80237248

# 3.去掉NaN行
# 使用pandas自带的dropna()方法：
# 删除表中某行全部为NaN的行 nonan_df = df.dropna(axis=0, how='all') #删除表中某行含有任何NaN的行
# nonan_df = df.dropna(axis=0, how='any')
# 补充：
# 删除行的参数axis = 0
# 删除列的参数axis = 1

# 4.融合数据

# import pandas as pd

# frame=pd.DataFrame([[2,4,1,5],[3,1,4,5],[5,1,4,2]],
# columns=['b','a','d','c'],index=['20201201','20201203','20201202'])

# print(frame)
# print('dataframe根据行索引进行降序排序（排序时默认升序，调节ascending参数）)：')
# print(frame.sort_index(ascending=False))
# print('dataframe根据列索引进行排序：')
# print(frame.sort_index(axis=1))
# print('dataframe根据值进行排序：')
# print(frame.sort_values(by='a'))
# print('通过多个索引进行排序：')
# print(frame.sort_values(by=['a','c']))

# frame2=pd.DataFrame([[4,4,1,5],[6,1,4,5],[10,1,4,2],[2,2,2,2]],
# columns=['b','a','d','c'],
# index=['20201201','20201203','20201202','20201204'])

# print(frame2)
# frameAdd = frame+frame2
# print(frameAdd)
# print('======去除NAN值后数据，求平均=========')
# frameAdd_average = frameAdd/2
# nonan_df = frameAdd_average.dropna(axis=0, how='any')
# print(nonan_df)
# print('======合并的数据=========')
# mergeList = pd.concat([frame,frame2],axis=0)
# print(mergeList)

# print('======合并后的数据去重数据=========')
# norepeat_df = mergeList[~mergeList.index.duplicated(keep='last')]
# print(norepeat_df)

# print('======最终合并后的数据且去重数据=========')
# 先添加的并集数据，相加求平均的数据在后面
# add_averg_and_allData = pd.concat([norepeat_df,nonan_df],axis=0)
# 相合并后的数据
# pd_data = add_averg_and_allData[~add_averg_and_allData.index.duplicated(keep='last')]
# print(pd_data)



# Series

# 创建series
# 一个series是一个一维的数据类型，其中每一个元素都有一个标签.类似于Numpy中元素带标签的数组.其中，标签可以是数字或者字符串
# pd.Series(['a', 'b', 'c', 'd', 'e'])
# pd.Series(('a', 'b', 'c', 'd', 'e'))

# 通过一维数组方式创建
# s = pd.Series([1, 2, 5, np.nan, 6, 8])

# 从ndarray创建一个系列
# data = np.array(['a','b','c','d'])
# ser02 = pd.Series(data)

#指定索引
# data = np.array(['a','b','c','d'])
# ser02 = pd.Series(data,index=[100,101,102,103])
# ser02 = pd.Series(data,index=['name','age','sex','address'])

# 从字典dict创建一个系列
# 字典(dict)可以作为输入传递，如果没有指定索引，则按排序顺序取得字典键以构造索引. 如果传递了索引，索引中与标签对应的数据中的值将被拉出.
# data = {'a':1,'b':2,'c':3}
# ser03 = pd.Series(data)

#指定索引
# data = {'a':1,'b':2,'c':3}
# ser03 = pd.Series(data,index = ['a','b','c','d'])

#标量创建 scalar value
# 一个具体的值，如果不指定索引长度为 1，指定索引后长度为索引的数量，每个索引的值都是它.
# ser04 = pd.Series(5,index = [0,1,2,3])
# pd.Series(5., index=['a', 'b', 'c', 'd', 'e'])

# Series值的获取
# 通过方括号+索引的方式读取对应索引的数据，有可能返回多条数据
# 通过方括号+下标值的方式读取对应下标值的数据，下标值的取值范围为：[0，len(Series.values))；另外下标值也可以是负数，表示从右往左获取数据
# Series获取多个值的方式类似NumPy中的ndarray的切片操作，通过方括号+下标值/索引值+冒号(:)的形式来截取series对象中的一部分数

#检索第一个元素.
# ser05 = pd.Series([1,2,3,4,5],index = ['a','b','c','d','e'])
# ser05[1]
# ser05['a']
# ser05['d']

#检索系列中的前三个元素
# ser05 = pd.Series([1,2,3,4,5],index = ['a','b','c','d','e'])

#通过索引来获取数据
# ser05[:3]
# ser05[::2]
# ser05[4:2:-1]

# 类似ndarray
# s = pd.Series([1,2,3,4,5,6,7,8])
# s[3] # 类似列表切片
# s[2:]
# s.median() # 平均值，包括其他的数学函数
# s[s > s.median()] # 筛选大于平均值的内容
# s[[1, 2, 1]] # 指定索引的内容，括号的列表是索引
# s.dtype # 数据类型
# s.array # 返回值的数列
# s.to_numpy() # 转为 numpy 的 ndarray
# 3 in s # 逻辑运算，检测索引

#通过标签（下标值）来获取数据
# print(ser05['b':'d'])
# ser05['a':'d':2]
# ser05['e':'c':-1]
# ser05[['a','b']]

# 类似字典的操作
# s = pd.Series([14.22, 21.34, 5.18],
#               index=['中国', '美国', '日本'],
#               name='人口')
# s['中国'] # 14.22 # 根 key 进行取值，如果没有报 KeyError
# s['印度'] = 13.54 # 类似字典一样增加一个数据
# '法国' in s # False 逻辑运算，检测索引


# Series的运算
# series = pd.Series({'a':941,'b':431,'c':9327})

# 向量计算和标签对齐
# s = pd.Series([1,2,3,4])
# s + s # 同索引相加，无索引位用 NaN 补齐
# s * 2 # 同索引相乘
# s[1:] + s[:-1] # 选取部分进行计算
# np.exp(s) # 求e的幂次方


#输出大于500的值
# series[series>500]

#计算加
# series+10

#计算减
# series-100

#计算乘
# series*10

#两个系列相加
# ser01 = pd.Series([1,2,3])
# ser02 = pd.Series([4,5,6])
# ser01+ser02

# 计算各个元素的指数e的x次方  e 约等于 2.71828
# np.exp(series)
# np.abs(series)

#sign()计算各个元素的正负号: 1 正数，0：零，-1：负数
# np.sign(series)

# Series自动对齐
# 当多个series对象之间进行运算的时候，如果不同series之间具有不同的索引值，那么运算会自动对齐不同索引值的数据，如果某个series没有某个索引值，那么最终结果会赋值为NaN.

# serA = pd.Series([1,2,3],index = ['a','b','c'])
# serB = pd.Series([4,5,6],index = ['b','c','d'])
# serA+serB

# 不同维度的 pandas 对象也可以做运算，它会自动进行对应，shift 用来做对齐操作.
# s = pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2)
# 对不同维度的 pandas 对象进行减法操作
# df.sub(s, axis='index')

# 名称属性
# Series 可以指定一个名称，如无名称不返回内容（NoneType）.
# s = pd.Series([1,2,3,4], name='数字')
# s.name # '数字'
# s = s.rename("number") # 修改名称
# s2 = s.rename("number") # 修改名称并赋值给一个新变量

# 其他操作
# s = pd.Series([1,2,3,4], name='数字')
# s.add(1) # 每个元素加1 abs()
# s.add_prefix(3) # 给索引前加个3，升位
# s.add_suffix(4) # 同上，在后增加
# s.sum() # 总和
# s.count() # 数量，长度
# s.agg('std') # 聚合，仅返回标准差, 与 s.std() 相同
# s.agg(['min', 'max']) # 聚合，返回最大最小值
# s.align(s2) # 联接
# s.any() # 是否有为假的
# s.all() # 是否全是真
# s.append(s2) # 追加另外一个 Series
# s.apply(lambda x:x+1) # 应用方法
# s.empty # 是否为空
# s3 = s.copy() # 深拷贝


# Numpy

# ndarray 创建
# np.array([1, 2, 3])
# np.array((1, 2, 3)) # 同上

# np.array(((1, 2),(1, 2)))
# np.array(([1, 2],[1, 2])) # 同上

# 使用函数创建
# np.arange(10) # 10个, 不包括10，步长为 1
# np.arange(3, 10, 0.1) # 从 3 开始到时，步长为 0.1
# np.linspace(2.0, 3.0, num=5, endpoint=False) # 从 2.0 开始到 3.0，生成均匀的 5 个值，不包括终终值 3.0
# np.random.randn(6, 4)  # 返回一个 6x4 的随机数组，float 型
# np.random.randint(3,7,size=(2, 4)) # 指定范围指定形状的数组，整型
# np.zeros(6) # 6个浮点 0. # 创建值为 0 的数组
# np.zeros((5, 6), dtype=int) # 5 x 6 整型 0
# np.ones(4) # 同上
# np.empty(4) # 同上
# np.zeros_like(np.arange(6))  # 创建一份和目标结构相同的 0 值数组
# np.ones_like(np.arange(6)) # 同上
# np.empty_like(np.arange(6)) # 同上

# 数组信息
# n.shape() # 数组的形状, 返回值是一个元组
# n.shape = (4, 1) # 改变形状
# a = n.reshape((2,2)) # 改变原数组的形状创建一个新的
# n.dtype # 数据类型
# n.ndim # 维度数
# n.size # 元素数
# np.typeDict # np 所有数据类型

# 计算
# 两个数组间的操作总是应用在每个元素上的.

# np.array( [10,20,30,40] )[:3] # 支持类似列表的切片
# a = np.array( [10,20,30,40] )
# b = np.array( [1, 2, 3, 4] )
# a+b # array([11, 22, 33, 44]) 矩阵相加
# a-1 # array([ 9, 19, 29, 39])
# 4*np.sin(a)

# 以下举例数学函数，还支持非常多的数据函数
# a.max() # 40
# a.min() # 10
# a.sum() # 100
# a.std() # 11.180339887498949
# a.all() # True
# a.cumsum() # array([ 10,  30,  60, 100])
# b.sum(axis=1) # 多维可以指定方向

# 类型
# np.int64 # 有符号 64 位 int 类型
# np.float32 # 标准双精度浮点类型
# np.complex # 由128位的浮点数组成的复数类型
# np.bool # TRUE 和 FALSE 的 bool 类型
# np.object # Python 中的 object 类型
# np.string # 固定长度的 string 类型
# np.unicode # 固定长度的 unicode 类型
# np.NaN # np.float 的子类型
# np.nan


# "D:\Cache\jSoft\Python3.6\python.exe"


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

---------------numpy-----------------------
arr = np.array([1,2,3], dtype=np.float64)
np.zeros((3,6))  np.empty((2,3,2)) np.arange(15)
arr.dtype arr.ndim arr.shape
arr.astype(np.int32) #np.float64 np.string_ np.unicode_
arr * arr arr - arr 1/arr
arr= np.arange(32).reshape((8,4))
arr[1:3, : ]  #正常切片
arr[[1,2,3]]  #花式索引
arr.T   arr.transpose((...))   arr.swapaxes(...) #转置
arr.dot #矩阵内积
np.sqrt(arr)   np.exp(arr)    randn(8)＃正态分布值   np.maximum(x,y)
np.where(cond, xarr, yarr)  ＃当cond为真，取xarr,否则取yarr
arr.mean()  arr.mean(axis=1)   #算术平均数
arr.sum()   arr.std()  arr.var()   #和、标准差、方差
arr.min()   arr.max()   #最小值、最大值
arr.argmin()   arr.argmax()    #最小索引、最大索引
arr.cumsum()    arr.cumprod()   #所有元素的累计和、累计积
arr.all()   arr.any()   # 检查数组中是否全为真、部分为真
arr.sort()   arr.sort(1)   #排序、1轴向上排序
arr.unique()   #去重
np.in1d(arr1, arr2)  #arr1的值是否在arr2中
np.load() np.loadtxt() np.save() np.savez() ＃读取、保存文件
np.concatenate([arr, arr], axis=1)  ＃连接两个arr，按行的方向


---------------pandas-----------------------
ser = Series()     ser = Series([...], index=[...])  #一维数组, 字典可以直接转化为series
ser.values    ser.index    ser.reindex([...], fill_value=0)  #数组的值、数组的索引、重新定义索引
ser.isnull()   pd.isnull(ser)   pd.notnull(ser)   #检测缺失数据
ser.name=       ser.index.name=    #ser本身的名字、ser索引的名字
ser.drop('x') #丢弃索引x对应的值
ser +ser  #算术运算
ser.sort_index()   ser.order()     ＃按索引排序、按值排序
df = DataFrame(data, columns=[...], index=[...]) #表结构的数据结构，既有行索引又有列索引
df.ix['x']  #索引为x的值    对于series，直接使用ser['x']
del df['ly']  #用del删除第ly列
df.T    #转置
df.index.name df.columns.name df.values
df.drop([...])
df + df   df1.add(df2, fill_vaule=0) #算术运算
df -ser   #df与ser的算术运算
f=lambda x: x.max()-x.min()   df.apply(f)
df.sort_index(axis=1, ascending=False)   #按行索引排序
df.sort_index(by=['a','b'])   #按a、b列索引排序
ser.rank()   df.rank(axis=1)  #排序，增设一个排名值
df.sum()   df.sum(axis=1)   #按列、按行求和
df.mean(axis=1, skipna=False)   #求各行的平均值，考虑na的存在
df.idxmax()   #返回最大值的索引
df.cumsum()   #累计求和
df.describe()  ser.describe()   #返回count mean std min max等值
ser.unique()  #去重
ser.value_counts()   df.value_counts()  ＃返回一个series，其索引为唯一值，值为频率
ser.isin(['x', 'y'])  #判断ser的值是否为x,y，得到布尔值
ser.dropna() ser.isnull() ser.notnull() ser.fillna(0)  #处理缺失数据，df相同
df.unstack()   #行列索引和值互换  df.unstack().stack()
df.swaplevel('key1','key2')   #接受两个级别编号或名称，并互换
df.sortlevel(1) #根据级别1进行排序，df的行、列索引可以有两级
df.set_index(['c','d'], drop=False)    #将c、d两列转换为行,因drop为false，在列中仍保留c,d
read_csv   read_table   read_fwf    #读取文件分隔符为逗号、分隔符为制表符('\t')、无分隔符（固定列宽）
pd.read_csv('...', nrows=5) #读取文件前5行
pd.read_csv('...', chunksize=1000) #按块读取，避免过大的文件占用内存
pd.load() #pd也有load方法，用来读取二进制文件
pd.ExcelFile('...xls').parse('Sheet1')  # 读取excel文件中的sheet1
df.to_csv('...csv', sep='|', index=False, header=False) #将数据写入csv文件，以｜为分隔符，默认以，为分隔符, 禁用列、行的标签
pd.merge(df1, df2, on='key', suffixes=('_left', '_right')) #合并两个数据集,类似数据库的inner join, 以二者共有的key列作为键,suffixes将两个key分别命名为key_left、key_right
pd.merge(df1, df2, left_on='lkey', right_on='rkey') #合并，类似数据库的inner join, 但二者没有同样的列名，分别指出，作为合并的参照
pd.merge(df1, df2, how='outer') #合并，但是是outer join；how='left'是笛卡尔积，how='inner'是...;还可以对多个键进行合并
df1.join(df2, on='key', how='outer')  #也是合并
pd.concat([ser1, ser2, ser3], axis=1) #连接三个序列，按行的方向
ser1.combine_first(ser2)   df1.combine_first(df2) #把2合并到1上，并对齐
df.stack() df.unstack()  #列旋转为行、行旋转为列
df.pivot()
df.duplicated()   df.drop_duplicates() #判断是否为重复数据、删除重复数据
df[''].map(lambda x: abs(x)) #将函数映射到df的指定列
ser.replace(-999, np.nan) #将－999全部替换为nan
df.rename(index={}, columns={}, inplace=True) #修改索引，inplace为真表示就地修改数据集
pd.cut(ser, bins)  #根据面元bin判断ser的各个数据属于哪一个区段，有labels、levels属性
df[(np.abs(df)>3).any(1)] #输出含有“超过3或－3的值”的行
permutation  take    #用来进行随机重排序
pd.get_dummies(df['key'], prefix='key')  #给df的所有列索引加前缀key
df[...].str.contains()  df[...].str.findall(pattern, flags=re.IGNORECASE)  df[...].str.match(pattern, flags=...)    df[...].str.get()  #矢量化的字符串函数

----绘图
ser.plot() df.plot() #pandas的绘图工具，有参数label, ax, style, alpha, kind, logy, use_index, rot, xticks, xlim, grid等，详见page257
kind='kde' #密度图
kind='bar' kind='barh' #垂直柱状图、水平柱状图，stacked=True为堆积图
ser.hist(bins=50) #直方图
plt.scatter(x,y) #绘制x,y组成的散点图
pd.scatter_matrix(df, diagonal='kde', color='k', alpha='0.3')  #将df各列分别组合绘制散点图

----聚合分组
groupby() 默认在axis=0轴上分组，也可以在1组上分组；可以用for进行分组迭代
df.groupby(df['key1']) #根据key1对df进行分组
df['key2'].groupby(df['key1'])  #根据key1对key2列进行分组
df['key3'].groupby(df['key1'], df['key2'])  #先根据key1、再根据key2对key3列进行分组
df['key2'].groupby(df['key1']).size() #size()返回一个含有分组大小的series
df.groupby(df['key1'])['data1']  等价于 df['data1'].groupby(df['key1'])
df.groupby(df['key1'])[['data1']]  等价于  df[['data1']].groupby(df['key1'])
df.groupby(mapping, axis=1)  ser(mapping) #定义mapping字典，根据字典的分组来进行分组
df.groupby(len) #通过函数来进行分组，如根据len函数
df.groupby(level='...', axis=1)  #根据索引级别来分组
df.groupby([], as_index=False)   #禁用索引，返回无索引形式的数据
df.groupby(...).agg(['mean', 'std'])   #一次使用多个聚合函数时，用agg方法
df.groupby(...).transform(np.mean)   #transform()可以将其内的函数用于各个分组
df.groupby().apply()  #apply方法会将待处理的对象拆分成多个片段，然后对各片段调用传入的函数，最后尝试将各片段组合到一起

----透视交叉
df.pivot_table(['',''], rows=['',''], cols='', margins=True)  #margins为真时会加一列all
pd.crosstab(df.col1, df.col2, margins=True) #margins作用同上


---------------matplotlib---------------
fig=plt.figure() ＃图像所在的基对象
ax=fig.add_subplot(2,2,1)  #2*2的图像，当前选中第1个
fig, axes = plt.subplots(nrows, nclos, sharex, sharey)  #创建图像，指定行、列、共享x轴刻度、共享y轴刻度
plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)
#调整subplot之间的距离，wspace、hspace用来控制宽度、高度百分比
ax.plot(x, y, linestyle='--', color='g')   #依据x,y坐标画图，设置线型、颜色
ax.set_xticks([...]) ax.set_xticklabels([...]) #设置x轴刻度
ax.set_xlabel('...') #设置x轴名称
ax.set_title('....') ＃设置图名
ax.legend(loc='best') #设置图例， loc指定将图例放在合适的位置
ax.text(x,y, 'hello', family='monospace', fontsize=10) #将注释hello放在x,y处，字体大小为10
ax.add_patch() #在图中添加块
plt.savefig('...png', dpi=400, bbox_inches='tight') #保存图片，dpi为分辨率，bbox＝tight表示将裁减空白部分




------------------------------------------
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
#可以用来绘制地图


-----------------时间序列--------------------------
pd.to_datetime(datestrs)    #将字符串型日期解析为日期格式
pd.date_range('1/1/2000', periods=1000)    #生成时间序列
ts.resample('D', how='mean')   #采样，将时间序列转换成以每天为固定频率的, 并计算均值；how='ohlc'是股票四个指数；
＃重采样会聚合，即将短频率（日）变成长频率（月），对应的值叠加；
＃升采样会插值，即将长频率变为短频率，中间产生新值
ts.shift(2, freq='D')   ts.shift(-2, freq='D') #后移、前移2天
now+Day() now+MonthEnd()
import pytz   pytz.timezone('US/Eastern')   #时区操作，需要安装pytz
pd.Period('2010', freq='A-DEC')   ＃period表示时间区间，叫做时期
pd.PeriodIndex    #时期索引
ts.to_period('M')   #时间转换为时期
pd.rolling_mean(...)    pd.rolling_std(...)   #移动窗口函数－平均值、标准差




判断一个变量是否已经定义
方法一：try except方法：
def isset(v):
try :
type (eval(v))
except :
return  0
else :
return  1

if isset('user_name'):
print 'user_name is defined'
else
print 'user_name is not defined'

方法二：使用命名空间：
'varname' in locals().keys() #获取已定义 对象字典
'varname' in  dir()         #获取已定义 对象列表
vars().has_key('var')       #获取已定义 对象字典


判空
df.empty ，这是 DataFrame 内置的属性，可以看到虽然调用简单，但他是最耗时的
len(df)==0 ，这是通过Python内置len方法判断 DataFrame 的行数，相对来说速度比较快，是第1种的3倍
len(df.index)==0 ，这是判断 DataFrame 的行索引的值数量，这已经到达纳秒级别了，是其中最快的方式




1.识别数据类型 (1. Identifying Data Types)
Find Categorical Data
查找分类数据
list(df.columns[df.dtypes == 'object'])
列表(df.columns [df.dtypes =='object'])
But, Categorical data can exist in Numerical format. eg. , days of a month, months(1C12), waist-size (24C38).
但是，分类数据可以数字格式存在. 例如. ，每月的某几天，几个月(1C12)，腰围(24C38).
2. Distinguish between Numerical and Categorical Data
2.区分数值数据和分类数据
df.nunique().sort_values()
df.nunique().sort_values()
Categorical ― The count of unique values should be 30 or less.
分类-唯一值的计数应小于或等于30.
对数值数据进行运算 (Perform operations on numerical data)
Correlations ― should only be done on numeric variables.
关联-仅应在数字变量上进行.
uniqueCount = df.nunique()
uniqueCount = df.nunique()
numerical_columns = list (uniqueCount [ uniqueCount > 30 ].keys())
numeric_columns =列表(uniqueCount [uniqueCount> 30] .keys())
df[numerical_columns] .corr()
df [numerical_columns] .corr()
对数值数据进行可视化 (Perform Visualisations on numerical data)
Scatter plot should always be feed with numerical data each side.
散点图应始终在两侧馈送数值数据.
2.固定行和列 (2. Fixing the rows and columns)
2.1检查格式 (2.1 Check Formatting)
Check the file format
检查文件格式
pd.read_csv() / pd.read_tsv()
pd.read_csv()/ pd.read_tsv()
2. Check the separator type
2.检查分隔符类型
pd.read_csv(sep = '|')
pd.read_csv(sep ='|')
2.2固定行 (2.2 Fixing rows)
Ignore irrelevant rows ― Rows other than the column name headers , use skiprows = 2
忽略不相关的行-列名标题以外的行，请使用skiprows = 2
Delete summary rows ― We derive such insights from dataset, hence not required.
删除摘要行-我们从数据集中获得此类见解，因此不是必需的.
Delete extra rows ― column number indicator, subsection, new page.
删除多余的行-列号指示符，小节，新页面.
df = df.loc['condition']
df = df.loc ['condition']
2.3固定柱 (2.3 Fixing columns)
Add column names if missing.
如果缺少，请添加列名称.
df.columns.values[i] = 'Column_name'
df.columns.values [i] ='列名'
2. Rename columns consistently: Abbreviations, encoded columns.
2.一致地重命名列：缩写，编码列.
df.rename(columns = {'old_name' : 'new_name'} )
df.rename(columns = {'old_name'：'new_name'})
3. Align misaligned columns -> Manual fix on file.
3.对齐未对齐的列->手动修复文件.
4. Delete columns having less analytical value. Like name, (system-generated)id columns.
4.删除分析值较小的列. 像名称一样，(系统生成的)id列.
df.drop(columns = ['col1', 'col2'])
df.drop(columns = ['col1'，'col2'])
df.drop(['col1', 'col2'], axis = 1)
df.drop(['col1'，'col2']，轴= 1)
5. Split/merge columns to get more understandable data.
5.拆分/合并列以获取更多可理解的数据.
df['avg'] = (df['M1'] + df['M2']) /2 ;
df ['avg'] =(df ['M1'] + df ['M2'])/ 2;
df['fullname'] = df['firstname] + ' ' + df[lastname]
df ['fullname'] = df ['firstname] +''+ df [lastname]
3.插补/删除缺失值 (3. Imputing/removing missing values)
3.1用空值替换空值 (3.1 Replace Empty values with Nulls)
Need to replace the empty strings or object NA,XX with nan
需要用nan替换空字符串或对象NA，XX
df.replace(r'^\s*$', np.NaN, regex=True)
df.replace(r'^ \ s * $'，np.NaN，regex = True)
df.replace(r'NA', np.NaN, regex=True)
df.replace(r'NA'，np.NaN，regex = True)
df.replace(r'XX', np.NaN, regex=True)
df.replace(r'XX'，np.NaN，regex = True)
3.2检查空值 (3.2 Check Nulls)
List null values
列出空值
df.columns[df.isna().any()].tolist()
df.columns [df.isna().any()].tolist()
2. Check the Percentage of nulls
2.检查空值百分比
df.isnull().sum()/len(df)*100).sort_values(ascending = False)
df.isnull().sum()/ len(df)* 100).sort_values(升序= False)
3. Profilers ― Check Null value visualizations.
3.事件探查器―检查“空值”可视化.
3.3删除行/列 (3.3 Removing Rows/Columns)
Removing the outlier data
删除异常数据
df = df[df['field1'] < Outliers]
df = df [df ['field1'] <离群值]
2. Removing data if the target value is missing
2.如果缺少目标值，则删除数据
df1 = df[~df['target-value'].isnull()].copy()
df1 = df [?df ['target-value'].isnull()].copy()
3. High Percentage of nulls
3.高百分比的空值
df.drop(columns = ['col1', 'col2']) / df.drop(['col1', 'col2'], axis = 1)
df.drop(columns = ['col1'，'col2'])/ df.drop(['col1'，'col2']，轴= 1)
df = df.loc[:, df.isnull().mean() < .95]
df = df.loc [:, df.isnull().mean()<.95]
3.4.1缺失值类型 (3.4.1 Missing Values Types)
MCAR: It stands for Missing Completely At Random. The reason behind a missing value is not dependent on any other feature.
MCAR ：代表随机完全丢失. 缺少值的原因不依赖于任何其他功能.
MAR: It stands for Missing At Random. The reason behind a missing value may be associated with some other features.
MAR ：代表“随机失踪”. 缺少值的原因可能与某些其他功能有关.
MNAR: It stands for Missing Not At Random. There is a specific reason behind a missing value.
MNAR ：代表不随机丢失. 缺少值背后有特定原因.
3.4.2缺失值处理 (3.4.2 Missing Values Treatment)
df.[field].fillna() → fill null values
df.[field] .fillna() →填充空值
Replacing all nulls with zeros →
用零替换所有空值→
df.fillna(0)
df.fillna(0)
2. Imputing values using Mean, Median, Mode
2.使用均值，中位数，众数估算值
Mean ― continuous, if data set does not have outliers.
均值-连续(如果数据集没有异常值).
df['field1'].mean()
df ['field1'].mean()
Median ― Has outliers
中位数-有异常值
df['field1'].median()
df ['field1'].median()
Mode ― Max occurrence value, categorical.
模式-最大发生值(绝对).
df['field1'].mode()[0]
df ['field1'].mode()[0]
3. Fill with a relevant value by looking at other columns of the same row.
3.通过查看同一行的其他列来填充相关值.
df['field'] = df.apply(lambda x : transform(x), axis =1)
df ['field'] = df.apply(lambda x：transform(x)，axis = 1)
4.处理异常值 (4. Handling outliers)
Image for post
Q3 = np.percentile(df['field1'], 75)
Q3 = np.percentile(df ['field1']，75)
Q1 = np.percentile(df['field1'], 25)
Q1 = np.percentile(df ['field1']，25)
IQR = Q3 ― Q1
IQR = Q3-Q1
Outliers = Q3 + 1.5 * IQR
离群值= Q3 + 1.5 * IQR
df = df[df['field1'] < Outliers] df['field1'].plot(kind='box')
df = df [df ['field1'] <离群值] df ['field1'].plot(kind ='box')
5.标准化值 (5. Standardising the values)
Standardise Precision ― for better presentation of data. e.g. change 4.5312341 kg to 4.53 kg.
标准化精度 -更好地呈现数据. 例如，将4.5312341公斤更改为4.53公斤.
df['field'] = df['field'] .apply(lambda x : round(x,2))
df ['field'] = df ['field'] .apply(lambda x：round(x，2))
2. Scale Values/Standardise Units ― Ensure all observations under one variable are expressed in a common and consistent unit.
2. 标度值/标准化单位 -确保在一个变量下的所有观察值均以相同且一致的单位表示.
df['field'] = df['field'] .apply(lambda x : transform(x))
df ['field'] = df ['field'] .apply(lambda x：transform(x))
3. Standardise Format ― It is important to standardise the format of other elements such as date and name. e.g., change 23/10/16 to 2016/10/23.
3. 标准化格式―标准化其他元素(例如日期和名称)的格式很重要. 例如，将23/10/16更改为2016/10/23.
df['field1'] = pd.to_datetime(df['field1'], format = '%d%b%Y:%H:%M:%S. %f')
df ['field1'] = pd.to_datetime(df ['field1']，format ='％d％b％Y：％H：％M：％S.％f')
strftimes ― Check the date format from this link.
strftimes-从此链接检查日期格式.
4. Standardise Case ― String variables may take various casing styles, e.g. FULLCAPS, lowercase, Title Case, Sentence case, etc.
4. 标准化大小写 ―字符串变量可以采用各种大小写样式，例如FULLCAPS，小写字母，标题大小写，句子大小写等.
df['field1'] = df['field1'].str.title()
df ['field1'] = df ['field1'].str.title()
5. Remove Characters ― Remove extra characters such as common prefixes/suffixes, leading/trailing/multiple spaces.
5. 删除字符 -删除多余的字符，例如常见的前缀/后缀，前导/后缀/多个空格.
df['field1'] = arrests['field1'].str.replace('$', '' )
df ['field1'] =逮捕['field1'].str.replace('$'，'')
df['field1'] = df['field1'].str.strip()
df ['field1'] = df ['field1'].str.strip()
6.修正无效值 (6. Fixing invalid values)
Encode unicode properly ― In case the data is being read as junk characters, try to change the encoding.
正确编码unicode-如果数据被读取为垃圾字符，请尝试更改编码.
df= pd.read_csv('file.csv',encoding ='cp1252')
df = pd.read_csv('file.csv'，encoding ='cp1252')
2. Convert incorrect data types ― Change the incorrect data types to the correct data types for ease of analysis.
2. 转换不正确的数据类型―将不正确的数据类型更改为正确的数据类型，以便于分析.
df['field1'] = df['field1'].astype(int)
df ['field1'] = df ['field1'].astype(int)
df[toNumFieldsArray] = df[toNumFieldsArray].apply(pd.to_numeric, errors='coerce',axis=1)
df [toNumFieldsArray] = df [toNumFieldsArray] .apply(pd.to_numeric，errors ='coerce'，axis = 1)
3. Correct values that lie beyond the range ― If some values lie beyond the logical range.
3. 纠正超出范围的值-如果某些值超出逻辑范围.
df.['field1'].describe() -> min/max values
df.['field1'].describe()->最小值/最大值
df.['field1'].plot(kind = 'box')
df.['field1'].plot(kind ='box')
4. Correct values that do not belong to the list ― Remove values that do not belong to a list. eg. a data set of blood groups of individuals, strings 'E' and 'F' are invalid values.
4. 纠正不属于列表的值―删除不属于列表的值. 例如. 在个人血型数据集中，字符串“ E”和“ F”是无效值.
df['field'].value_counts()
df ['field'].value_counts()
5. Fix incorrect structure ― Values that do not follow a defined structure can be removed Eg. a phone number of 12 digits is an invalid value.
5. 修复错误的结构-可以删除不遵循定义的结构的值，例如. 12位电话号码是无效值.
df['len'] = df['filed1'].apply(lambda x : len(str(x)))
df ['len'] = df ['filed1'].apply(lambda x：len(str(x)))
6. Validate internal rules ― Internal rules, if present, should be correct and consistent. eg. a product's date of delivery cannot be less than date of purchase.
6. 验证内部规则-内部规则(如果存在)应正确且一致. 例如. 产品的交货日期不能少于购买日期.
df[df['field1']>df['field2']]
df [df ['field1']> df ['field2']]
7.过滤数据 (7. Filtering the data)
Deduplicate data ― Remove identical rows and rows in which some columns are identical.
重复数据删除―删除相同的行和某些列相同的行.
df.drop_duplicates(subset =”field1",keep = first/last, inplace = True)
df.drop_duplicates(subset =“ field1”，keep = first / last，inplace = True)
2. Filter rows ― Filter rows by segment and date period to obtain only rows that are relevant to the analysis.
2. 筛选行―按细分和日期期间筛选行，以仅获取与分析相关的行.
df = df.loc['condition']
df = df.loc ['condition']
3. Filter columns ― Filter columns that are relevant to the analysis.
3. 过滤器列-与分析相关的过滤器列.
df_derived = df.filter(regex = “^COMMON_EXP”, axis = 1)
df_derived = df.filter(regex =“ ^ COMMON_EXP”，轴= 1)
4. Binning Data ― converting a numerical data to categorical.
4. 合并数据 -将数字数据转换为分类数据.
df['field-group'] = pd.cut(df['field1'] , bins=np.linspace (min,max,bin_count))
df ['field-group'] = pd.cut(df ['field1']，bins = np.linspace(min，max，bin_count))

构造函数

DataFrame([data, index, columns, dtype, copy]) #构造数据框


属性和数据
DataFrame.axes                                #index: 行标签；columns: 列标签
DataFrame.as_matrix([columns])                #转换为矩阵
DataFrame.dtypes                              #返回数据的类型
DataFrame.ftypes                              #返回每一列的 数据类型float64:dense
DataFrame.get_dtype_counts()                  #返回数据框数据类型的个数
DataFrame.get_ftype_counts()                  #返回数据框数据类型float64:dense的个数
DataFrame.select_dtypes([include, include])   #根据数据类型选取子数据框
DataFrame.values                              #Numpy的展示方式
DataFrame.axes                                #返回横纵坐标的标签名
DataFrame.ndim                                #返回数据框的纬度
DataFrame.size                                #返回数据框元素的个数
DataFrame.shape                               #返回数据框的形状
DataFrame.memory_usage()                      #每一列的存储

类型转换
DataFrame.astype(dtype[, copy, errors])       #转换数据类型
DataFrame.copy([deep])                        #deep深度复制数据
DataFrame.isnull()                            #以布尔的方式返回空值
DataFrame.notnull()                           #以布尔的方式返回非空值

索引和迭代
DataFrame.head([n])                           #返回前n行数据
DataFrame.at                                  #快速标签常量访问器
DataFrame.iat                                 #快速整型常量访问器
DataFrame.loc                                 #标签定位，使用名称
DataFrame.iloc                                #整型定位，使用数字
DataFrame.insert(loc, column, value)          #在特殊地点loc[数字]插入column[列名]某列数据
DataFrame.iter()                              #Iterate over infor axis
DataFrame.iteritems()                         #返回列名和序列的迭代器
DataFrame.iterrows()                          #返回索引和序列的迭代器
DataFrame.itertuples([index, name])           #Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple.
DataFrame.lookup(row_labels, col_labels)      #Label-based “fancy indexing” function for DataFrame.
DataFrame.pop(item)                           #返回删除的项目
DataFrame.tail([n])                           #返回最后n行
DataFrame.xs(key[, axis, level, drop_level])  #Returns a cross-section (row(s) or column(s)) from the Series/DataFrame.
DataFrame.isin(values)                        #是否包含数据框中的元素
DataFrame.where(cond[, other, inplace, …])    #条件筛选
DataFrame.mask(cond[, other, inplace, …])     #Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other.
DataFrame.query(expr[, inplace])              #Query the columns of a frame with a boolean expression.

二元运算
DataFrame.add(other[,axis,fill_value])        #加法，元素指向
DataFrame.sub(other[,axis,fill_value])        #减法，元素指向
DataFrame.mul(other[, axis,fill_value])       #乘法，元素指向
DataFrame.div(other[, axis,fill_value])       #小数除法，元素指向
DataFrame.truediv(other[, axis, level, …])    #真除法，元素指向
DataFrame.floordiv(other[, axis, level, …])   #向下取整除法，元素指向
DataFrame.mod(other[, axis,fill_value])       #模运算，元素指向
DataFrame.pow(other[, axis,fill_value])       #幂运算，元素指向
DataFrame.radd(other[, axis,fill_value])      #右侧加法，元素指向
DataFrame.rsub(other[, axis,fill_value])      #右侧减法，元素指向
DataFrame.rmul(other[, axis,fill_value])      #右侧乘法，元素指向
DataFrame.rdiv(other[, axis,fill_value])      #右侧小数除法，元素指向
DataFrame.rtruediv(other[, axis, …])          #右侧真除法，元素指向
DataFrame.rfloordiv(other[, axis, …])         #右侧向下取整除法，元素指向
DataFrame.rmod(other[, axis,fill_value])      #右侧模运算，元素指向
DataFrame.rpow(other[, axis,fill_value])      #右侧幂运算，元素指向
DataFrame.lt(other[, axis, level])            #类似Array.lt
DataFrame.gt(other[, axis, level])            #类似Array.gt
DataFrame.le(other[, axis, level])            #类似Array.le
DataFrame.ge(other[, axis, level])            #类似Array.ge
DataFrame.ne(other[, axis, level])            #类似Array.ne
DataFrame.eq(other[, axis, level])            #类似Array.eq
DataFrame.combine(other,func[,fill_value, …]) #Add two DataFrame objects and do not propagate NaN values, so if for a
DataFrame.combine_first(other)                #Combine two DataFrame objects and default to non-null values in frame calling the method.

函数应用&分组&窗口
DataFrame.apply(func[, axis, broadcast, …])   #应用函数
DataFrame.applymap(func)                      #Apply a function to a DataFrame that is intended to operate elementwise, i.e.
DataFrame.aggregate(func[, axis])             #Aggregate using callable, string, dict, or list of string/callables
DataFrame.transform(func, *args, **kwargs)    #Call function producing a like-indexed NDFrame
DataFrame.groupby([by, axis, level, …])       #分组
DataFrame.rolling(window[, min_periods, …])   #滚动窗口
DataFrame.expanding([min_periods, freq, …])   #拓展窗口
DataFrame.ewm([com, span, halflife,  …])      #指数权重窗口

描述统计学
DataFrame.abs()                               #返回绝对值
DataFrame.all([axis, bool_only, skipna])      #Return whether all elements are True over requested axis
DataFrame.any([axis, bool_only, skipna])      #Return whether any element is True over requested axis
DataFrame.clip([lower, upper, axis])          #Trim values at input threshold(s).
DataFrame.clip_lower(threshold[, axis])       #Return copy of the input with values below given value(s) truncated.
DataFrame.clip_upper(threshold[, axis])       #Return copy of input with values above given value(s) truncated.
DataFrame.corr([method, min_periods])         #返回本数据框成对列的相关性系数
DataFrame.corrwith(other[, axis, drop])       #返回不同数据框的相关性
DataFrame.count([axis, level, numeric_only])  #返回非空元素的个数
DataFrame.cov([min_periods])                  #计算协方差
DataFrame.cummax([axis, skipna])              #Return cumulative max over requested axis.
DataFrame.cummin([axis, skipna])              #Return cumulative minimum over requested axis.
DataFrame.cumprod([axis, skipna])             #返回累积
DataFrame.cumsum([axis, skipna])              #返回累和
DataFrame.describe([percentiles,include, …])  #整体描述数据框
DataFrame.diff([periods, axis])               #1st discrete difference of object
DataFrame.eval(expr[, inplace])               #Evaluate an expression in the context of the calling DataFrame instance.
DataFrame.kurt([axis, skipna, level, …])      #返回无偏峰度Fisher’s  (kurtosis of normal == 0.0).
DataFrame.mad([axis, skipna, level])          #返回偏差
DataFrame.max([axis, skipna, level, …])       #返回最大值
DataFrame.mean([axis, skipna, level, …])      #返回均值
DataFrame.median([axis, skipna, level, …])    #返回中位数
DataFrame.min([axis, skipna, level, …])       #返回最小值
DataFrame.mode([axis, numeric_only])          #返回众数
DataFrame.pct_change([periods, fill_method])  #返回百分比变化
DataFrame.prod([axis, skipna, level, …])      #返回连乘积
DataFrame.quantile([q, axis, numeric_only])   #返回分位数
DataFrame.rank([axis, method, numeric_only])  #返回数字的排序
DataFrame.round([decimals])                   #Round a DataFrame to a variable number of decimal places.
DataFrame.sem([axis, skipna, level, ddof])    #返回无偏标准误
DataFrame.skew([axis, skipna, level, …])      #返回无偏偏度
DataFrame.sum([axis, skipna, level, …])       #求和
DataFrame.std([axis, skipna, level, ddof])    #返回标准误差
DataFrame.var([axis, skipna, level, ddof])    #返回无偏误差

从新索引&选取&标签操作
DataFrame.add_prefix(prefix)                  #添加前缀
DataFrame.add_suffix(suffix)                  #添加后缀
DataFrame.align(other[, join, axis, level])   #Align two object on their axes with the
DataFrame.drop(labels[, axis, level, …])      #返回删除的列
DataFrame.drop_duplicates([subset, keep, …])  #Return DataFrame with duplicate rows removed, optionally only
DataFrame.duplicated([subset, keep])          #Return boolean Series denoting duplicate rows, optionally only
DataFrame.equals(other)                       #两个数据框是否相同
DataFrame.filter([items, like, regex, axis])  #过滤特定的子数据框
DataFrame.first(offset)                       #Convenience method for subsetting initial periods of time series data based on a date offset.
DataFrame.head([n])                           #返回前n行
DataFrame.idxmax([axis, skipna])              #Return index of first occurrence of maximum over requested axis.
DataFrame.idxmin([axis, skipna])              #Return index of first occurrence of minimum over requested axis.
DataFrame.last(offset)                        #Convenience method for subsetting final periods of time series data based on a date offset.
DataFrame.reindex([index, columns])           #Conform DataFrame to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index.
DataFrame.reindex_axis(labels[, axis, …])     #Conform input object to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index.
DataFrame.reindex_like(other[, method, …])    #Return an object with matching indices to myself.
DataFrame.rename([index, columns])            #Alter axes input function or functions.
DataFrame.rename_axis(mapper[, axis, copy])   #Alter index and / or columns using input function or functions.
DataFrame.reset_index([level, drop, …])       #For DataFrame with multi-level index, return new DataFrame with labeling information in the columns under the index names, defaulting to ‘level_0’, ‘level_1’, etc.
DataFrame.sample([n, frac, replace, …])       #返回随机抽样
DataFrame.select(crit[, axis])                #Return data corresponding to axis labels matching criteria
DataFrame.set_index(keys[, drop, append ])    #Set the DataFrame index (row labels) using one or more existing columns.
DataFrame.tail([n])                           #返回最后几行
DataFrame.take(indices[, axis, convert])      #Analogous to ndarray.take
DataFrame.truncate([before, after, axis ])    #Truncates a sorted NDFrame before and/or after some particular index value.

处理缺失值
DataFrame.dropna([axis, how, thresh, …])      #Return object with labels on given axis omitted where alternately any
DataFrame.fillna([value, method, axis, …])    #填充空值
DataFrame.replace([to_replace, value, …])     #Replace values given in ‘to_replace’ with ‘value’.

从新定型&排序&转变形态
DataFrame.pivot([index, columns, values])     #Reshape data (produce a “pivot” table) based on column values.
DataFrame.reorder_levels(order[, axis])       #Rearrange index levels using input order.
DataFrame.sort_values(by[, axis, ascending])  #Sort by the values along either axis
DataFrame.sort_index([axis, level, …])        #Sort object by labels (along an axis)
DataFrame.nlargest(n, columns[, keep])        #Get the rows of a DataFrame sorted by the n largest values of columns.
DataFrame.nsmallest(n, columns[, keep])       #Get the rows of a DataFrame sorted by the n smallest values of columns.
DataFrame.swaplevel([i, j, axis])             #Swap levels i and j in a MultiIndex on a particular axis
DataFrame.stack([level, dropna])              #Pivot a level of the (possibly hierarchical) column labels, returning a DataFrame (or Series in the case of an object with a single level of column labels) having a hierarchical index with a new inner-most level of row labels.
DataFrame.unstack([level, fill_value])        #Pivot a level of the (necessarily hierarchical) index labels, returning a DataFrame having a new level of column labels whose inner-most level consists of the pivoted index labels.
DataFrame.melt([id_vars, value_vars, …])      #“Unpivots” a DataFrame from wide format to long format, optionally
DataFrame.T                                   #Transpose index and columns
DataFrame.to_panel()                          #Transform long (stacked) format (DataFrame) into wide (3D, Panel) format.
DataFrame.to_xarray()                         #Return an xarray object from the pandas object.
DataFrame.transpose(*args, **kwargs)          #Transpose index and columns

Combining& joining&merging
DataFrame.append(other[, ignore_index, …])    #追加数据
DataFrame.assign(**kwargs)                    #Assign new columns to a DataFrame, returning a new object (a copy) with all the original columns in addition to the new ones.
DataFrame.join(other[, on, how, lsuffix, …])  #Join columns with other DataFrame either on index or on a key column.
DataFrame.merge(right[, how, on, left_on, …]) #Merge DataFrame objects by performing a database-style join operation by columns or indexes.
DataFrame.update(other[, join, overwrite, …]) #Modify DataFrame in place using non-NA values from passed DataFrame.

时间序列
DataFrame.asfreq(freq[, method, how, …])      #将时间序列转换为特定的频次
DataFrame.asof(where[, subset])               #The last row without any NaN is taken (or the last row without
DataFrame.shift([periods, freq, axis])        #Shift index by desired number of periods with an optional time freq
DataFrame.first_valid_index()                 #Return label for first non-NA/null value
DataFrame.last_valid_index()                  #Return label for last non-NA/null value
DataFrame.resample(rule[, how, axis, …])      #Convenience method for frequency conversion and resampling of time series.
DataFrame.to_period([freq, axis, copy])       #Convert DataFrame from DatetimeIndex to PeriodIndex with desired
DataFrame.to_timestamp([freq, how, axis])     #Cast to DatetimeIndex of timestamps, at beginning of period
DataFrame.tz_convert(tz[, axis, level, copy]) #Convert tz-aware axis to target time zone.
DataFrame.tz_localize(tz[, axis, level, …])   #Localize tz-naive TimeSeries to target time zone.

作图
DataFrame.plot([x, y, kind, ax, ….])          #DataFrame plotting accessor and method
DataFrame.plot.area([x, y])                   #面积图Area plot
DataFrame.plot.bar([x, y])                    #垂直条形图Vertical bar plot
DataFrame.plot.barh([x, y])                   #水平条形图Horizontal bar plot
DataFrame.plot.box([by])                      #箱图Boxplot
DataFrame.plot.density(**kwds)                #核密度Kernel Density Estimate plot
DataFrame.plot.hexbin(x, y[, C, …])           #Hexbin plot
DataFrame.plot.hist([by, bins])               #直方图Histogram
DataFrame.plot.kde(**kwds)                    #核密度Kernel Density Estimate plot
DataFrame.plot.line([x, y])                   #线图Line plot
DataFrame.plot.pie([y])                       #饼图Pie chart
DataFrame.plot.scatter(x, y[, s, c])          #散点图Scatter plot
DataFrame.boxplot([column, by, ax, …])        #Make a box plot from DataFrame column optionally grouped by some columns or
DataFrame.hist(data[, column, by, grid, …])   #Draw histogram of the DataFrame’s series using matplotlib / pylab.

转换为其他格式
DataFrame.from_csv(path[, header, sep, …])    #Read CSV file (DEPRECATED, please use pandas.read_csv() instead).
DataFrame.from_dict(data[, orient, dtype])    #Construct DataFrame from dict of array-like or dicts
DataFrame.from_items(items[,columns,orient])  #Convert (key, value) pairs to DataFrame.
DataFrame.from_records(data[, index, …])      #Convert structured or record ndarray to DataFrame
DataFrame.info([verbose, buf, max_cols, …])   #Concise summary of a DataFrame.
DataFrame.to_pickle(path[, compression, …])   #Pickle (serialize) object to input file path.
DataFrame.to_csv([path_or_buf, sep, na_rep])  #Write DataFrame to a comma-separated values (csv) file
DataFrame.to_hdf(path_or_buf, key, **kwargs)  #Write the contained data to an HDF5 file using HDFStore.
DataFrame.to_sql(name, con[, flavor, …])      #Write records stored in a DataFrame to a SQL database.
DataFrame.to_dict([orient, into])             #Convert DataFrame to dictionary.
DataFrame.to_excel(excel_writer[, …])         #Write DataFrame to an excel sheet
DataFrame.to_json([path_or_buf, orient, …])   #Convert the object to a JSON string.
DataFrame.to_html([buf, columns, col_space])  #Render a DataFrame as an HTML table.
DataFrame.to_feather(fname)                   #write out the binary feather-format for DataFrames
DataFrame.to_latex([buf, columns, …])         #Render an object to a tabular environment table.
DataFrame.to_stata(fname[, convert_dates, …]) #A class for writing Stata binary dta files from array-like objects
DataFrame.to_msgpack([path_or_buf, encoding]) #msgpack (serialize) object to input file path
DataFrame.to_sparse([fill_value, kind])       #Convert to SparseDataFrame
DataFrame.to_dense()                          #Return dense representation of NDFrame (as opposed to sparse)
DataFrame.to_string([buf, columns, …])        #Render a DataFrame to a console-friendly tabular output.
DataFrame.to_clipboard([excel, sep])          #Attempt to write text representation of object to the system clipboard



# import datetime
# import sys
# import tushare as ts
# import backtrader as bt
# import twilio
# from twilio.rest import Client
# import yagmail
# import keyring
# import schedule
# import imbox
# from imbox import Imbox
# import sklearn
# from factor_analyzer import FactorAnalyzer
# import requests
# import arrow
# import pandas.io.data as web
# import pandas_datareader.data as web
# import mpl_finance as mpf
# import yfinance as yf
# import baostock as bs
# import json
# from log import logger
# import smtplib
# from email.mime.text import MIMEText
# from email.mime.multipart import MIMEMultipart
# from email.header import Header
# from email.mime.image import MIMEImage
# import poplib
# from email.parser import Parser
# from email.utils import parseaddr
# import urllib.request
# import urllib.parse
# import urllib3
# import jsonpath
# import ipywidgets as widgets
# from IPython.display import display
# from ipywidgets import Layout, Button, Box, FloatText, Textarea, Dropdown, Label, IntSlider
# from termcolor import colored, cprint
# import threading
# from timeit import timeit
# from apscheduler.schedulers.blocking import BlockingScheduler
# from apscheduler.schedulers.background import BackgroundScheduler
# from sklearn import datasets#引入数据集,sklearn包含众多数据集
# from sklearn.model_selection import train_test_split#将数据分为测试集和训练集
# from sklearn.neighbors import KNeighborsClassifier#利用邻近点方式训练数据
# from sklearn.linear_model import LinearRegression#引入线性回归模型
# from sklearn import preprocessing
# from sklearn.datasets.samples_generator import make_classification
# from sklearn.svm import SVC
# from sklearn.datasets import load_iris
# from sklearn.externals import joblib
# import pickle
# from joblib import dump, load
# import socket
# from pytdx.exhq import TdxExHq_API, TDXParams
# from pytdx.hq import TdxHq_API
# import struct
# import subprocess
# import seaborn as sns
# from scipy.stats import bartlett


